{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b44a017-4a9d-4a41-8059-40dcd7e35fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  python-dotenv text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2 huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "988b6079-601b-4882-93d4-caf4e3b95d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method HfFolder.save_token of <class 'huggingface_hub.utils._hf_folder.HfFolder'>>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv \n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "os.environ[\"DEBUG\"] = \"1\"  # Set to \"1\" if you want to use debug mode.\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f26016ca-de36-4233-8680-f79276e8f075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method HfFolder.save_token of <class 'huggingface_hub.utils._hf_folder.HfFolder'>>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "\n",
    "from huggingface_hub.hf_api import HfFolder\n",
    "HfFolder.save_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2e42e34-f392-4aef-aad6-72f730279112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'HuggingFaceH4/zephyr-7b-beta', 'task': 'text-generation', 'model_kwargs': {'max_new_tokens': 512, 'top_k': 30, 'temperature': 0.1, 'repetition_penalty': 1.03}}\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0a9fc97-b773-4803-acff-41abf18f8dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.schema import (\n",
    "#     HumanMessage,\n",
    "#     SystemMessage,\n",
    "# )\n",
    "# from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "\n",
    "# messages = [\n",
    "#     SystemMessage(content=\"Explain the meaning and origin of a user-provided idiom or proverb, including its figurative meaning, typical usage, historical context, and any interesting origin stories\"),\n",
    "#     HumanMessage(\n",
    "#         content=\"Break a leg?\"\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "# chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d40010c1-89af-4dca-a2bf-e631875c32b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['text'], messages=[SystemMessage(content='Explain the meaning and origin of a user-provided idiom or proverb, including its figurative meaning, typical usage, historical context, and any interesting origin stories'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]) last=ChatHuggingFace(llm=HuggingFaceHub(client=<InferenceClient(model='mistralai/Mistral-7B-v0.1', timeout=None)>, repo_id='mistralai/Mistral-7B-v0.1', task='text-generation', model_kwargs={'max_new_tokens': 1024, 'top_k': 30, 'temperature': 0.9, 'repetition_penalty': 1.03}), tokenizer=LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}, model_id='mistralai/Mistral-7B-v0.1')\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"Explain the meaning and origin of a user-provided idiom or proverb, including its figurative meaning, typical usage, historical context, and any interesting origin stories\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat_model\n",
    "\n",
    "messages = prompt.format_messages(text=\"Once in blue moon\")\n",
    "\n",
    "print(chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63da2187-9289-4847-97a0-c320a2e30e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\nExplain the meaning and origin of a user-provided idiom or proverb, including its figurative meaning, typical usage, historical context, and any interesting origin stories</s>\\n<|user|>\\nOnce in blue moon</s>\\n<|assistant|>\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect how the chat messages are formatted for the LLM call.\n",
    "chat_model._to_chat_prompt(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bd6824b-30f0-4bff-b500-fd9d036f0a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Explain the meaning and origin of a user-provided idiom or proverb, including its figurative meaning, typical usage, historical context, and any interesting origin stories</s>\n",
      "<|user|>\n",
      "Once in blue moon</s>\n",
      "<|assistant|>\n",
      "The idiom \"once in a blue moon\" refers to a rare or infrequent occurrence. The phrase originated from the ancient Roman belief that a full moon appeared once every 29 or 30 days, making it a rare sight when it coincided with a second full moon in a calendar month, which happens approximately every 2.7 years. This second full moon was called a \"blue moon,\" as it was believed that the moon appeared blue during this rare occurrence.\n",
      "\n",
      "However, the modern usage of the term \"blue moon\" has evolved to mean simply an extremely rare occurrence, and does not necessarily refer to a blue-colored moon. In fact, the moon never actually appears blue, as its color is determined by the amount of dust and pollution in the atmosphere, not by the phase of the moon.\n",
      "\n",
      "The expression \"once in a blue moon\" is often used to describe events that are so infrequent that they are almost never seen or experienced. For example, someone might say, \"I haven't seen you in ages! It feels like it's been once in a blue moon!\" or \"We hardly ever get snow in the winter here, so when it does happen, it's like a blue moon!\"\n",
      "\n",
      "The exact origin of the term \"blue moon\" is unclear, as it has been used in various ways throughout history. Some believe that the term originated in the 19th century as a way to describe an extra full moon that occurred in a calendar month, while others suggest that it may have originated in the 15th century as a way to describe a rare celestial event. Regardless of its origins, the expression \"once in a blue moon\" has become a popular idiom in English, and is widely recognized and understood around the world.\n"
     ]
    }
   ],
   "source": [
    "res = chat_model.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "548eb662-9952-4391-a03d-8156c28254ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.9,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52415021-ee00-4df7-bbdf-de4a0afdfed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba15cfd748f64770bf9e1c81df7603ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0346a6792e423f8b6b03af44e0ce41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd43939a7d048b48685063d0be7450e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eeae55990d944c493acfa436e481a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>[INST] Explain the meaning and origin of the user-provided idiom or proverb Once in a blue moon',\n",
       " 'including its figurative meaning',\n",
       " 'typical usage',\n",
       " 'historical context',\n",
       " 'and any interesting origin stories. [/INST] It could be anything like a phrase from a poem',\n",
       " 'an expression of a person',\n",
       " 'an old saying',\n",
       " 'or a proverb.\\n\\nOnce in a blue moon:\\n\\n1. Figurative meaning:\\nIt’s an English idiom that basically means “not often”.\\n\\n2. Typical Usage:\\n– Once in a blue moon',\n",
       " 'I go out for coffee with my friends. [Not often]\\n– That person is once in a blue moon.\\n– He is such a nice guy; he helps me every once in a blue moon.\\n\\n3. Historical Context:\\nIt originated from a folktale according to which',\n",
       " 'it used to rain gold for a night when a blue moon appeared in the sky. The phrase came into being as a reference to this phenomenon and it was considered an indicator of good fortune.\\n\\nHowever',\n",
       " 'the term was not widely used until the 1940s. The expression was first used by a scientist in his research paper on lunar phases.\\n\\nThe phrase is still used to mean “not often” but people do not link it with a belief about gold falling from the sky anymore.\\nNow that you know the meaning and origin of the expression',\n",
       " 'use it in your daily conversation.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain the meaning and origin of the user-provided idiom or proverb {input}, including its figurative meaning, typical usage, historical context, and any interesting origin stories.\")\n",
    "chain = prompt | chat |output_parser\n",
    "\n",
    "\n",
    "chain.invoke({\"input\": \"Once in a blue moon\"})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9405cc25-bac5-43f0-aab5-b1b7047dd1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7badf-fd42-4751-8884-7bc4bd4108ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
