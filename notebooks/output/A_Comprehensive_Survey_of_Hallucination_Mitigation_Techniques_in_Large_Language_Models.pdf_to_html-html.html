<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft10{font-size:21px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft11{font-size:17px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft12{font-size:11px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft13{font-size:11px;font-family:FRNIHB+CMSY8;color:#000000;}
	.ft14{font-size:11px;font-family:FRNIHB+CMSY8;color:#00007f;}
	.ft15{font-size:11px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft16{font-size:17px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft17{font-size:17px;font-family:GAGOQD+Inconsolatazi4;color:#000000;}
	.ft18{font-size:14px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft19{font-size:14px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft110{font-size:8px;font-family:LNLVHQ+CMSY6;color:#000000;}
	.ft111{font-size:13px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft112{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft113{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft114{font-size:30px;font-family:Times;color:#7f7f7f;-moz-transform: matrix(         0,         -1,          1,          0, 0, 0);-webkit-transform: matrix(         0,         -1,          1,          0, 0, 0);-o-transform: matrix(         0,         -1,          1,          0, 0, 0);-ms-transform: matrix(         0,         -1,          1,          0, 0, 0);-moz-transform-origin: left 75%;-webkit-transform-origin: left 75%;-o-transform-origin: left 75%;-ms-transform-origin: left 75%;}
	.ft115{font-size:14px;line-height:17px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft116{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page1-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html001.png" alt="background image"/>
<p style="position:absolute;top:114px;left:106px;white-space:nowrap" class="ft10">A&#160;Comprehensive&#160;Survey&#160;of&#160;Hallucination&#160;Mitigation&#160;Techniques&#160;in&#160;Large</p>
<p style="position:absolute;top:138px;left:365px;white-space:nowrap" class="ft10">Language&#160;Models</p>
<p style="position:absolute;top:178px;left:86px;white-space:nowrap" class="ft11">S.M&#160;Towhidul&#160;Islam&#160;Tonmoy1,&#160;S&#160;M&#160;Mehedi&#160;Zaman1,&#160;Vinija&#160;Jain3,4∗,&#160;Anku&#160;Rani2,&#160;Vipula&#160;Rawte2,</p>
<p style="position:absolute;top:199px;left:325px;white-space:nowrap" class="ft11">Aman&#160;Chadha<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#1">3,4∗</a>,&#160;Amitava&#160;Das2</p>
<p style="position:absolute;top:218px;left:279px;white-space:nowrap" class="ft15">1Islamic&#160;University&#160;of&#160;Technology,&#160;Bangladesh</p>
<p style="position:absolute;top:239px;left:274px;white-space:nowrap" class="ft15">2AI&#160;Institute,&#160;University&#160;of&#160;South&#160;Carolina,&#160;USA</p>
<p style="position:absolute;top:260px;left:281px;white-space:nowrap" class="ft15">3Stanford&#160;University,&#160;USA,&#160;4Amazon&#160;AI,&#160;USA</p>
<p style="position:absolute;top:284px;left:328px;white-space:nowrap" class="ft17">towhidulislam@iut-dhaka.edu</p>
<p style="position:absolute;top:334px;left:237px;white-space:nowrap" class="ft11">Abstract</p>
<p style="position:absolute;top:369px;left:132px;white-space:nowrap" class="ft115">As&#160;Large&#160;Language&#160;Models&#160;(LLMs)&#160;continue<br/>to&#160;advance&#160;in&#160;their&#160;ability&#160;to&#160;write&#160;human-like<br/>text,&#160;a&#160;key&#160;challenge&#160;remains&#160;around&#160;their&#160;ten-<br/>dency&#160;to&#160;“hallucinate”&#160;–&#160;generating&#160;content&#160;that<br/>appears&#160;factual&#160;but&#160;is&#160;ungrounded.&#160;This&#160;issue&#160;of<br/>hallucination&#160;is&#160;arguably&#160;the&#160;biggest&#160;hindrance<br/>to&#160;safely&#160;deploying&#160;these&#160;powerful&#160;LLMs&#160;into<br/>real-world&#160;production&#160;systems&#160;that&#160;impact&#160;peo-<br/>ple’s&#160;lives.&#160;The&#160;journey&#160;toward&#160;widespread<br/>adoption&#160;of&#160;LLMs&#160;in&#160;practical&#160;settings&#160;heavily<br/>relies&#160;on&#160;addressing&#160;and&#160;mitigating&#160;hallucina-<br/>tions.&#160;Unlike&#160;traditional&#160;AI&#160;systems&#160;focused<br/>on&#160;limited&#160;tasks,&#160;LLMs&#160;have&#160;been&#160;exposed&#160;to<br/>vast&#160;amounts&#160;of&#160;online&#160;text&#160;data&#160;during&#160;train-<br/>ing.&#160;While&#160;this&#160;allows&#160;them&#160;to&#160;display&#160;impres-<br/>sive&#160;language&#160;fluency,&#160;it&#160;also&#160;means&#160;they&#160;are<br/>capable&#160;of&#160;extrapolating&#160;information&#160;from&#160;the<br/>biases&#160;in&#160;training&#160;data,&#160;misinterpreting&#160;ambigu-<br/>ous&#160;prompts,&#160;or&#160;modifying&#160;the&#160;information&#160;to<br/>align&#160;superficially&#160;with&#160;the&#160;input.&#160;This&#160;becomes<br/>hugely&#160;alarming&#160;when&#160;we&#160;rely&#160;on&#160;language&#160;gen-<br/>eration&#160;capabilities&#160;for&#160;sensitive&#160;applications,<br/>such&#160;as&#160;summarizing&#160;medical&#160;records,&#160;cus-<br/>tomer&#160;support&#160;conversations,&#160;financial&#160;analysis<br/>reports,&#160;and&#160;providing&#160;erroneous&#160;legal&#160;advice.<br/>Small&#160;errors&#160;could&#160;lead&#160;to&#160;harm,&#160;revealing&#160;the<br/>LLMs’&#160;lack&#160;of&#160;actual&#160;comprehension&#160;despite<br/>advances&#160;in&#160;self-learning.&#160;This&#160;paper&#160;presents<br/>a&#160;comprehensive&#160;survey&#160;of&#160;over&#160;thirty-two&#160;tech-<br/>niques&#160;developed&#160;to&#160;mitigate&#160;hallucination&#160;in<br/>LLMs.&#160;Notable&#160;among&#160;these&#160;are&#160;Retrieval-</p>
<p style="position:absolute;top:925px;left:131px;white-space:nowrap" class="ft18">Augmented&#160;Generation&#160;(RAG)&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Lewis&#160;et&#160;al.,</a></p>
<p style="position:absolute;top:943px;left:132px;white-space:nowrap" class="ft115"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">2021),&#160;</a>Knowledge&#160;Retrieval&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Varshney&#160;et&#160;al.,<br/>2023),&#160;</a>CoNLI&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Lei&#160;et&#160;al.,&#160;2023),&#160;</a>and&#160;CoVe</p>
<p style="position:absolute;top:979px;left:131px;white-space:nowrap" class="ft18"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Dhuliawala&#160;et&#160;al.,&#160;2023).&#160;</a>Furthermore,&#160;we&#160;in-</p>
<p style="position:absolute;top:997px;left:132px;white-space:nowrap" class="ft115">troduce&#160;a&#160;detailed&#160;taxonomy&#160;categorizing&#160;these<br/>methods&#160;based&#160;on&#160;various&#160;parameters,&#160;such<br/>as&#160;dataset&#160;utilization,&#160;common&#160;tasks,&#160;feedback<br/>mechanisms,&#160;and&#160;retriever&#160;types.&#160;This&#160;classifi-<br/>cation&#160;helps&#160;distinguish&#160;the&#160;diverse&#160;approaches<br/>specifically&#160;designed&#160;to&#160;tackle&#160;hallucination&#160;is-<br/>sues&#160;in&#160;LLMs.&#160;Additionally,&#160;we&#160;analyze&#160;the<br/>challenges&#160;and&#160;limitations&#160;inherent&#160;in&#160;these</p>
<p style="position:absolute;top:1147px;left:131px;white-space:nowrap" class="ft110">∗&#160;Work&#160;does&#160;not&#160;relate&#160;to&#160;position&#160;at&#160;Amazon.</p>
<p style="position:absolute;top:336px;left:485px;white-space:nowrap" class="ft115">techniques,&#160;providing&#160;a&#160;solid&#160;foundation&#160;for&#160;fu-<br/>ture&#160;research&#160;in&#160;addressing&#160;hallucinations&#160;and<br/>related&#160;phenomena&#160;within&#160;the&#160;realm&#160;of&#160;LLMs.</p>
<p style="position:absolute;top:414px;left:459px;white-space:nowrap" class="ft11">1</p>
<p style="position:absolute;top:414px;left:486px;white-space:nowrap" class="ft11">Introduction</p>
<p style="position:absolute;top:451px;left:459px;white-space:nowrap" class="ft116">Hallucination&#160;in&#160;Large&#160;Language&#160;Models&#160;(LLMs)<br/>entails&#160;the&#160;creation&#160;of&#160;factually&#160;erroneous&#160;informa-<br/>tion&#160;spanning&#160;a&#160;multitude&#160;of&#160;subjects.&#160;Given&#160;the<br/>extensive&#160;domain&#160;coverage&#160;of&#160;LLMs,&#160;their&#160;applica-<br/>tion&#160;extends&#160;across&#160;numerous&#160;scholarly&#160;and&#160;profes-<br/>sional&#160;areas.&#160;These&#160;include,&#160;but&#160;are&#160;not&#160;limited&#160;to,<br/>academic&#160;research,&#160;programming,&#160;creative&#160;writing,<br/>technical&#160;advisement,&#160;and&#160;the&#160;facilitation&#160;of&#160;skill&#160;ac-<br/>quisition.&#160;Consequently,&#160;LLMs&#160;have&#160;emerged&#160;as&#160;an<br/>indispensable&#160;component&#160;in&#160;our&#160;daily&#160;lives,&#160;playing<br/>a&#160;crucial&#160;role&#160;in&#160;dispensing&#160;accurate&#160;and&#160;reliable&#160;in-<br/>formation.&#160;Nevertheless,&#160;a&#160;fundamental&#160;issue&#160;with<br/>LLMs&#160;is&#160;their&#160;propensity&#160;to&#160;yield&#160;erroneous&#160;or&#160;fab-<br/>ricated&#160;details&#160;about&#160;real-world&#160;subjects.&#160;This&#160;ten-<br/>dency&#160;to&#160;furnish&#160;incorrect&#160;data,&#160;commonly&#160;referred<br/>to&#160;as&#160;hallucination,&#160;poses&#160;a&#160;significant&#160;challenge<br/>for&#160;researchers&#160;in&#160;the&#160;field.&#160;It&#160;leads&#160;to&#160;scenarios</p>
<p style="position:absolute;top:797px;left:459px;white-space:nowrap" class="ft112">where&#160;advanced&#160;models&#160;like&#160;GPT-4&#160;and&#160;others&#160;of</p>
<p style="position:absolute;top:817px;left:459px;white-space:nowrap" class="ft116">its&#160;ilk&#160;may&#160;generate&#160;references&#160;that&#160;are&#160;inaccurate<br/>or&#160;completely&#160;unfounded&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Rawte&#160;et&#160;al.,&#160;2023).&#160;</a>This<br/>issue&#160;arises&#160;due&#160;to&#160;the&#160;training&#160;phase’s&#160;pattern&#160;gen-<br/>eration&#160;techniques&#160;and&#160;the&#160;absence&#160;of&#160;real-time<br/>internet&#160;updates,&#160;contributing&#160;to&#160;discrepancies&#160;in<br/>the&#160;information&#160;output&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Ray,&#160;2023).</a></p>
<p style="position:absolute;top:941px;left:476px;white-space:nowrap" class="ft112">In&#160;contemporary&#160;computational&#160;linguistics,&#160;miti-</p>
<p style="position:absolute;top:961px;left:459px;white-space:nowrap" class="ft116">gating&#160;hallucination&#160;is&#160;a&#160;critical&#160;focus.&#160;Researchers<br/>have&#160;proposed&#160;various&#160;strategies,&#160;encompassing<br/>feedback&#160;mechanisms,&#160;external&#160;information&#160;re-<br/>trieval,&#160;and&#160;early&#160;refinement&#160;in&#160;language&#160;model<br/>generation,&#160;to&#160;address&#160;this&#160;challenge.&#160;This&#160;paper<br/>assumes&#160;significance&#160;by&#160;consolidating&#160;and&#160;organiz-<br/>ing&#160;these&#160;diverse&#160;techniques&#160;into&#160;a&#160;comprehensive<br/>taxonomy.&#160;In&#160;essence,&#160;the&#160;contributions&#160;of&#160;this&#160;pa-<br/>per&#160;to&#160;the&#160;realm&#160;of&#160;LLM&#160;hallucination&#160;are&#160;threefold:</p>
<p style="position:absolute;top:887px;left:48px;white-space:nowrap" class="ft114">arXiv:2401.01313v2 &#160;[cs.CL] &#160;3 Jan 2024</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft20{font-size:7px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft21{font-size:6px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft22{font-size:6px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft23{font-size:6px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft24{font-size:13px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft25{font-size:13px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft26{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft27{font-size:17px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft28{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft29{font-size:16px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft210{font-size:13px;line-height:15px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft211{font-size:13px;line-height:14px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft212{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft213{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page2-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html002.png" alt="background image"/>
<p style="position:absolute;top:292px;left:108px;white-space:nowrap" class="ft20">Hallucination&#160;Mitigation</p>
<p style="position:absolute;top:301px;left:114px;white-space:nowrap" class="ft20">Techniques&#160;in&#160;LLMs</p>
<p style="position:absolute;top:224px;left:212px;white-space:nowrap" class="ft21">Prompt&#160;Engineering&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#2">§2.1</a></p>
<p style="position:absolute;top:149px;left:325px;white-space:nowrap" class="ft21">Retrieval&#160;Augmented</p>
<p style="position:absolute;top:157px;left:330px;white-space:nowrap" class="ft21">Generation&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#2">§2.1.1</a></p>
<p style="position:absolute;top:114px;left:417px;white-space:nowrap" class="ft21">Before&#160;Generation&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#2">§2.1.1.1</a></p>
<p style="position:absolute;top:110px;left:595px;white-space:nowrap" class="ft21">LLM-Augmenter&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Peng&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:119px;left:604px;white-space:nowrap" class="ft21">FreshPrompt&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Vu&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:143px;left:416px;white-space:nowrap" class="ft21">During&#160;Generation&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#3">§2.1.1.2</a></p>
<p style="position:absolute;top:135px;left:584px;white-space:nowrap" class="ft21">Knowledge&#160;Retrieval&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Varshney&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:143px;left:559px;white-space:nowrap" class="ft21">Decompose-and&#160;Query&#160;framework&#160;(D&amp;Q)&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Cao&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:151px;left:610px;white-space:nowrap" class="ft21">EVER&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Kang&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:171px;left:419px;white-space:nowrap" class="ft21">After&#160;Generation&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#4">§2.1.1.3</a></p>
<p style="position:absolute;top:167px;left:622px;white-space:nowrap" class="ft21">RARR&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Gao&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:176px;left:559px;white-space:nowrap" class="ft21">High&#160;Entropy&#160;Word&#160;Spotting&#160;and&#160;Replacement&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Rawte&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:192px;left:427px;white-space:nowrap" class="ft21">End-to-End&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#4">§2.1.1.4</a></p>
<p style="position:absolute;top:192px;left:559px;white-space:nowrap" class="ft21">Retrieval-Augmented&#160;Generation&#160;(RAG)&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Lewis&#160;et&#160;al.,&#160;2021)</a></p>
<p style="position:absolute;top:237px;left:371px;white-space:nowrap" class="ft21">Self&#160;Refinement&#160;through</p>
<p style="position:absolute;top:245px;left:360px;white-space:nowrap" class="ft21">Feedback&#160;and&#160;Reasoning&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#5">§2.1.2</a></p>
<p style="position:absolute;top:207px;left:578px;white-space:nowrap" class="ft21">Prompting&#160;GPT-3&#160;To&#160;Be&#160;Reliable&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Si&#160;et&#160;al.,&#160;2022)</a></p>
<p style="position:absolute;top:216px;left:600px;white-space:nowrap" class="ft21">ChatProtect&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Mündler&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:224px;left:584px;white-space:nowrap" class="ft21">Self-Reflection&#160;Methodology&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Ji&#160;et&#160;al.,&#160;2023b)</a></p>
<p style="position:absolute;top:233px;left:573px;white-space:nowrap" class="ft21">Structured&#160;Comparative&#160;reasoning&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Yan&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:241px;left:603px;white-space:nowrap" class="ft21">Mind’s&#160;Mirror&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Liu&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:250px;left:611px;white-space:nowrap" class="ft21">DRESS&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Chen&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:258px;left:605px;white-space:nowrap" class="ft21">MixAlign&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Zhang&#160;et&#160;al.,&#160;2023b)</a></p>
<p style="position:absolute;top:267px;left:606px;white-space:nowrap" class="ft21">CoVe&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Dhuliawala&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:275px;left:615px;white-space:nowrap" class="ft21">CoNLI&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Lei&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:295px;left:375px;white-space:nowrap" class="ft21">Prompt&#160;Tuning&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#6">§2.1.3</a></p>
<p style="position:absolute;top:291px;left:608px;white-space:nowrap" class="ft21">UPRISE&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Cheng&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:299px;left:611px;white-space:nowrap" class="ft21">SynTra&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Jones&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:371px;left:217px;white-space:nowrap" class="ft21">Developing&#160;Models&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#7">§3</a></p>
<p style="position:absolute;top:319px;left:367px;white-space:nowrap" class="ft21">Introducing&#160;New&#160;Decoding</p>
<p style="position:absolute;top:328px;left:387px;white-space:nowrap" class="ft21">Strategy&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#7">§3.1</a></p>
<p style="position:absolute;top:315px;left:617px;white-space:nowrap" class="ft21">CAD&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Shi&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:324px;left:611px;white-space:nowrap" class="ft21">DoLa&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Chuang&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:332px;left:575px;white-space:nowrap" class="ft21">Inference-Time&#160;Intervention&#160;(ITI)&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Li&#160;et&#160;al.,&#160;2023a)</a></p>
<p style="position:absolute;top:348px;left:370px;white-space:nowrap" class="ft21">Utilization&#160;of&#160;Knowledge</p>
<p style="position:absolute;top:356px;left:390px;white-space:nowrap" class="ft21">Graph&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#7">§3.2</a></p>
<p style="position:absolute;top:348px;left:618px;white-space:nowrap" class="ft21">RHO&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Ji&#160;et&#160;al.,&#160;2023a)</a></p>
<p style="position:absolute;top:356px;left:610px;white-space:nowrap" class="ft21">FLEEK&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Bayat&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:372px;left:371px;white-space:nowrap" class="ft21">Introducing&#160;Faithfulness</p>
<p style="position:absolute;top:381px;left:369px;white-space:nowrap" class="ft21">based&#160;Loss&#160;Function&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#8">§3.3</a></p>
<p style="position:absolute;top:372px;left:593px;white-space:nowrap" class="ft21">THAM&#160;Framework&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Yoon&#160;et&#160;al.,&#160;2022)</a></p>
<p style="position:absolute;top:381px;left:588px;white-space:nowrap" class="ft21">Loss&#160;Weighting&#160;Method&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Qiu&#160;et&#160;al.,&#160;2023b)</a></p>
<p style="position:absolute;top:418px;left:367px;white-space:nowrap" class="ft21">Supervised&#160;Finetuning&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#8">§3.4</a></p>
<p style="position:absolute;top:396px;left:542px;white-space:nowrap" class="ft21">Knowledge&#160;Injection&#160;and&#160;Teacher-Student&#160;Approach&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Elaraby&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:405px;left:612px;white-space:nowrap" class="ft21">HAR&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Köksal&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:413px;left:559px;white-space:nowrap" class="ft21">Fine-tuning&#160;Language&#160;Models&#160;for&#160;Factuality&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Tian&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:422px;left:596px;white-space:nowrap" class="ft21">BEINFO&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Razumovskaia&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:430px;left:605px;white-space:nowrap" class="ft21">R-Tuning&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Zhang&#160;et&#160;al.,&#160;2023a)</a></p>
<p style="position:absolute;top:439px;left:610px;white-space:nowrap" class="ft21">TWEAK&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Qiu&#160;et&#160;al.,&#160;2023a)</a></p>
<p style="position:absolute;top:465px;left:106px;white-space:nowrap" class="ft211">Figure&#160;1:&#160;Taxonomy&#160;of&#160;hallucination&#160;mitigation&#160;techniques&#160;in&#160;LLMs,&#160;focusing&#160;on&#160;prevalent&#160;methods&#160;that&#160;involve&#160;model<br/>development&#160;and&#160;prompting&#160;techniques.&#160;Model&#160;development&#160;branches&#160;into&#160;various&#160;approaches,&#160;including&#160;new&#160;decoding<br/>strategies,&#160;knowledge&#160;graph-based&#160;optimizations,&#160;the&#160;addition&#160;of&#160;novel&#160;loss&#160;function&#160;components,&#160;and&#160;supervised&#160;fine-tuning.<br/>Meanwhile,&#160;prompt&#160;engineering&#160;can&#160;involve&#160;retrieval&#160;augmentation-based&#160;methods,&#160;feedback-based&#160;strategies,&#160;or&#160;prompt&#160;tuning.</p>
<p style="position:absolute;top:559px;left:119px;white-space:nowrap" class="ft26">1.&#160;Introduction&#160;of&#160;a&#160;systematic&#160;taxonomy&#160;de-</p>
<p style="position:absolute;top:579px;left:139px;white-space:nowrap" class="ft212">signed&#160;to&#160;categorize&#160;hallucination&#160;mitigation<br/>techniques&#160;for&#160;LLMs,&#160;encompassing&#160;Vision<br/>Language&#160;Models&#160;(VLMs).</p>
<p style="position:absolute;top:666px;left:119px;white-space:nowrap" class="ft26">2.&#160;Synthesis&#160;of&#160;the&#160;essential&#160;features&#160;characteriz-</p>
<p style="position:absolute;top:687px;left:139px;white-space:nowrap" class="ft212">ing&#160;these&#160;mitigation&#160;techniques,&#160;thereby&#160;guid-<br/>ing&#160;more&#160;structured&#160;future&#160;research&#160;endeavors</p>
<p style="position:absolute;top:727px;left:138px;white-space:nowrap" class="ft26">within&#160;this&#160;domain.</p>
<p style="position:absolute;top:774px;left:119px;white-space:nowrap" class="ft26">3.&#160;Deliberation&#160;on&#160;the&#160;limitations&#160;and&#160;challenges</p>
<p style="position:absolute;top:794px;left:139px;white-space:nowrap" class="ft212">inherent&#160;in&#160;these&#160;techniques,&#160;accompanied&#160;by<br/>potential&#160;solutions&#160;and&#160;proposed&#160;directions&#160;for<br/>future&#160;research.</p>
<p style="position:absolute;top:884px;left:106px;white-space:nowrap" class="ft27">2</p>
<p style="position:absolute;top:884px;left:133px;white-space:nowrap" class="ft27">Hallucination&#160;Mitigation</p>
<p style="position:absolute;top:925px;left:106px;white-space:nowrap" class="ft26">The&#160;detection&#160;of&#160;hallucinations&#160;has&#160;emerged&#160;as&#160;a</p>
<p style="position:absolute;top:945px;left:106px;white-space:nowrap" class="ft212">significant&#160;concern,&#160;given&#160;the&#160;integral&#160;role&#160;of&#160;gen-<br/>erative&#160;LLMs&#160;in&#160;critical&#160;tasks.&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Qiu&#160;et&#160;al.,&#160;2023b)<br/></a>introduced&#160;mFACT&#160;as&#160;a&#160;method&#160;to&#160;identify&#160;hal-<br/>lucination&#160;in&#160;summaries,&#160;extending&#160;its&#160;applicabil-<br/>ity&#160;beyond&#160;English&#160;to&#160;other&#160;languages.&#160;Addition-<br/>ally,&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Zhang&#160;et&#160;al.,&#160;2023b)&#160;</a>proposed&#160;a&#160;framework<br/>for&#160;hallucination&#160;detection&#160;based&#160;on&#160;contextual&#160;in-<br/>formation.&#160;Another&#160;perspective&#160;on&#160;understanding<br/>hallucination&#160;causation&#160;is&#160;presented&#160;by&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Mündler<br/>et&#160;al.,&#160;2023),&#160;</a>who&#160;explores&#160;self-contradiction&#160;as&#160;a<br/>contributing&#160;factor.</p>
<p style="position:absolute;top:558px;left:459px;white-space:nowrap" class="ft29">2.1</p>
<p style="position:absolute;top:558px;left:496px;white-space:nowrap" class="ft29">Prompt&#160;Engineering</p>
<p style="position:absolute;top:586px;left:459px;white-space:nowrap" class="ft26">Prompt&#160;engineering&#160;is&#160;the&#160;process&#160;of&#160;experimenting</p>
<p style="position:absolute;top:607px;left:459px;white-space:nowrap" class="ft26">with&#160;various&#160;instructions&#160;to&#160;get&#160;the&#160;best&#160;output&#160;pos-</p>
<p style="position:absolute;top:627px;left:459px;white-space:nowrap" class="ft212">sible&#160;from&#160;an&#160;AI&#160;text&#160;generation&#160;model&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(White&#160;et&#160;al.,<br/>2023).&#160;</a>In&#160;terms&#160;of&#160;hallucination&#160;mitigation,&#160;this<br/>process&#160;can&#160;provide&#160;specific&#160;context&#160;and&#160;expected<br/>outcomes&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Feldman&#160;et&#160;al.,&#160;2023).&#160;</a>The&#160;prompt&#160;en-<br/>gineering&#160;mitigation&#160;techniques&#160;can&#160;be&#160;outlined&#160;as<br/>follows:</p>
<p style="position:absolute;top:763px;left:459px;white-space:nowrap" class="ft29">2.1.1</p>
<p style="position:absolute;top:763px;left:508px;white-space:nowrap" class="ft29">Retrieval&#160;Augmented&#160;Generation</p>
<p style="position:absolute;top:789px;left:459px;white-space:nowrap" class="ft212">Retrieval-Augmented&#160;Generation&#160;(RAG)&#160;enhances<br/>the&#160;responses&#160;of&#160;LLMs&#160;by&#160;tapping&#160;into&#160;external,&#160;au-<br/>thoritative&#160;knowledge&#160;bases&#160;rather&#160;than&#160;relying&#160;on<br/>potentially&#160;outdated&#160;training&#160;data&#160;or&#160;the&#160;model’s&#160;in-<br/>ternal&#160;knowledge.&#160;This&#160;approach&#160;addresses&#160;the&#160;key<br/>challenges&#160;of&#160;accuracy&#160;and&#160;currency&#160;in&#160;LLM&#160;out-<br/>puts&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Kang&#160;et&#160;al.,&#160;2023).&#160;</a>RAG&#160;effectively&#160;mitigates<br/>the&#160;issue&#160;of&#160;hallucination&#160;in&#160;LLMs&#160;by&#160;generating<br/>responses&#160;that&#160;are&#160;not&#160;only&#160;pertinent&#160;and&#160;current<br/>but&#160;also&#160;verifiable,&#160;thereby&#160;reinforcing&#160;user&#160;confi-<br/>dence&#160;and&#160;offering&#160;developers&#160;an&#160;economical&#160;way<br/>to&#160;enhance&#160;the&#160;fidelity&#160;and&#160;utility&#160;of&#160;LLMs&#160;across<br/>different&#160;applications.&#160;The&#160;mitigation&#160;techniques<br/>following&#160;this&#160;system&#160;can&#160;be&#160;further&#160;categorized&#160;as:</p>
<p style="position:absolute;top:1087px;left:459px;white-space:nowrap" class="ft29">2.1.1.1</p>
<p style="position:absolute;top:1087px;left:521px;white-space:nowrap" class="ft29">Before&#160;generation</p>
<p style="position:absolute;top:1108px;left:463px;white-space:nowrap" class="ft26">For&#160;the&#160;following&#160;techniques,&#160;the&#160;information&#160;re-</p>
<p style="position:absolute;top:1128px;left:459px;white-space:nowrap" class="ft212">trieval&#160;happens&#160;before&#160;the&#160;generation&#160;of&#160;AI&#160;text:<br/>LLM-Augmenter:&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Peng&#160;et&#160;al.,&#160;2023)&#160;</a>proposes&#160;a</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft30{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft31{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft32{font-size:16px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft33{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft34{font-size:16px;line-height:20px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page3-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html003.png" alt="background image"/>
<p style="position:absolute;top:112px;left:106px;white-space:nowrap" class="ft33">system&#160;that&#160;augments&#160;a&#160;black-box&#160;LLM&#160;with&#160;a&#160;set<br/>of&#160;Plug-And-Play&#160;(PnP)&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Li&#160;et&#160;al.,&#160;2023b)&#160;</a>modules.</p>
<p style="position:absolute;top:152px;left:106px;white-space:nowrap" class="ft30">The&#160;system&#160;makes&#160;the&#160;LLM&#160;generate&#160;responses</p>
<p style="position:absolute;top:173px;left:106px;white-space:nowrap" class="ft33">grounded&#160;in&#160;external&#160;knowledge.&#160;It&#160;also&#160;iteratively<br/>revises&#160;LLM&#160;prompts&#160;to&#160;improve&#160;model&#160;responses<br/>using&#160;feedback&#160;generated&#160;by&#160;utility&#160;functions.&#160;In<br/>this&#160;paper,&#160;the&#160;authors&#160;present&#160;LLM-Augmenter<br/>to&#160;improve&#160;LLMs&#160;with&#160;external&#160;knowledge&#160;and<br/>automated&#160;feedback&#160;using&#160;PnP&#160;modules,&#160;which<br/>do&#160;not&#160;require&#160;any&#160;training&#160;and&#160;can&#160;be&#160;used&#160;in-<br/>stantly.&#160;Given&#160;a&#160;user&#160;query,&#160;the&#160;framework&#160;first<br/>retrieves&#160;evidence&#160;from&#160;external&#160;knowledge&#160;and<br/>performs&#160;reasoning&#160;to&#160;form&#160;evidence&#160;chains.&#160;Then<br/>LLM-Augmenter&#160;queries&#160;a&#160;fixed&#160;LLM&#160;(GPT-3.5)<br/>using&#160;a&#160;prompt&#160;that&#160;contains&#160;the&#160;consolidated&#160;evi-<br/>dence&#160;for&#160;the&#160;LLM&#160;to&#160;generate&#160;a&#160;candidate&#160;response<br/>grounded&#160;in&#160;external&#160;knowledge&#160;(evidence).&#160;LLM-</p>
<p style="position:absolute;top:457px;left:106px;white-space:nowrap" class="ft30">Augmenter&#160;then&#160;verifies&#160;the&#160;candidate’s&#160;response,</p>
<p style="position:absolute;top:477px;left:106px;white-space:nowrap" class="ft33">e.g.,&#160;by&#160;checking&#160;whether&#160;it&#160;hallucinates&#160;evidence.<br/>If&#160;so,&#160;LLM-Augmenter&#160;generates&#160;a&#160;feedback&#160;mes-<br/>sage.&#160;The&#160;message&#160;is&#160;used&#160;to&#160;revise&#160;the&#160;prompt&#160;to<br/>query&#160;GPT-3.5&#160;again.&#160;The&#160;process&#160;iterates&#160;until&#160;a<br/>candidate&#160;response&#160;passes&#160;the&#160;verification&#160;and&#160;is<br/>sent&#160;to&#160;the&#160;user.<br/>FreshPrompt:&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Vu&#160;et&#160;al.,&#160;2023)&#160;</a>address&#160;the&#160;static<br/>nature&#160;of&#160;most&#160;LLMs,&#160;highlighting&#160;their&#160;inabil-<br/>ity&#160;to&#160;adapt&#160;to&#160;the&#160;evolving&#160;world.&#160;The&#160;authors<br/>introduce&#160;FreshQA,&#160;a&#160;dynamic&#160;QA&#160;benchmark,<br/>evaluating&#160;LLMs&#160;on&#160;questions&#160;requiring&#160;current</p>
<p style="position:absolute;top:701px;left:106px;white-space:nowrap" class="ft33">world&#160;knowledge&#160;and&#160;those&#160;with&#160;false&#160;premises.<br/>Through&#160;a&#160;two-mode&#160;evaluation,&#160;correctness&#160;and</p>
<p style="position:absolute;top:742px;left:106px;white-space:nowrap" class="ft33">hallucination&#160;are&#160;measured,&#160;revealing&#160;limitations<br/>and&#160;the&#160;need&#160;for&#160;improvement,&#160;particularly&#160;in&#160;fast-<br/>changing&#160;knowledge&#160;scenarios.&#160;To&#160;address&#160;these<br/>challenges,&#160;the&#160;authors&#160;present&#160;FreshPrompt,&#160;a&#160;few-<br/>shot&#160;prompting&#160;method&#160;that&#160;leverages&#160;a&#160;search&#160;en-<br/>gine&#160;to&#160;incorporate&#160;relevant&#160;and&#160;up-to-date&#160;infor-<br/>mation&#160;into&#160;prompts.&#160;FreshPrompt&#160;outperforms<br/>competing&#160;methods&#160;and&#160;commercial&#160;systems,&#160;with<br/>further&#160;analysis&#160;emphasizing&#160;the&#160;impact&#160;of&#160;the&#160;num-<br/>ber&#160;and&#160;order&#160;of&#160;retrieved&#160;evidence&#160;on&#160;correctness.</p>
<p style="position:absolute;top:945px;left:106px;white-space:nowrap" class="ft30">The&#160;work&#160;contributes&#160;a&#160;detailed&#160;evaluation&#160;of&#160;LLM</p>
<p style="position:absolute;top:965px;left:106px;white-space:nowrap" class="ft33">capabilities&#160;in&#160;adapting&#160;to&#160;evolving&#160;knowledge,&#160;in-<br/>troducing&#160;the&#160;FreshQA&#160;dataset&#160;and&#160;an&#160;effective<br/>prompting&#160;method,&#160;FreshPrompt,&#160;to&#160;enhance&#160;dy-<br/>namic&#160;question&#160;answering.</p>
<p style="position:absolute;top:1067px;left:106px;white-space:nowrap" class="ft32">2.1.1.2</p>
<p style="position:absolute;top:1067px;left:168px;white-space:nowrap" class="ft32">During&#160;generation</p>
<p style="position:absolute;top:1087px;left:113px;white-space:nowrap" class="ft30">The&#160;below&#160;techniques&#160;demonstrate&#160;knowledge</p>
<p style="position:absolute;top:1108px;left:106px;white-space:nowrap" class="ft33">retrieval&#160;at&#160;a&#160;sentence-by-sentence&#160;level,&#160;where&#160;the<br/>model&#160;goes&#160;through&#160;information&#160;retrieval&#160;while<br/>generating&#160;each&#160;sentence.</p>
<p style="position:absolute;top:111px;left:459px;white-space:nowrap" class="ft33">Knowledge&#160;Retrieval:&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Varshney&#160;et&#160;al.,&#160;2023)<br/></a>suggest&#160;a&#160;method&#160;that&#160;entails&#160;actively&#160;detecting<br/>and&#160;reducing&#160;hallucinations&#160;as&#160;they&#160;arise.&#160;Before<br/>moving&#160;on&#160;to&#160;the&#160;creation&#160;of&#160;sentences,&#160;the<br/>approach&#160;first&#160;uses&#160;the&#160;logit&#160;output&#160;values&#160;from&#160;the<br/>model&#160;to&#160;identify&#160;possible&#160;hallucinations,&#160;validate<br/>that&#160;they&#160;are&#160;accurate,&#160;and&#160;then&#160;mitigate&#160;any<br/>hallucinations&#160;that&#160;are&#160;found.&#160;The&#160;most&#160;important<br/>realization&#160;is&#160;that&#160;handling&#160;hallucinations&#160;in&#160;the<br/>generation&#160;process&#160;is&#160;critical&#160;because&#160;it&#160;raises<br/>the&#160;probability&#160;of&#160;producing&#160;a&#160;sentence&#160;with<br/>hallucinations&#160;when&#160;the&#160;model&#160;has&#160;previously<br/>experienced&#160;hallucinations&#160;in&#160;its&#160;output.</p>
<p style="position:absolute;top:356px;left:757px;white-space:nowrap" class="ft30">This</p>
<p style="position:absolute;top:376px;left:459px;white-space:nowrap" class="ft30">study&#160;investigates&#160;the&#160;use&#160;of&#160;logit&#160;output&#160;values</p>
<p style="position:absolute;top:396px;left:457px;white-space:nowrap" class="ft30">–&#160;which&#160;are&#160;produced&#160;by&#160;models&#160;like&#160;the&#160;GPT-3</p>
<p style="position:absolute;top:416px;left:459px;white-space:nowrap" class="ft34">and&#160;others&#160;–&#160;in&#160;the&#160;identification&#160;of&#160;hallucinations.<br/>However,&#160;it&#160;acknowledges&#160;that&#160;some&#160;models<br/>available&#160;solely&#160;through&#160;API&#160;calls&#160;might&#160;not<br/>give&#160;logit&#160;output&#160;values&#160;and&#160;emphasizes&#160;that&#160;this<br/>information&#160;is&#160;a&#160;supplementary&#160;source&#160;rather&#160;than<br/>a&#160;necessary&#160;prerequisite&#160;for&#160;the&#160;hallucination<br/>detection&#160;approach.&#160;The&#160;method&#160;uses&#160;retrieved<br/>knowledge&#160;as&#160;support&#160;for&#160;the&#160;correction&#160;phase,<br/>instructing&#160;the&#160;model&#160;to&#160;repair&#160;the&#160;phrase&#160;by<br/>either&#160;eliminating&#160;or&#160;substituting&#160;hallucinated<br/>information&#160;to&#160;reduce&#160;hallucinations&#160;in&#160;the&#160;created<br/>sentence.<br/>Decompose&#160;and&#160;Query&#160;framework&#160;(D&amp;Q):<br/>In&#160;their&#160;research,&#160;the&#160;authors&#160;of</p>
<p style="position:absolute;top:681px;left:707px;white-space:nowrap" class="ft30"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Cao&#160;et&#160;al.,</a></p>
<p style="position:absolute;top:701px;left:459px;white-space:nowrap" class="ft33"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">2023)&#160;</a>address&#160;challenges&#160;faced&#160;by&#160;LLMs&#160;in<br/>Question&#160;Answering,&#160;focusing&#160;on&#160;hallucinations<br/>and&#160;difficulties&#160;with&#160;multi-hop&#160;relations.&#160;They<br/>propose&#160;the&#160;D&amp;Q&#160;framework&#160;to&#160;guide&#160;models&#160;in<br/>utilizing&#160;external&#160;knowledge&#160;while&#160;constraining<br/>reasoning&#160;to&#160;reliable&#160;information,&#160;thus&#160;mitigating<br/>the&#160;risk&#160;of&#160;hallucinations.&#160;Experimental&#160;results<br/>demonstrate&#160;D&amp;Q’s&#160;effectiveness,&#160;showcasing<br/>competitive&#160;performance&#160;against&#160;GPT-3.5&#160;on<br/>ChitChatQA&#160;and&#160;achieving&#160;a&#160;noteworthy&#160;59.6%<br/>F1&#160;score&#160;on&#160;HotPotQA&#160;(question-only).</p>
<p style="position:absolute;top:904px;left:761px;white-space:nowrap" class="ft30">The</p>
<p style="position:absolute;top:925px;left:459px;white-space:nowrap" class="ft33">framework&#160;involves&#160;a&#160;supervised&#160;fine-tuning<br/>phase&#160;without&#160;tool&#160;invocation,&#160;and&#160;during&#160;the<br/>prediction&#160;phase,&#160;the&#160;model&#160;uses&#160;external&#160;tools&#160;to<br/>query&#160;a&#160;reliable&#160;question-answer&#160;base,&#160;allowing<br/>for&#160;backtracking&#160;and&#160;initiating&#160;new&#160;searches&#160;if<br/>needed.&#160;The&#160;findings&#160;underscore&#160;D&amp;Q’s&#160;potential<br/>to&#160;enhance&#160;the&#160;robustness&#160;and&#160;performance&#160;of<br/>LLMs&#160;in&#160;question-answering&#160;tasks.<br/>Real-time</p>
<p style="position:absolute;top:1087px;left:548px;white-space:nowrap" class="ft32">Verification</p>
<p style="position:absolute;top:1087px;left:650px;white-space:nowrap" class="ft32">and</p>
<p style="position:absolute;top:1087px;left:697px;white-space:nowrap" class="ft32">Rectification</p>
<p style="position:absolute;top:1107px;left:459px;white-space:nowrap" class="ft32">(EVER):&#160;LLMs&#160;often&#160;struggle&#160;with&#160;the&#160;challenge</p>
<p style="position:absolute;top:1128px;left:459px;white-space:nowrap" class="ft33">of&#160;producing&#160;inaccurate&#160;or&#160;hallucinated&#160;content,<br/>especially&#160;in&#160;reasoning&#160;tasks.&#160;In&#160;response&#160;to&#160;this</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft40{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft41{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft42{font-size:16px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft43{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft44{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft45{font-size:16px;line-height:20px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page4-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html004.png" alt="background image"/>
<p style="position:absolute;top:112px;left:106px;white-space:nowrap" class="ft43">issue&#160;prevalent&#160;in&#160;both&#160;non-retrieval-based&#160;and<br/>retrieval-augmented&#160;generation&#160;approaches,&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Kang<br/>et&#160;al.,&#160;2023)&#160;</a>introduces&#160;the&#160;EVER&#160;framework.<br/>Unlike&#160;existing&#160;methods&#160;that&#160;rectify&#160;hallucinations<br/>post-hoc,&#160;EVER&#160;employs&#160;a&#160;real-time,&#160;stepwise<br/>strategy&#160;during&#160;the&#160;generation&#160;process&#160;to&#160;detect<br/>and&#160;rectify&#160;hallucinations&#160;as&#160;they&#160;occur.</p>
<p style="position:absolute;top:234px;left:408px;white-space:nowrap" class="ft40">The</p>
<p style="position:absolute;top:254px;left:106px;white-space:nowrap" class="ft43">three-stage&#160;process&#160;involves&#160;generation,&#160;validation,<br/>and&#160;rectification,&#160;effectively&#160;identifying&#160;and<br/>correcting&#160;intrinsic&#160;and&#160;extrinsic&#160;hallucinations.<br/>EVER&#160;outperforms&#160;both&#160;retrieval-based&#160;and&#160;non-<br/>retrieval-based&#160;baselines,&#160;showcasing&#160;significant<br/>improvements&#160;in&#160;generating&#160;trustworthy&#160;and<br/>factually&#160;accurate&#160;text&#160;across&#160;diverse&#160;tasks&#160;such<br/>as&#160;short-form&#160;QA,&#160;biography&#160;generation,&#160;and<br/>multi-hop&#160;reasoning.&#160;The&#160;framework’s&#160;efficacy&#160;is<br/>empirically&#160;validated,&#160;demonstrating&#160;its&#160;ability&#160;to<br/>mitigate&#160;the&#160;“snowballing”&#160;issue&#160;of&#160;hallucination,<br/>making&#160;it&#160;a&#160;valuable&#160;contribution&#160;to&#160;enhancing&#160;the<br/>accuracy&#160;and&#160;reliability&#160;of&#160;LLMs.</p>
<p style="position:absolute;top:538px;left:106px;white-space:nowrap" class="ft42">2.1.1.3</p>
<p style="position:absolute;top:538px;left:168px;white-space:nowrap" class="ft42">After&#160;generation</p>
<p style="position:absolute;top:559px;left:110px;white-space:nowrap" class="ft40">The&#160;following&#160;techniques&#160;employ&#160;the&#160;information</p>
<p style="position:absolute;top:579px;left:106px;white-space:nowrap" class="ft43">retrieval&#160;system&#160;after&#160;generating&#160;the&#160;entirety&#160;of&#160;its<br/>output:<br/>Retrofit&#160;Attribution&#160;using&#160;Research&#160;and&#160;Revi-<br/>sion&#160;(RARR):&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Gao&#160;et&#160;al.,&#160;2023)&#160;</a>In&#160;the&#160;realm&#160;of<br/>LLMs,&#160;notable&#160;advancements&#160;have&#160;been&#160;achieved<br/>across&#160;various&#160;tasks;&#160;however,&#160;issues&#160;persist,&#160;such<br/>as&#160;generating&#160;content&#160;without&#160;proper&#160;support&#160;or&#160;ac-<br/>curacy.&#160;The&#160;challenge&#160;of&#160;determining&#160;trustworthi-<br/>ness&#160;in&#160;LLM&#160;outputs,&#160;due&#160;to&#160;a&#160;lack&#160;of&#160;attributability,<br/>prompted&#160;the&#160;introduction&#160;of&#160;RARR.&#160;This&#160;model-<br/>agnostic&#160;system,&#160;presented&#160;in&#160;the&#160;introduction,&#160;au-<br/>tomates&#160;the&#160;attribution&#160;process&#160;for&#160;any&#160;text&#160;gen-<br/>eration&#160;model.&#160;Inspired&#160;by&#160;fact-checking&#160;work-<br/>flows,&#160;RARR&#160;conducts&#160;research&#160;and&#160;post-editing<br/>to&#160;align&#160;content&#160;with&#160;retrieved&#160;evidence&#160;while&#160;pre-<br/>serving&#160;original&#160;qualities,&#160;operating&#160;seamlessly&#160;af-<br/>ter&#160;LLM&#160;generation.&#160;Contributions&#160;outlined&#160;in&#160;the<br/>introduction&#160;encompass&#160;formalizing&#160;the&#160;Editing&#160;for</p>
<p style="position:absolute;top:945px;left:106px;white-space:nowrap" class="ft40">Attribution&#160;task,&#160;introducing&#160;new&#160;metrics,&#160;bench-</p>
<p style="position:absolute;top:965px;left:106px;white-space:nowrap" class="ft43">marking&#160;existing&#160;revision&#160;models,&#160;and&#160;proposing<br/>a&#160;research-and-revise&#160;model.&#160;The&#160;conclusion&#160;un-<br/>derscores&#160;RARR’s&#160;ability&#160;to&#160;enhance&#160;attribution</p>
<p style="position:absolute;top:1026px;left:106px;white-space:nowrap" class="ft40">while&#160;preserving&#160;essential&#160;text&#160;properties,&#160;provid-</p>
<p style="position:absolute;top:1047px;left:106px;white-space:nowrap" class="ft43">ing&#160;a&#160;practical&#160;solution&#160;to&#160;bolster&#160;the&#160;reliability&#160;of<br/>LLM&#160;outputs.<br/>High&#160;Entropy&#160;Word&#160;Spotting&#160;and&#160;Replacement:</p>
<p style="position:absolute;top:1108px;left:106px;white-space:nowrap" class="ft40">While&#160;the&#160;technical&#160;feasibility&#160;of&#160;detecting&#160;high</p>
<p style="position:absolute;top:1128px;left:106px;white-space:nowrap" class="ft43">entropy&#160;words&#160;may&#160;be&#160;apparent,&#160;a&#160;significant<br/>challenge&#160;arises&#160;due&#160;to&#160;the&#160;closed-source&#160;nature</p>
<p style="position:absolute;top:112px;left:459px;white-space:nowrap" class="ft43">of&#160;many&#160;contemporary&#160;LLMs,&#160;with&#160;subscription-<br/>based&#160;APIs&#160;limiting&#160;accessibility.&#160;The&#160;proposed<br/>solution&#160;by&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Rawte&#160;et&#160;al.,&#160;2023)&#160;</a>involves&#160;utilizing<br/>open-source&#160;LLMs&#160;to&#160;identify&#160;high&#160;entropy&#160;words,<br/>followed&#160;by&#160;their&#160;replacement&#160;using&#160;a&#160;lower&#160;Hal-<br/>lucination&#160;Vulnerability&#160;Index-based&#160;LLM.&#160;The&#160;re-<br/>sults&#160;underscore&#160;the&#160;exceptional&#160;performance&#160;of<br/>albert-large-v2&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Lan&#160;et&#160;al.,&#160;2020)&#160;</a>in&#160;detecting&#160;high<br/>entropy&#160;words&#160;in&#160;GPT-3-generated&#160;content.&#160;Con-</p>
<p style="position:absolute;top:295px;left:459px;white-space:nowrap" class="ft40">versely,&#160;distilroberta-base&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Sanh&#160;et&#160;al.,&#160;2019)&#160;</a>ex-</p>
<p style="position:absolute;top:315px;left:459px;white-space:nowrap" class="ft43">hibits&#160;superior&#160;performance&#160;in&#160;replacing&#160;high&#160;en-<br/>tropy&#160;words,&#160;leading&#160;to&#160;a&#160;reduction&#160;in&#160;hallucina-<br/>tions.&#160;An&#160;integral&#160;aspect&#160;of&#160;this&#160;approach&#160;is&#160;the<br/>treatment&#160;of&#160;consecutive&#160;high-entropy&#160;words&#160;as<br/>a&#160;unified&#160;unit,&#160;where&#160;these&#160;words&#160;are&#160;collectively<br/>masked&#160;before&#160;replacement,&#160;proving&#160;particularly&#160;ef-<br/>fective&#160;in&#160;addressing&#160;hallucinations&#160;related&#160;to&#160;Gen-<br/>erated&#160;Golem&#160;or&#160;Acronym&#160;Ambiguity.</p>
<p style="position:absolute;top:494px;left:459px;white-space:nowrap" class="ft42">2.1.1.4</p>
<p style="position:absolute;top:494px;left:521px;white-space:nowrap" class="ft42">End-to-End&#160;RAG</p>
<p style="position:absolute;top:515px;left:464px;white-space:nowrap" class="ft40">The&#160;end-to-end&#160;process&#160;of&#160;RAG&#160;proposed&#160;in&#160;the</p>
<p style="position:absolute;top:535px;left:459px;white-space:nowrap" class="ft43">paper&#160;by&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Lewis&#160;et&#160;al.,&#160;2021)&#160;</a>involves&#160;integrating&#160;a<br/>pre-trained&#160;sequence-to-sequence&#160;(seq2seq)&#160;trans-<br/>former&#160;with&#160;a&#160;dense&#160;vector&#160;index&#160;of&#160;Wikipedia,&#160;ac-<br/>cessed&#160;through&#160;the&#160;Dense&#160;Passage&#160;Retriever&#160;(DPR).</p>
<p style="position:absolute;top:616px;left:459px;white-space:nowrap" class="ft40">This&#160;innovative&#160;combination&#160;allows&#160;the&#160;model&#160;to</p>
<p style="position:absolute;top:637px;left:459px;white-space:nowrap" class="ft43">condition&#160;its&#160;output&#160;generation&#160;on&#160;both&#160;the&#160;input<br/>query&#160;and&#160;latent&#160;documents&#160;provided&#160;by&#160;the&#160;DPR.</p>
<p style="position:absolute;top:679px;left:476px;white-space:nowrap" class="ft40">In&#160;this&#160;process,&#160;the&#160;DPR&#160;acts&#160;as&#160;a&#160;neural&#160;retriever,</p>
<p style="position:absolute;top:699px;left:459px;white-space:nowrap" class="ft40">supplying&#160;relevant&#160;documents&#160;based&#160;on&#160;the&#160;input.</p>
<p style="position:absolute;top:720px;left:459px;white-space:nowrap" class="ft40">These&#160;documents&#160;are&#160;then&#160;used&#160;by&#160;the&#160;seq2seq</p>
<p style="position:absolute;top:740px;left:459px;white-space:nowrap" class="ft43">model,&#160;specifically&#160;BART,&#160;to&#160;generate&#160;the&#160;final&#160;out-<br/>put.&#160;The&#160;model&#160;employs&#160;a&#160;top-K&#160;approximation<br/>to&#160;marginalize&#160;these&#160;latent&#160;documents,&#160;which&#160;can<br/>be&#160;done&#160;on&#160;a&#160;per-output&#160;basis&#160;(assuming&#160;one&#160;doc-<br/>ument&#160;is&#160;responsible&#160;for&#160;all&#160;tokens)&#160;or&#160;a&#160;per-token<br/>basis&#160;(allowing&#160;different&#160;documents&#160;to&#160;influence<br/>different&#160;parts&#160;of&#160;the&#160;output).</p>
<p style="position:absolute;top:884px;left:476px;white-space:nowrap" class="ft40">Crucially,&#160;both&#160;the&#160;generator&#160;and&#160;the&#160;retriever</p>
<p style="position:absolute;top:904px;left:459px;white-space:nowrap" class="ft43">in&#160;this&#160;RAG&#160;setup&#160;are&#160;trained&#160;end-to-end,&#160;ensuring<br/>that&#160;they&#160;learn&#160;jointly&#160;and&#160;improve&#160;each&#160;other’s<br/>performance.&#160;This&#160;methodology&#160;contrasts&#160;with&#160;pre-</p>
<p style="position:absolute;top:965px;left:459px;white-space:nowrap" class="ft40">vious&#160;approaches&#160;that&#160;required&#160;architectures&#160;with</p>
<p style="position:absolute;top:986px;left:459px;white-space:nowrap" class="ft43">non-parametric&#160;memory&#160;to&#160;be&#160;built&#160;from&#160;scratch<br/>for&#160;specific&#160;tasks.&#160;Instead,&#160;RAG&#160;uses&#160;pre-trained<br/>components,&#160;pre-loaded&#160;with&#160;extensive&#160;knowledge,<br/>allowing&#160;the&#160;model&#160;to&#160;access&#160;and&#160;integrate&#160;a&#160;vast<br/>range&#160;of&#160;information&#160;without&#160;the&#160;need&#160;for&#160;addi-<br/>tional&#160;training.&#160;This&#160;end-to-end&#160;approach&#160;results<br/>in&#160;enhanced&#160;performance&#160;on&#160;various&#160;knowledge-<br/>intensive&#160;tasks,&#160;demonstrating&#160;the&#160;efficacy&#160;of&#160;com-<br/>bining&#160;parametric&#160;and&#160;non-parametric&#160;memory&#160;in</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft50{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft51{font-size:16px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft52{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft53{font-size:16px;line-height:20px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft54{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page5-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html005.png" alt="background image"/>
<p style="position:absolute;top:112px;left:106px;white-space:nowrap" class="ft50">generation&#160;models.</p>
<p style="position:absolute;top:146px;left:106px;white-space:nowrap" class="ft51">2.1.2</p>
<p style="position:absolute;top:146px;left:155px;white-space:nowrap" class="ft53">Self-refinement&#160;through&#160;feedback&#160;and<br/>reasoning</p>
<p style="position:absolute;top:193px;left:106px;white-space:nowrap" class="ft50">After&#160;an&#160;LLM&#160;provides&#160;an&#160;output&#160;for&#160;a&#160;specific</p>
<p style="position:absolute;top:213px;left:106px;white-space:nowrap" class="ft54">prompt,&#160;proper&#160;feedback&#160;about&#160;the&#160;output&#160;can&#160;make<br/>the&#160;LLM&#160;give&#160;better&#160;and&#160;more&#160;accurate&#160;outputs<br/>in&#160;its&#160;consecutive&#160;iterations&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Madaan&#160;et&#160;al.,&#160;2023).</a></p>
<p style="position:absolute;top:274px;left:106px;white-space:nowrap" class="ft50">Abiding&#160;by&#160;this&#160;method,&#160;the&#160;following&#160;are&#160;the&#160;spe-</p>
<p style="position:absolute;top:295px;left:106px;white-space:nowrap" class="ft54">cific&#160;hallucination&#160;mitigation&#160;techniques:<br/>Prompting&#160;GPT-3&#160;To&#160;Be&#160;Reliable:&#160;According&#160;to</p>
<p style="position:absolute;top:335px;left:106px;white-space:nowrap" class="ft50"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Si&#160;et&#160;al.,&#160;2022)’</a>s&#160;paper,&#160;LLMs,&#160;particularly&#160;GPT-</p>
<p style="position:absolute;top:356px;left:106px;white-space:nowrap" class="ft54">3,&#160;exhibit&#160;remarkable&#160;few-shot&#160;prompting&#160;abilities,<br/>enhancing&#160;their&#160;applications&#160;in&#160;real-world&#160;language<br/>tasks.&#160;Despite&#160;this,&#160;the&#160;issue&#160;of&#160;improving&#160;GPT-3’s<br/>reliability&#160;remains&#160;underexplored.&#160;This&#160;study&#160;de-<br/>composes&#160;reliability&#160;into&#160;four&#160;crucial&#160;facets&#160;–&#160;gen-<br/>eralizability,&#160;social&#160;biases,&#160;calibration,&#160;and&#160;factual-<br/>ity&#160;–&#160;and&#160;introduces&#160;simple&#160;and&#160;effective&#160;prompts<br/>to&#160;enhance&#160;each&#160;aspect.&#160;The&#160;research&#160;surpasses<br/>smaller-scale&#160;supervised&#160;models&#160;on&#160;all&#160;reliability<br/>metrics,&#160;offering&#160;practical&#160;strategies&#160;for&#160;improving<br/>GPT-3’s&#160;performance.&#160;The&#160;paper&#160;outlines&#160;previous</p>
<p style="position:absolute;top:579px;left:106px;white-space:nowrap" class="ft50">works&#160;on&#160;LLM&#160;reliability,&#160;highlighting&#160;the&#160;novelty</p>
<p style="position:absolute;top:599px;left:106px;white-space:nowrap" class="ft54">of&#160;this&#160;study’s&#160;comprehensive&#160;analysis&#160;and&#160;focus&#160;on<br/>effective&#160;prompting&#160;strategies.&#160;Drawing&#160;inspiration<br/>from&#160;ML&#160;safety&#160;surveys,&#160;the&#160;reliability&#160;framework<br/>aligns&#160;with&#160;identified&#160;risks&#160;in&#160;existing&#160;conceptual<br/>frameworks.&#160;Lastly,&#160;the&#160;systematic&#160;exploration&#160;of<br/>GPT-3’s&#160;reliability&#160;has&#160;been&#160;summarized,&#160;which<br/>introduces&#160;practical&#160;prompting&#160;strategies,&#160;and&#160;em-<br/>phasizes&#160;the&#160;study’s&#160;contribution&#160;to&#160;insights&#160;into<br/>LLMs&#160;and&#160;practical&#160;recommendations&#160;for&#160;GPT-3<br/>users.<br/>ChatProtect:&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Mündler&#160;et&#160;al.,&#160;2023)&#160;</a>focuses&#160;on<br/>an&#160;important&#160;type&#160;of&#160;hallucination&#160;called&#160;self-<br/>contradiction,&#160;which&#160;occurs&#160;when&#160;an&#160;LLM&#160;gener-<br/>ates&#160;two&#160;logically&#160;inconsistent&#160;sentences&#160;given&#160;the<br/>same&#160;context.&#160;They&#160;propose&#160;a&#160;three-step&#160;pipeline<br/>for&#160;reasoning&#160;about&#160;self-contradictions.&#160;Impor-<br/>tantly,&#160;the&#160;approach&#160;is&#160;built&#160;upon&#160;prompting&#160;strate-<br/>gies,&#160;making&#160;it&#160;applicable&#160;to&#160;black-box&#160;LLMs&#160;with-<br/>out&#160;requiring&#160;external&#160;grounded&#160;knowledge.&#160;They<br/>conducted&#160;an&#160;extensive&#160;evaluation&#160;targeting&#160;four<br/>modern&#160;instruction-tuned&#160;LMs&#160;on&#160;the&#160;task&#160;of&#160;open-<br/>domain&#160;text&#160;generation,&#160;demonstrating&#160;the&#160;substan-<br/>tial&#160;benefits&#160;of&#160;the&#160;approach:&#160;it&#160;effectively&#160;exposes<br/>self-contradictions,&#160;accurately&#160;detects&#160;them,&#160;and<br/>appropriately&#160;mitigates&#160;their&#160;occurrence.<br/>Self-Reflection&#160;Methodology:&#160;The&#160;paper&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Ji&#160;et&#160;al.,<br/>2023b)&#160;</a>explores&#160;and&#160;addresses&#160;the&#160;phenomenon&#160;of<br/>hallucination&#160;in&#160;medical&#160;generative&#160;QA&#160;systems</p>
<p style="position:absolute;top:112px;left:459px;white-space:nowrap" class="ft54">utilizing&#160;widely&#160;adopted&#160;LLMs&#160;and&#160;datasets.&#160;The<br/>focus&#160;is&#160;on&#160;identifying&#160;and&#160;understanding&#160;problem-<br/>atic&#160;answers,&#160;emphasizing&#160;hallucination.&#160;To&#160;tackle<br/>this&#160;challenge,&#160;the&#160;paper&#160;introduces&#160;an&#160;interactive<br/>self-reflection&#160;methodology&#160;that&#160;integrates&#160;knowl-<br/>edge&#160;acquisition&#160;and&#160;answer&#160;generation.&#160;Through<br/>this&#160;iterative&#160;feedback&#160;process,&#160;the&#160;approach&#160;sys-<br/>tematically&#160;improves&#160;the&#160;factuality,&#160;consistency,<br/>and&#160;entailment&#160;of&#160;generated&#160;answers.&#160;Leveraging<br/>the&#160;interactivity&#160;and&#160;multitasking&#160;ability&#160;of&#160;LLMs,<br/>the&#160;method&#160;produces&#160;progressively&#160;more&#160;precise<br/>and&#160;accurate&#160;answers.&#160;Experimental&#160;results,&#160;both<br/>automatic&#160;and&#160;human&#160;evaluations,&#160;highlight&#160;the<br/>effectiveness&#160;of&#160;this&#160;approach&#160;in&#160;reducing&#160;halluci-<br/>nations&#160;compared&#160;to&#160;baselines.&#160;The&#160;investigation<br/>into&#160;hallucinations&#160;in&#160;generation&#160;tasks,&#160;particularly<br/>in&#160;the&#160;medical&#160;domain,&#160;is&#160;crucial&#160;for&#160;AI’s&#160;account-<br/>ability&#160;and&#160;trustworthiness.&#160;The&#160;proposed&#160;iterative<br/>self-reflection&#160;method,&#160;employing&#160;a&#160;generate-score-<br/>refine&#160;strategy&#160;on&#160;background&#160;knowledge&#160;and&#160;an-<br/>swers,&#160;is&#160;empirically&#160;proven&#160;to&#160;be&#160;effective,&#160;gener-<br/>alizable,&#160;and&#160;scalable&#160;in&#160;mitigating&#160;hallucinations.<br/>Structured&#160;Comparative&#160;(SC)&#160;reasoning:&#160;In&#160;the<br/>realm&#160;of&#160;text&#160;preference&#160;prediction,&#160;where&#160;LLMs<br/>often&#160;grapple&#160;with&#160;inconsistencies&#160;in&#160;reasoning,</p>
<p style="position:absolute;top:620px;left:459px;white-space:nowrap" class="ft50"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Yan&#160;et&#160;al.,&#160;2023)&#160;</a>introduces&#160;the&#160;SC&#160;reasoning</p>
<p style="position:absolute;top:640px;left:459px;white-space:nowrap" class="ft54">method.&#160;SC&#160;employs&#160;a&#160;prompting&#160;approach&#160;that<br/>predicts&#160;text&#160;preferences&#160;by&#160;generating&#160;structured<br/>intermediate&#160;comparisons.&#160;It&#160;starts&#160;by&#160;proposing<br/>aspects&#160;of&#160;comparison&#160;and&#160;then&#160;generates&#160;textual<br/>comparisons&#160;under&#160;each&#160;aspect.&#160;Utilizing&#160;a&#160;pair-</p>
<p style="position:absolute;top:742px;left:459px;white-space:nowrap" class="ft50">wise&#160;consistency&#160;comparator,&#160;SC&#160;ensures&#160;that&#160;each</p>
<p style="position:absolute;top:762px;left:459px;white-space:nowrap" class="ft54">aspect’s&#160;comparisons&#160;distinctly&#160;differentiate&#160;be-<br/>tween&#160;texts,&#160;effectively&#160;reducing&#160;hallucination&#160;and<br/>enhancing&#160;consistency.&#160;The&#160;methodology&#160;is&#160;show-<br/>cased&#160;across&#160;various&#160;NLP&#160;tasks,&#160;including&#160;summa-<br/>rization,&#160;retrieval,&#160;and&#160;automatic&#160;rating,&#160;demon-<br/>strating&#160;that&#160;SC&#160;equips&#160;LLMs&#160;with&#160;state-of-the-art<br/>performance&#160;in&#160;text&#160;preference&#160;prediction.&#160;The<br/>structured&#160;reasoning&#160;approach&#160;of&#160;SC,&#160;along&#160;with<br/>its&#160;consistency&#160;enforcement,&#160;is&#160;validated&#160;through<br/>comprehensive&#160;evaluations&#160;and&#160;ablation&#160;studies,<br/>emphasizing&#160;its&#160;effectiveness&#160;in&#160;improving&#160;accu-<br/>racy&#160;and&#160;coherence&#160;across&#160;diverse&#160;tasks.&#160;Human<br/>evaluations&#160;further&#160;underscore&#160;SC’s&#160;interpretative<br/>capabilities,&#160;assisting&#160;users&#160;in&#160;making&#160;informed&#160;de-<br/>cisions.<br/>Mind’s&#160;Mirror:&#160;While&#160;chain-of-thought&#160;(CoT)<br/>distillation&#160;methods&#160;show&#160;promise&#160;for&#160;downsizing<br/>LLMs&#160;to&#160;small&#160;language&#160;models&#160;(SLMs),&#160;there&#160;is<br/>a&#160;risk&#160;of&#160;carrying&#160;over&#160;flawed&#160;reasoning&#160;and&#160;hallu-<br/>cinations.&#160;To&#160;address&#160;this,&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Liu&#160;et&#160;al.,&#160;2023)&#160;</a>pro-</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft60{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft61{font-size:16px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft62{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft63{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft64{font-size:16px;line-height:20px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page6-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html006.png" alt="background image"/>
<p style="position:absolute;top:112px;left:106px;white-space:nowrap" class="ft63">posed&#160;a&#160;methodology&#160;with&#160;two&#160;key&#160;components:<br/>First,&#160;a&#160;novel&#160;approach&#160;introduces&#160;distilling&#160;the&#160;self-<br/>evaluation&#160;capability&#160;inherent&#160;in&#160;LLMs&#160;into&#160;SLMs,<br/>aiming&#160;to&#160;mitigate&#160;adverse&#160;effects&#160;and&#160;reduce&#160;hal-<br/>lucinations.&#160;Second,&#160;a&#160;comprehensive&#160;distillation<br/>process&#160;incorporates&#160;multiple&#160;distinct&#160;CoT&#160;and&#160;self-<br/>evaluation&#160;paradigms&#160;for&#160;holistic&#160;knowledge&#160;trans-<br/>fer&#160;into&#160;SLMs.</p>
<p style="position:absolute;top:274px;left:123px;white-space:nowrap" class="ft60">The&#160;methodology&#160;trains&#160;SLMs&#160;to&#160;possess&#160;self-</p>
<p style="position:absolute;top:295px;left:106px;white-space:nowrap" class="ft63">evaluation&#160;capabilities,&#160;recognizing&#160;and&#160;correcting<br/>hallucinations&#160;and&#160;unreliable&#160;reasoning,&#160;enhancing<br/>predictive&#160;accuracy&#160;and&#160;reliability&#160;on&#160;various&#160;NLP<br/>tasks.&#160;Comprehensive&#160;experiments&#160;demonstrate&#160;the<br/>superiority&#160;of&#160;this&#160;method&#160;across&#160;reasoning&#160;tasks,<br/>offering&#160;a&#160;promising&#160;approach&#160;to&#160;responsibly&#160;down-<br/>size&#160;LLMs.<br/>DRESS:&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Chen&#160;et&#160;al.,&#160;2023)&#160;</a>propose&#160;using&#160;natural<br/>language&#160;feedback&#160;(NLF),&#160;specifically&#160;critique&#160;and<br/>refinement&#160;NLF,&#160;to&#160;improve&#160;alignment&#160;with&#160;human<br/>preferences&#160;and&#160;interaction&#160;capabilities&#160;of&#160;large&#160;vi-<br/>sion&#160;language&#160;models&#160;(LVLMs).&#160;They&#160;generalize<br/>conditional&#160;reinforcement&#160;learning&#160;to&#160;effectively<br/>incorporate&#160;non-differentiable&#160;NLF&#160;by&#160;training&#160;the<br/>model&#160;to&#160;generate&#160;corresponding&#160;responses&#160;con-<br/>ditioned&#160;on&#160;the&#160;NLF.&#160;Experiments&#160;show&#160;relative<br/>improvements&#160;in&#160;DRESS&#160;over&#160;prior&#160;state-of-the-<br/>art&#160;LVLMs&#160;in&#160;metrics&#160;of&#160;helpfulness,&#160;honesty,&#160;and<br/>harmlessness&#160;alignment.<br/>MixAlign:&#160;Despite&#160;having&#160;accurate&#160;reference<br/>points,&#160;LLMs&#160;may&#160;disregard&#160;them&#160;and&#160;rely&#160;on&#160;in-<br/>correct&#160;references&#160;or&#160;biases&#160;instead.&#160;This&#160;tendency<br/>to&#160;hallucinate&#160;arises&#160;when&#160;users&#160;ask&#160;questions&#160;that<br/>do&#160;not&#160;directly&#160;align&#160;with&#160;the&#160;retrieved&#160;references,<br/>lacking&#160;detailed&#160;knowledge&#160;of&#160;the&#160;stored&#160;informa-<br/>tion.&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Zhang&#160;et&#160;al.,&#160;2023b)&#160;</a>focus&#160;on&#160;this&#160;knowl-<br/>edge&#160;alignment&#160;problem&#160;and&#160;introduce&#160;MixAlign,<br/>a&#160;framework&#160;that&#160;interacts&#160;with&#160;both&#160;the&#160;user&#160;and<br/>knowledge&#160;base&#160;to&#160;clarify&#160;how&#160;the&#160;user&#160;question<br/>relates&#160;to&#160;the&#160;stored&#160;information.&#160;MixAlign&#160;uses&#160;a<br/>language&#160;model&#160;to&#160;achieve&#160;automatic&#160;knowledge<br/>alignment&#160;and,&#160;if&#160;needed,&#160;further&#160;enhances&#160;this<br/>alignment&#160;through&#160;user&#160;clarifications.&#160;MixAlign&#160;fo-<br/>cuses&#160;on&#160;utilizing&#160;grounding&#160;knowledge&#160;for&#160;faithful<br/>decision-making.&#160;In&#160;cases&#160;of&#160;uncertainty&#160;or&#160;unclear<br/>evidence,&#160;MixAlign&#160;generates&#160;a&#160;question&#160;seeking<br/>clarification&#160;from&#160;the&#160;user&#160;-&#160;a&#160;process&#160;referred&#160;to&#160;as<br/>human-assisted&#160;knowledge&#160;alignment.<br/>Chain-of-Verification&#160;(CoVe):&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Dhuliawala&#160;et&#160;al.,<br/>2023)&#160;</a>develop&#160;the&#160;CoVe&#160;method&#160;where&#160;the&#160;model</p>
<p style="position:absolute;top:1116px;left:119px;white-space:nowrap" class="ft60">1.&#160;Drafts&#160;an&#160;initial&#160;response.</p>
<p style="position:absolute;top:1148px;left:119px;white-space:nowrap" class="ft60">2.&#160;Plans&#160;verification&#160;questions&#160;to&#160;fact-check&#160;its</p>
<p style="position:absolute;top:112px;left:492px;white-space:nowrap" class="ft60">draft.</p>
<p style="position:absolute;top:147px;left:472px;white-space:nowrap" class="ft60">3.&#160;Answers&#160;those&#160;questions&#160;independently&#160;so&#160;the</p>
<p style="position:absolute;top:167px;left:492px;white-space:nowrap" class="ft60">answers&#160;are&#160;unbiased.</p>
<p style="position:absolute;top:202px;left:472px;white-space:nowrap" class="ft60">4.&#160;Generates&#160;a&#160;final&#160;verified&#160;response.</p>
<p style="position:absolute;top:237px;left:476px;white-space:nowrap" class="ft60">Experiments&#160;show&#160;CoVe&#160;decreases&#160;hallucina-</p>
<p style="position:absolute;top:257px;left:459px;white-space:nowrap" class="ft63">tions&#160;across&#160;tasks&#160;like&#160;list-based&#160;Wikidata&#160;ques-<br/>tions&#160;and&#160;long-form&#160;text&#160;generation.&#160;Given&#160;a&#160;user<br/>query,&#160;an&#160;LLM&#160;generates&#160;a&#160;baseline&#160;response&#160;that<br/>may&#160;contain&#160;inaccuracies&#160;like&#160;factual&#160;hallucina-<br/>tions.&#160;CoVe&#160;first&#160;generates&#160;verification&#160;questions&#160;to<br/>ask,&#160;then&#160;answers&#160;them&#160;to&#160;check&#160;for&#160;agreement.<br/>Chain&#160;of&#160;Natural&#160;Language&#160;Inference&#160;(CoNLI):</p>
<p style="position:absolute;top:400px;left:459px;white-space:nowrap" class="ft60"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Lei&#160;et&#160;al.,&#160;2023)&#160;</a>address&#160;the&#160;challenge&#160;of&#160;halluci-</p>
<p style="position:absolute;top:420px;left:459px;white-space:nowrap" class="ft63">nations&#160;generated&#160;by&#160;LLMs&#160;when&#160;provided&#160;back-<br/>ground&#160;context.&#160;Despite&#160;fluency&#160;in&#160;natural&#160;lan-<br/>guage&#160;generation,&#160;LLMs&#160;often&#160;produce&#160;ungrounded<br/>hallucinations&#160;unsupported&#160;by&#160;the&#160;given&#160;sources.</p>
<p style="position:absolute;top:501px;left:476px;white-space:nowrap" class="ft60">The&#160;proposed&#160;hierarchical&#160;framework&#160;focuses</p>
<p style="position:absolute;top:522px;left:459px;white-space:nowrap" class="ft60">on&#160;detecting&#160;and&#160;mitigating&#160;such&#160;hallucinations</p>
<p style="position:absolute;top:542px;left:459px;white-space:nowrap" class="ft60">without&#160;requiring&#160;fine-tuning&#160;or&#160;domain-specific</p>
<p style="position:absolute;top:562px;left:459px;white-space:nowrap" class="ft63">prompts.&#160;The&#160;framework&#160;utilizes&#160;Chain&#160;of&#160;Natural<br/>Language&#160;Inference&#160;(CoNLI)&#160;for&#160;state-of-the-art<br/>hallucination&#160;detection&#160;by&#160;identifying&#160;ungrounded<br/>content.&#160;Post-editing&#160;is&#160;then&#160;used&#160;to&#160;reduce&#160;hallu-<br/>cinations&#160;and&#160;enhance&#160;text&#160;quality&#160;without&#160;model<br/>adjustment.&#160;Extensive&#160;experiments&#160;on&#160;text-to-text<br/>datasets&#160;demonstrate&#160;effectiveness&#160;in&#160;both&#160;hallu-<br/>cination&#160;detection&#160;and&#160;reduction.&#160;By&#160;formulating<br/>detection&#160;as&#160;a&#160;chain&#160;of&#160;natural&#160;language&#160;inference<br/>tasks,&#160;the&#160;framework&#160;incorporates&#160;sentence&#160;and<br/>entity-level&#160;judgments&#160;with&#160;interpretability.</p>
<p style="position:absolute;top:786px;left:476px;white-space:nowrap" class="ft60">The&#160;plug-and-play&#160;framework&#160;allows&#160;seamless</p>
<p style="position:absolute;top:807px;left:459px;white-space:nowrap" class="ft63">deployment&#160;across&#160;contexts&#160;with&#160;competitive&#160;hallu-<br/>cination&#160;detection&#160;and&#160;reduction&#160;performance&#160;while<br/>preserving&#160;text&#160;quality.</p>
<p style="position:absolute;top:879px;left:459px;white-space:nowrap" class="ft61">2.1.3</p>
<p style="position:absolute;top:879px;left:508px;white-space:nowrap" class="ft61">Prompt&#160;Tuning</p>
<p style="position:absolute;top:904px;left:459px;white-space:nowrap" class="ft63">Prompt&#160;tuning&#160;is&#160;a&#160;technique&#160;that&#160;involves&#160;adjusting<br/>the&#160;instructions&#160;provided&#160;to&#160;a&#160;pre-trained&#160;LLM&#160;dur-<br/>ing&#160;the&#160;fine-tuning&#160;phase&#160;to&#160;make&#160;the&#160;model&#160;more<br/>effective&#160;at&#160;specific&#160;tasks.&#160;The&#160;LLM&#160;learns&#160;from</p>
<p style="position:absolute;top:986px;left:456px;white-space:nowrap" class="ft60">‘Soft&#160;Prompts’,&#160;which&#160;are&#160;not&#160;predetermined&#160;but&#160;are</p>
<p style="position:absolute;top:1006px;left:459px;white-space:nowrap" class="ft63">instead&#160;learned&#160;by&#160;the&#160;model&#160;through&#160;backpropaga-<br/>tion&#160;during&#160;the&#160;fine-tuning&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Lester&#160;et&#160;al.,&#160;2021).&#160;</a>For<br/>hallucination&#160;mitigation,&#160;the&#160;following&#160;techniques,</p>
<p style="position:absolute;top:1067px;left:459px;white-space:nowrap" class="ft60">which&#160;involve&#160;prompt&#160;tuning,&#160;have&#160;been&#160;proposed</p>
<p style="position:absolute;top:1087px;left:459px;white-space:nowrap" class="ft63">as&#160;of&#160;now:<br/>Universal&#160;Prompt&#160;Retrieval&#160;for&#160;Improving&#160;zero-<br/>Shot&#160;Evaluation&#160;(UPRISE):&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Cheng&#160;et&#160;al.,&#160;2023)<br/></a>propose&#160;UPRISE,&#160;which&#160;tunes&#160;a&#160;lightweight</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft70{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft71{font-size:16px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft72{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft73{font-size:17px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft74{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page7-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html007.png" alt="background image"/>
<p style="position:absolute;top:112px;left:106px;white-space:nowrap" class="ft74">and&#160;versatile&#160;retriever&#160;that&#160;automatically&#160;retrieves<br/>prompts&#160;for&#160;a&#160;given&#160;zero-shot&#160;task&#160;input.&#160;Specifi-<br/>cally,&#160;they&#160;demonstrate&#160;universality&#160;in&#160;a&#160;cross-task<br/>and&#160;cross-model&#160;scenario:&#160;the&#160;retriever&#160;is&#160;tuned&#160;on<br/>a&#160;diverse&#160;set&#160;of&#160;tasks,&#160;but&#160;tested&#160;on&#160;unseen&#160;type<br/>tasks.&#160;The&#160;retriever&#160;is&#160;trained&#160;to&#160;retrieve&#160;prompts<br/>for&#160;multiple&#160;tasks,&#160;enabling&#160;it&#160;to&#160;generalize&#160;to&#160;un-<br/>seen&#160;task&#160;types&#160;during&#160;inference.<br/>SynTra:&#160;Large&#160;language&#160;models&#160;(LLMs)&#160;often&#160;ex-<br/>hibit&#160;hallucination&#160;in&#160;abstractive&#160;summarization<br/>tasks,&#160;even&#160;when&#160;the&#160;necessary&#160;information&#160;is<br/>present.&#160;Addressing&#160;this&#160;challenge&#160;is&#160;difficult&#160;due<br/>to&#160;the&#160;intricate&#160;evaluation&#160;of&#160;hallucination&#160;during<br/>optimization.&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Jones&#160;et&#160;al.,&#160;2023)&#160;</a>introduce&#160;SynTra,<br/>a&#160;method&#160;that&#160;uses&#160;a&#160;synthetic&#160;task&#160;to&#160;efficiently<br/>reduce&#160;hallucination&#160;on&#160;downstream&#160;summariza-<br/>tion&#160;tasks.&#160;SynTra&#160;optimizes&#160;the&#160;LLM’s&#160;system<br/>message&#160;via&#160;prefix-tuning&#160;on&#160;the&#160;synthetic&#160;task,<br/>then&#160;transfers&#160;this&#160;capability&#160;to&#160;more&#160;challenging,<br/>realistic&#160;summarization&#160;tasks.&#160;Experiments&#160;demon-<br/>strate&#160;reduced&#160;hallucination&#160;for&#160;two&#160;13B&#160;parameter<br/>LLMs,&#160;highlighting&#160;the&#160;effectiveness&#160;of&#160;synthetic<br/>data&#160;for&#160;mitigating&#160;undesired&#160;behaviors.</p>
<p style="position:absolute;top:597px;left:106px;white-space:nowrap" class="ft73">3</p>
<p style="position:absolute;top:597px;left:133px;white-space:nowrap" class="ft73">Developing&#160;Models</p>
<p style="position:absolute;top:633px;left:106px;white-space:nowrap" class="ft74">Some&#160;papers&#160;focused&#160;on&#160;developing&#160;novel&#160;models<br/>to&#160;mitigate&#160;hallucinations.&#160;It&#160;is&#160;an&#160;ongoing&#160;and<br/>evolving&#160;process&#160;requiring&#160;a&#160;combination&#160;of&#160;algo-<br/>rithmic&#160;advancements&#160;and&#160;data&#160;quality&#160;improve-<br/>ments.&#160;Instead&#160;of&#160;going&#160;for&#160;fine-tuning&#160;models,&#160;the<br/>following&#160;techniques&#160;implemented&#160;whole&#160;model<br/>architecture&#160;to&#160;tackle&#160;hallucinations.&#160;These&#160;tech-<br/>niques&#160;can&#160;be&#160;categorized&#160;as&#160;follows:</p>
<p style="position:absolute;top:813px;left:106px;white-space:nowrap" class="ft71">3.1</p>
<p style="position:absolute;top:813px;left:143px;white-space:nowrap" class="ft71">Introducing&#160;new&#160;decoding&#160;strategy</p>
<p style="position:absolute;top:842px;left:106px;white-space:nowrap" class="ft74">Decoding&#160;strategy&#160;generally&#160;involves&#160;designing<br/>techniques&#160;that&#160;specifically&#160;target&#160;the&#160;generation<br/>phase&#160;of&#160;a&#160;model.&#160;In&#160;terms&#160;of&#160;hallucination,&#160;the<br/>techniques&#160;aim&#160;to&#160;reduce&#160;the&#160;occurrence&#160;of&#160;halluci-<br/>nations&#160;in&#160;the&#160;generated&#160;outputs&#160;by&#160;guiding&#160;the&#160;gen-<br/>eration&#160;phase&#160;towards&#160;authentic&#160;or&#160;context-specific<br/>generation&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Lango&#160;and&#160;Dusek,&#160;2023).&#160;</a>The&#160;follow-<br/>ing&#160;techniques&#160;make&#160;use&#160;of&#160;the&#160;decoding&#160;strategy:</p>
<p style="position:absolute;top:1006px;left:123px;white-space:nowrap" class="ft71">Context-Aware&#160;Decoding&#160;(CAD):&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Shi&#160;et&#160;al.,</a></p>
<p style="position:absolute;top:1026px;left:106px;white-space:nowrap" class="ft74"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">2023)&#160;</a>present&#160;CAD,&#160;which&#160;follows&#160;a&#160;contrastive<br/>output&#160;distribution&#160;that&#160;amplifies&#160;the&#160;difference&#160;be-<br/>tween&#160;the&#160;output&#160;probabilities&#160;when&#160;a&#160;model&#160;is<br/>used&#160;with&#160;and&#160;without&#160;context.&#160;CAD&#160;is&#160;particu-<br/>larly&#160;effective&#160;in&#160;overriding&#160;a&#160;model’s&#160;prior&#160;knowl-<br/>edge&#160;when&#160;it&#160;contradicts&#160;the&#160;provided&#160;context,&#160;lead-<br/>ing&#160;to&#160;substantial&#160;improvements&#160;in&#160;tasks&#160;where&#160;re-</p>
<p style="position:absolute;top:112px;left:459px;white-space:nowrap" class="ft74">solving&#160;the&#160;knowledge&#160;conflict&#160;is&#160;essential.&#160;CAD<br/>can&#160;be&#160;used&#160;with&#160;off-the-shelf&#160;pre-trained&#160;language<br/>models&#160;without&#160;any&#160;additional&#160;training.&#160;More&#160;no-<br/>tably,&#160;CAD&#160;is&#160;especially&#160;beneficial&#160;for&#160;knowledge-<br/>conflicting&#160;tasks,&#160;where&#160;the&#160;context&#160;contains&#160;infor-<br/>mation&#160;contradictory&#160;to&#160;the&#160;model’s&#160;prior&#160;knowl-<br/>edge.&#160;The&#160;results&#160;demonstrate&#160;the&#160;potential&#160;of<br/>CAD&#160;in&#160;mitigating&#160;hallucinations&#160;in&#160;text&#160;generation<br/>and&#160;overriding&#160;prior&#160;knowledge&#160;with&#160;reliable&#160;and<br/>trusted&#160;information.<br/>Decoding&#160;by&#160;Contrasting&#160;Layers&#160;(DoLa):</p>
<p style="position:absolute;top:335px;left:459px;white-space:nowrap" class="ft70"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Chuang&#160;et&#160;al.,&#160;2023)&#160;</a>introduce&#160;DoLa,&#160;a&#160;simple</p>
<p style="position:absolute;top:356px;left:459px;white-space:nowrap" class="ft74">decoding&#160;strategy&#160;designed&#160;to&#160;mitigate&#160;hallucina-<br/>tions&#160;in&#160;pre-trained&#160;LLMs&#160;without&#160;the&#160;need&#160;for<br/>external&#160;knowledge&#160;conditioning&#160;or&#160;additional&#160;fine-<br/>tuning.&#160;DoLa&#160;achieves&#160;the&#160;next-token&#160;distribution<br/>by&#160;contrasting&#160;logit&#160;differences&#160;between&#160;later&#160;and<br/>earlier&#160;layers&#160;projected&#160;into&#160;the&#160;vocabulary&#160;space.</p>
<p style="position:absolute;top:477px;left:459px;white-space:nowrap" class="ft70">This&#160;leverages&#160;the&#160;observed&#160;localization&#160;of&#160;factual</p>
<p style="position:absolute;top:498px;left:459px;white-space:nowrap" class="ft74">knowledge&#160;in&#160;specific&#160;transformer&#160;layers.&#160;Conse-<br/>quently,&#160;DoLa&#160;enhances&#160;the&#160;identification&#160;of&#160;factual<br/>knowledge&#160;and&#160;minimizes&#160;the&#160;generation&#160;of&#160;incor-<br/>rect&#160;facts.&#160;Across&#160;various&#160;tasks,&#160;including&#160;multiple-<br/>choice&#160;and&#160;open-ended&#160;generation&#160;tasks&#160;like&#160;Truth-<br/>fulQA,&#160;DoLa&#160;consistently&#160;improves&#160;truthfulness,<br/>enhancing&#160;the&#160;performance&#160;of&#160;LLaMA&#160;family&#160;mod-<br/>els.<br/>Inference-Time&#160;Intervention&#160;(ITI):&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Li&#160;et&#160;al.,<br/>2023a)&#160;</a>introduce&#160;ITI,&#160;a&#160;technique&#160;designed&#160;to&#160;en-<br/>hance&#160;the&#160;“truthfulness”&#160;of&#160;LLMs.&#160;ITI&#160;operates<br/>by&#160;shifting&#160;model&#160;activations&#160;during&#160;inference,&#160;fol-<br/>lowing&#160;a&#160;set&#160;of&#160;directions&#160;across&#160;a&#160;limited&#160;number<br/>of&#160;attention&#160;heads.&#160;This&#160;intervention&#160;significantly<br/>improves&#160;the&#160;performance&#160;of&#160;LLaMA&#160;models&#160;on<br/>the&#160;TruthfulQA&#160;benchmark.&#160;The&#160;technique&#160;first<br/>identifies&#160;a&#160;sparse&#160;set&#160;of&#160;attention&#160;heads&#160;with&#160;high<br/>linear&#160;probing&#160;accuracy&#160;for&#160;truthfulness.&#160;Then,&#160;dur-<br/>ing&#160;inference,&#160;they&#160;shift&#160;activations&#160;along&#160;these<br/>truth-correlated&#160;directions.&#160;It&#160;repeats&#160;the&#160;same&#160;in-<br/>tervention&#160;autoregressively&#160;until&#160;the&#160;whole&#160;answer<br/>is&#160;generated.&#160;ITI&#160;results&#160;in&#160;a&#160;significant&#160;perfor-<br/>mance&#160;increase&#160;on&#160;the&#160;TruthfulQA&#160;benchmark.</p>
<p style="position:absolute;top:979px;left:459px;white-space:nowrap" class="ft71">3.2</p>
<p style="position:absolute;top:979px;left:496px;white-space:nowrap" class="ft71">Utilization&#160;of&#160;Knowledge&#160;Graph&#160;(KG)</p>
<p style="position:absolute;top:1006px;left:459px;white-space:nowrap" class="ft74">KGs&#160;are&#160;organized&#160;collections&#160;of&#160;data&#160;that&#160;include<br/>details&#160;about&#160;entities&#160;(i.e.,&#160;people,&#160;places,&#160;or&#160;ob-<br/>jects),&#160;their&#160;characteristics,&#160;and&#160;the&#160;connections<br/>between&#160;them&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Sun&#160;et&#160;al.,&#160;2023a).&#160;</a>It&#160;arranges&#160;data<br/>such&#160;that&#160;machines&#160;can&#160;comprehend&#160;the&#160;relation-<br/>ships&#160;and&#160;semantic&#160;meaning&#160;of&#160;the&#160;material.&#160;KGs<br/>offer&#160;a&#160;basis&#160;for&#160;sophisticated&#160;reasoning,&#160;data&#160;analy-<br/>sis,&#160;and&#160;information&#160;retrieval.&#160;Thus,&#160;several&#160;studies</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft80{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft81{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft82{font-size:16px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft83{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft84{font-size:16px;line-height:20px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page8-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html008.png" alt="background image"/>
<p style="position:absolute;top:112px;left:106px;white-space:nowrap" class="ft83">have&#160;used&#160;KGs&#160;in&#160;the&#160;context&#160;of&#160;hallucination&#160;miti-<br/>gation&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Bayat&#160;et&#160;al.,&#160;2023).&#160;</a>They&#160;are:</p>
<p style="position:absolute;top:152px;left:123px;white-space:nowrap" class="ft82">RHO:&#160;To&#160;handle&#160;the&#160;hallucination&#160;challenge&#160;in</p>
<p style="position:absolute;top:173px;left:106px;white-space:nowrap" class="ft83">dialogue&#160;response&#160;generation,&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Ji&#160;et&#160;al.,&#160;2023a)<br/></a>proposes&#160;a&#160;framework&#160;called&#160;RHO&#160;that&#160;utilizes<br/>the&#160;representations&#160;of&#160;linked&#160;entities&#160;and&#160;relation<br/>predicates&#160;from&#160;a&#160;KG&#160;to&#160;generate&#160;more&#160;faithful&#160;re-<br/>sponses.&#160;To&#160;improve&#160;faithfulness,&#160;they&#160;introduce<br/>local&#160;and&#160;global&#160;knowledge-grounding&#160;techniques<br/>into&#160;dialogue&#160;generation&#160;and&#160;further&#160;utilize&#160;a&#160;con-</p>
<p style="position:absolute;top:315px;left:106px;white-space:nowrap" class="ft80">versational&#160;reasoning&#160;model&#160;to&#160;re-rank&#160;the&#160;gener-</p>
<p style="position:absolute;top:335px;left:106px;white-space:nowrap" class="ft84">ated&#160;responses.&#160;These&#160;two&#160;knowledge&#160;groundings<br/>help&#160;the&#160;model&#160;effectively&#160;encode&#160;and&#160;inject&#160;the<br/>knowledge&#160;information&#160;from&#160;context-related&#160;sub-<br/>graphs&#160;with&#160;proper&#160;attention.&#160;Their&#160;work&#160;improves<br/>the&#160;fusion&#160;and&#160;interaction&#160;between&#160;external&#160;knowl-<br/>edge&#160;and&#160;dialogue&#160;context&#160;via&#160;various&#160;knowledge<br/>groundings&#160;and&#160;reasoning&#160;techniques,&#160;further&#160;re-<br/>ducing&#160;hallucination.<br/>FactuaL&#160;Error&#160;detection&#160;and&#160;correction&#160;with<br/>Evidence&#160;Retrieved&#160;from&#160;external&#160;Knowledge</p>
<p style="position:absolute;top:539px;left:106px;white-space:nowrap" class="ft82">(FLEEK):&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Bayat&#160;et&#160;al.,&#160;2023)&#160;</a>introduce&#160;FLEEK,</p>
<p style="position:absolute;top:559px;left:106px;white-space:nowrap" class="ft83">an&#160;intelligent&#160;and&#160;model-agnostic&#160;tool&#160;aimed&#160;at<br/>aiding&#160;end&#160;users,&#160;such&#160;as&#160;human&#160;graders,&#160;in&#160;fact</p>
<p style="position:absolute;top:600px;left:106px;white-space:nowrap" class="ft80">verification&#160;and&#160;correction.</p>
<p style="position:absolute;top:600px;left:307px;white-space:nowrap" class="ft80">FLEEK&#160;features&#160;a</p>
<p style="position:absolute;top:620px;left:106px;white-space:nowrap" class="ft83">user-friendly&#160;interface&#160;capable&#160;of&#160;autonomously<br/>identifying&#160;potentially&#160;verifiable&#160;facts&#160;within&#160;the<br/>input&#160;text.&#160;It&#160;formulates&#160;questions&#160;for&#160;each&#160;fact<br/>and&#160;queries&#160;both&#160;curated&#160;knowledge&#160;graphs&#160;and<br/>the&#160;open&#160;web&#160;to&#160;gather&#160;evidence.&#160;The&#160;tool&#160;subse-<br/>quently&#160;verifies&#160;the&#160;correctness&#160;of&#160;the&#160;facts&#160;using<br/>the&#160;acquired&#160;evidence&#160;and&#160;proposes&#160;revisions&#160;to<br/>the&#160;original&#160;text.&#160;The&#160;verification&#160;process&#160;is&#160;inher-<br/>ently&#160;interpretable,&#160;with&#160;extracted&#160;facts,&#160;generated<br/>questions,&#160;and&#160;retrieved&#160;evidence&#160;directly&#160;reflecting<br/>the&#160;information&#160;units&#160;contributing&#160;to&#160;the&#160;verifica-<br/>tion&#160;process.&#160;For&#160;instance,&#160;FLEEK&#160;would&#160;visually<br/>highlight&#160;verifiable&#160;facts&#160;with&#160;distinct&#160;colors&#160;indi-<br/>cating&#160;their&#160;factuality&#160;levels,&#160;allowing&#160;users&#160;to&#160;in-<br/>teract&#160;with&#160;clickable&#160;highlights&#160;that&#160;reveal&#160;evidence<br/>supporting&#160;or&#160;refuting&#160;each&#160;claim.&#160;Future&#160;work&#160;in-<br/>cludes&#160;comprehensive&#160;evaluations&#160;of&#160;FLEEK,&#160;test-<br/>ing&#160;its&#160;compatibility&#160;with&#160;various&#160;LLMs,&#160;and&#160;sub-<br/>jecting&#160;it&#160;to&#160;a&#160;comprehensive&#160;benchmark.</p>
<p style="position:absolute;top:1020px;left:106px;white-space:nowrap" class="ft82">3.3</p>
<p style="position:absolute;top:1020px;left:143px;white-space:nowrap" class="ft84">Introducing&#160;faithfulness&#160;based&#160;loss<br/>function</p>
<p style="position:absolute;top:1067px;left:106px;white-space:nowrap" class="ft83">Creating&#160;a&#160;metric&#160;to&#160;gauge&#160;how&#160;closely&#160;a&#160;model’s<br/>outputs&#160;match&#160;input&#160;data&#160;or&#160;ground&#160;truth&#160;is&#160;the<br/>task&#160;of&#160;this&#160;section.&#160;In&#160;this&#160;sense,&#160;faithfulness&#160;de-<br/>scribes&#160;the&#160;model’s&#160;capacity&#160;to&#160;faithfully&#160;and&#160;prop-<br/>erly&#160;reflect&#160;data&#160;from&#160;the&#160;input&#160;without&#160;adding&#160;er-</p>
<p style="position:absolute;top:112px;left:459px;white-space:nowrap" class="ft80">rors,&#160;omissions,&#160;or&#160;distortions&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Chrysostomou&#160;and</a></p>
<p style="position:absolute;top:132px;left:459px;white-space:nowrap" class="ft81"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">Aletras,&#160;2021).&#160;</a>The&#160;following&#160;methods&#160;portray&#160;the</p>
<p style="position:absolute;top:152px;left:459px;white-space:nowrap" class="ft80">use&#160;of&#160;technique:</p>
<p style="position:absolute;top:173px;left:476px;white-space:nowrap" class="ft82">Text</p>
<p style="position:absolute;top:173px;left:522px;white-space:nowrap" class="ft82">Hallucination</p>
<p style="position:absolute;top:173px;left:634px;white-space:nowrap" class="ft82">Mitigating</p>
<p style="position:absolute;top:173px;left:725px;white-space:nowrap" class="ft82">(THAM)</p>
<p style="position:absolute;top:193px;left:459px;white-space:nowrap" class="ft82">Framework:&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Yoon&#160;et&#160;al.,&#160;2022)&#160;</a>introduce&#160;the</p>
<p style="position:absolute;top:213px;left:459px;white-space:nowrap" class="ft83">THAM&#160;framework&#160;for&#160;Video-grounded&#160;Dialogue.<br/>THAM&#160;considers&#160;the&#160;text&#160;hallucination&#160;problem,<br/>which&#160;copies&#160;input&#160;texts&#160;for&#160;answer&#160;generation<br/>without&#160;the&#160;understanding&#160;of&#160;the&#160;question.</p>
<p style="position:absolute;top:274px;left:776px;white-space:nowrap" class="ft80">It</p>
<p style="position:absolute;top:295px;left:459px;white-space:nowrap" class="ft83">mitigates&#160;feature-level&#160;hallucination&#160;effects&#160;by<br/>introducing&#160;information-theoretic&#160;regularization.</p>
<p style="position:absolute;top:335px;left:459px;white-space:nowrap" class="ft80">THAM&#160;framework&#160;incorporates&#160;Text&#160;Hallucination</p>
<p style="position:absolute;top:356px;left:459px;white-space:nowrap" class="ft83">Regularization&#160;(THR)&#160;loss&#160;derived&#160;from&#160;the&#160;mutual<br/>information&#160;between&#160;the&#160;response&#160;language&#160;model<br/>and&#160;the&#160;proposed&#160;hallucination&#160;language&#160;model.<br/>Minimizing&#160;THR&#160;loss&#160;contributes&#160;to&#160;reducing<br/>indiscriminate&#160;text&#160;copying&#160;and&#160;boosting&#160;dialogue<br/>performances.&#160;THAM&#160;framework&#160;incorporates</p>
<p style="position:absolute;top:478px;left:459px;white-space:nowrap" class="ft80">Text&#160;Hallucination&#160;Regularization&#160;loss&#160;derived</p>
<p style="position:absolute;top:498px;left:459px;white-space:nowrap" class="ft83">from&#160;the&#160;proposed&#160;information-theoretic&#160;text<br/>hallucination&#160;measurement&#160;approach.<br/>Loss&#160;Weighting&#160;Method:&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Qiu&#160;et&#160;al.,&#160;2023b)&#160;</a>focus<br/>on&#160;low&#160;resource&#160;language&#160;summarization&#160;and<br/>develops&#160;a&#160;novel&#160;metric,&#160;mFACT&#160;to&#160;evaluate&#160;the<br/>faithfulness&#160;of&#160;non-English&#160;summaries,&#160;leveraging<br/>translation-based&#160;transfer&#160;from&#160;multiple&#160;English<br/>faithfulness&#160;metrics.&#160;It&#160;is&#160;developed&#160;from&#160;four<br/>English&#160;faithfulness&#160;metrics.&#160;They&#160;study&#160;hallucina-<br/>tion&#160;in&#160;a&#160;cross-lingual&#160;transfer&#160;setting.&#160;They&#160;apply<br/>mFACT&#160;to&#160;study&#160;the&#160;faithfulness&#160;in&#160;summarisation<br/>of&#160;the&#160;recent&#160;multilingual&#160;LLMs.&#160;The&#160;proposed<br/>metric&#160;consists&#160;of&#160;weighting&#160;training&#160;samples’&#160;loss<br/>based&#160;on&#160;their&#160;faithfulness&#160;score.&#160;The&#160;experiments<br/>show&#160;that&#160;while&#160;common&#160;cross-lingual&#160;transfer<br/>methods&#160;benefit&#160;summarisation&#160;performance,&#160;they<br/>amplify&#160;hallucinations&#160;compared&#160;to&#160;monolingual<br/>counterparts.&#160;To&#160;reduce&#160;these&#160;hallucinations,&#160;they<br/>adapt&#160;several&#160;monolingual&#160;methods&#160;to&#160;cross-lingual<br/>transfer&#160;and&#160;propose&#160;a&#160;new&#160;method&#160;based&#160;on</p>
<p style="position:absolute;top:904px;left:459px;white-space:nowrap" class="ft80">weighting&#160;the&#160;loss&#160;according&#160;to&#160;the&#160;mFACT&#160;score</p>
<p style="position:absolute;top:925px;left:459px;white-space:nowrap" class="ft80">of&#160;each&#160;training&#160;example.</p>
<p style="position:absolute;top:959px;left:459px;white-space:nowrap" class="ft82">3.4</p>
<p style="position:absolute;top:959px;left:496px;white-space:nowrap" class="ft82">Supervised&#160;fine-tuning&#160;(SFT)</p>
<p style="position:absolute;top:986px;left:459px;white-space:nowrap" class="ft83">SFT&#160;serves&#160;as&#160;a&#160;vital&#160;phase&#160;in&#160;aligning&#160;LLMs&#160;for<br/>downstream&#160;tasks&#160;using&#160;labeled&#160;data.&#160;It&#160;helps&#160;the<br/>model&#160;follow&#160;human&#160;commands&#160;for&#160;specific&#160;tasks</p>
<p style="position:absolute;top:1047px;left:459px;white-space:nowrap" class="ft80"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Wang&#160;et&#160;al.,&#160;2023;&#160;</a><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">Chung&#160;et&#160;al.,&#160;2022;&#160;Iyer&#160;et&#160;al.,</a></p>
<p style="position:absolute;top:1067px;left:459px;white-space:nowrap" class="ft83"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">2023;&#160;</a><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">Sun&#160;et&#160;al.,&#160;2023b)&#160;</a>and&#160;eventually&#160;increases<br/>the&#160;faithfulness&#160;of&#160;the&#160;model’s&#160;outputs.&#160;In&#160;the&#160;con-<br/>text&#160;of&#160;SFT,&#160;the&#160;quality&#160;of&#160;the&#160;data&#160;stands&#160;as&#160;the<br/>most&#160;pivotal&#160;concern,&#160;as&#160;it&#160;directly&#160;determines&#160;the<br/>fine-tuned&#160;model’s&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">performance(Xu&#160;et&#160;al.,&#160;2023;</a></p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft90{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft91{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft92{font-size:16px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft93{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft94{font-size:16px;line-height:20px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page9-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html009.png" alt="background image"/>
<p style="position:absolute;top:112px;left:106px;white-space:nowrap" class="ft90"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">Touvron&#160;et&#160;al.,&#160;2023).&#160;</a>During&#160;supervised&#160;fine-</p>
<p style="position:absolute;top:132px;left:106px;white-space:nowrap" class="ft93">tuning,&#160;the&#160;LLM’s&#160;weights&#160;are&#160;adjusted&#160;based&#160;on<br/>the&#160;gradients&#160;from&#160;a&#160;task-specific&#160;loss&#160;function&#160;that<br/>measures&#160;the&#160;difference&#160;between&#160;the&#160;LLM’s&#160;pre-<br/>dictions&#160;and&#160;ground&#160;truth&#160;labels.&#160;This&#160;technique<br/>has&#160;proven&#160;particularly&#160;effective&#160;in&#160;enhancing&#160;the<br/>adaptability&#160;of&#160;LLMs,&#160;enabling&#160;them&#160;to&#160;excel&#160;at<br/>previously&#160;unseen&#160;tasks.</p>
<p style="position:absolute;top:274px;left:123px;white-space:nowrap" class="ft92">Knowledge&#160;Injection&#160;and&#160;Teacher-Student</p>
<p style="position:absolute;top:294px;left:106px;white-space:nowrap" class="ft92">Approaches:&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Elaraby&#160;et&#160;al.,&#160;2023)&#160;</a>focus&#160;on</p>
<p style="position:absolute;top:315px;left:106px;white-space:nowrap" class="ft93">measuring&#160;and&#160;reducing&#160;hallucinations&#160;in&#160;weaker<br/>open-source&#160;large&#160;language&#160;models&#160;(LLMs)&#160;like<br/>BLOOM&#160;7B&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Workshop&#160;et&#160;al.,&#160;2022).&#160;</a>They&#160;in-<br/>troduce&#160;HALOCHECK,&#160;a&#160;lightweight&#160;knowledge-<br/>free&#160;framework&#160;to&#160;quantify&#160;hallucination&#160;severity&#160;in<br/>LLMs.&#160;The&#160;authors&#160;explore&#160;techniques&#160;like&#160;knowl-<br/>edge&#160;injection&#160;and&#160;teacher-student&#160;approaches&#160;to<br/>alleviate&#160;hallucinations&#160;in&#160;low-parameter&#160;LLMs.</p>
<p style="position:absolute;top:477px;left:106px;white-space:nowrap" class="ft91">The&#160;framework&#160;uses&#160;sentence-level&#160;entailment&#160;to</p>
<p style="position:absolute;top:498px;left:106px;white-space:nowrap" class="ft91">quantitatively&#160;assess&#160;hallucination&#160;levels.</p>
<p style="position:absolute;top:518px;left:123px;white-space:nowrap" class="ft91">The&#160;work&#160;aims&#160;to&#160;enhance&#160;smaller&#160;LLM</p>
<p style="position:absolute;top:538px;left:106px;white-space:nowrap" class="ft93">knowledge&#160;through&#160;Knowledge&#160;Injection&#160;(KI)<br/>by&#160;fine-tuning&#160;with&#160;domain&#160;knowledge,&#160;without<br/>relying&#160;on&#160;expensive&#160;instructions&#160;from&#160;stronger<br/>models.&#160;They&#160;investigate&#160;leveraging&#160;a&#160;more&#160;pow-<br/>erful&#160;LLM&#160;like&#160;GPT-4&#160;to&#160;guide&#160;weaker&#160;LLMs&#160;by<br/>generating&#160;detailed&#160;question&#160;answers.&#160;By&#160;assessing<br/>hallucination&#160;severity,&#160;they&#160;optimize&#160;teacher&#160;LLM<br/>engagement&#160;to&#160;reduce&#160;the&#160;computational&#160;costs&#160;of<br/>relying&#160;extensively&#160;on&#160;large&#160;models.&#160;This&#160;alleviates<br/>the&#160;need&#160;for&#160;frequent&#160;queries&#160;to&#160;the&#160;teacher&#160;model.<br/>Hallucination</p>
<p style="position:absolute;top:741px;left:238px;white-space:nowrap" class="ft92">Augmented</p>
<p style="position:absolute;top:741px;left:354px;white-space:nowrap" class="ft92">Recitations</p>
<p style="position:absolute;top:762px;left:106px;white-space:nowrap" class="ft92">(HAR):&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Köksal&#160;et&#160;al.,&#160;2023)&#160;</a>introduce&#160;the</p>
<p style="position:absolute;top:782px;left:106px;white-space:nowrap" class="ft93">concept&#160;of&#160;attribution&#160;in&#160;LLMs&#160;to&#160;control&#160;infor-<br/>mation&#160;sources&#160;and&#160;enhance&#160;factuality.</p>
<p style="position:absolute;top:803px;left:393px;white-space:nowrap" class="ft91">While</p>
<p style="position:absolute;top:823px;left:106px;white-space:nowrap" class="ft93">existing&#160;methods&#160;rely&#160;on&#160;open-book&#160;question<br/>answering&#160;to&#160;improve&#160;attribution,&#160;the&#160;challenge<br/>arises&#160;when&#160;factual&#160;datasets&#160;reward&#160;models&#160;for<br/>recalling&#160;pretraining&#160;data&#160;rather&#160;than&#160;demonstrat-<br/>ing&#160;true&#160;attribution.&#160;To&#160;address&#160;this,&#160;the&#160;authors<br/>propose&#160;HAR,&#160;a&#160;novel&#160;approach&#160;utilizing&#160;LLM<br/>hallucination&#160;to&#160;create&#160;counterfactual&#160;datasets&#160;and<br/>enhance&#160;attribution.&#160;Through&#160;a&#160;case&#160;study&#160;on&#160;open<br/>book&#160;QA,&#160;specifically&#160;CF-TriviaQA,&#160;the&#160;results<br/>demonstrate&#160;that&#160;models&#160;fine-tuned&#160;with&#160;these<br/>counterfactual&#160;datasets&#160;significantly&#160;improve&#160;text<br/>grounding&#160;and&#160;outperform&#160;those&#160;trained&#160;on&#160;factual<br/>datasets,&#160;even&#160;with&#160;smaller&#160;training&#160;datasets&#160;and<br/>model&#160;sizes.</p>
<p style="position:absolute;top:1087px;left:209px;white-space:nowrap" class="ft91">The&#160;observed&#160;improvements&#160;are</p>
<p style="position:absolute;top:1108px;left:106px;white-space:nowrap" class="ft93">consistent&#160;across&#160;various&#160;open-book&#160;QA&#160;tasks,<br/>including&#160;multi-hop,&#160;biomedical,&#160;and&#160;adversarial<br/>questions.</p>
<p style="position:absolute;top:111px;left:459px;white-space:nowrap" class="ft92">Fine-tuning&#160;Language&#160;Models&#160;for&#160;Factuality:</p>
<p style="position:absolute;top:132px;left:459px;white-space:nowrap" class="ft91"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Tian&#160;et&#160;al.,&#160;2023)&#160;</a>address&#160;hallucination&#160;by</p>
<p style="position:absolute;top:152px;left:459px;white-space:nowrap" class="ft93">leveraging&#160;recent&#160;NLP&#160;innovations,&#160;employing<br/>automated&#160;fact-checking&#160;methods&#160;and&#160;preference-<br/>based&#160;learning&#160;through&#160;the&#160;Direct&#160;Preference<br/>Optimization&#160;algorithm.&#160;The&#160;researchers&#160;fine-tune<br/>the&#160;Llama-2&#160;model&#160;for&#160;factuality&#160;without&#160;human<br/>labeling,&#160;achieving&#160;notable&#160;error&#160;reductions,<br/>particularly&#160;in&#160;biographies&#160;and&#160;medical&#160;questions.</p>
<p style="position:absolute;top:295px;left:459px;white-space:nowrap" class="ft91">Their&#160;approach&#160;involves&#160;reference-based&#160;and</p>
<p style="position:absolute;top:315px;left:459px;white-space:nowrap" class="ft93">reference-free&#160;truthfulness&#160;evaluations,&#160;demon-<br/>strating&#160;a&#160;cost-effective&#160;way&#160;to&#160;enhance&#160;model<br/>factuality&#160;in&#160;long-form&#160;text&#160;generation.&#160;The&#160;study<br/>proposes&#160;new&#160;benchmark&#160;tasks,&#160;discusses&#160;future<br/>avenues,&#160;and&#160;highlights&#160;the&#160;potential&#160;scalability&#160;of<br/>factual&#160;reinforcement&#160;learning&#160;for&#160;larger&#160;models&#160;in<br/>safety-critical&#160;domains.<br/>BEINFO:&#160;To&#160;mitigate&#160;the&#160;issue&#160;and&#160;increase&#160;faith-<br/>fulness&#160;of&#160;information-seeking&#160;dialogue&#160;systems,</p>
<p style="position:absolute;top:498px;left:459px;white-space:nowrap" class="ft91"><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Razumovskaia&#160;et&#160;al.,&#160;2023)&#160;</a>introduce&#160;BEINFO,&#160;a</p>
<p style="position:absolute;top:518px;left:459px;white-space:nowrap" class="ft93">simple&#160;yet&#160;effective&#160;method&#160;that&#160;applies&#160;behavioral<br/>tuning&#160;to&#160;aid&#160;information-seeking&#160;dialogue.&#160;In<br/>this&#160;work,&#160;the&#160;authors&#160;propose&#160;BEINFO,&#160;a&#160;simple</p>
<p style="position:absolute;top:579px;left:459px;white-space:nowrap" class="ft91">yet&#160;effective&#160;method&#160;that&#160;applies&#160;‘behavioral</p>
<p style="position:absolute;top:599px;left:459px;white-space:nowrap" class="ft93">finetuning’&#160;to&#160;increase&#160;the&#160;faithfulness&#160;of&#160;the<br/>generated&#160;responses&#160;for&#160;information-seeking<br/>dialogue.&#160;The&#160;model&#160;is&#160;tuned&#160;on&#160;a&#160;large&#160;collection<br/>of&#160;dialogues&#160;with&#160;the&#160;true&#160;knowledge&#160;source(s)<br/>extended&#160;with&#160;randomly&#160;sampled&#160;facts&#160;from&#160;a<br/>large&#160;knowledge&#160;base.<br/>Refusal-Aware&#160;Instruction&#160;Tuning&#160;(R-Tuning):<br/>In&#160;their&#160;recent&#160;work,&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Zhang&#160;et&#160;al.,&#160;2023a)&#160;</a>present<br/>a&#160;novel&#160;approach&#160;called&#160;R-Tuning&#160;for&#160;instilling<br/>refusal&#160;skills&#160;in&#160;large&#160;language&#160;models&#160;(LLMs).</p>
<p style="position:absolute;top:803px;left:459px;white-space:nowrap" class="ft91">This&#160;approach&#160;formalizes&#160;the&#160;idea&#160;of&#160;identifying</p>
<p style="position:absolute;top:823px;left:459px;white-space:nowrap" class="ft93">knowledge&#160;gaps&#160;between&#160;an&#160;LLM’s&#160;parametric<br/>knowledge&#160;and&#160;the&#160;instructional&#160;tuning&#160;data&#160;used<br/>to&#160;train&#160;it.&#160;Based&#160;on&#160;this&#160;knowledge&#160;gap,&#160;R-Tuning<br/>constructs&#160;refusal-aware&#160;training&#160;data&#160;to&#160;teach&#160;the<br/>LLM&#160;when&#160;to&#160;refrain&#160;from&#160;responding,&#160;specifically</p>
<p style="position:absolute;top:925px;left:459px;white-space:nowrap" class="ft91">when&#160;a&#160;question&#160;falls&#160;outside&#160;its&#160;competence.&#160;The</p>
<p style="position:absolute;top:945px;left:459px;white-space:nowrap" class="ft91">R-Tuning&#160;methodology&#160;involves&#160;two&#160;key&#160;steps:</p>
<p style="position:absolute;top:974px;left:472px;white-space:nowrap" class="ft91">1.&#160;Measuring&#160;the&#160;knowledge&#160;gap&#160;between&#160;the</p>
<p style="position:absolute;top:994px;left:492px;white-space:nowrap" class="ft93">LLM’s&#160;parametric&#160;knowledge&#160;and&#160;the&#160;instruc-<br/>tional&#160;tuning&#160;questions,&#160;to&#160;identify&#160;uncertain<br/>questions.&#160;By&#160;inferring&#160;on&#160;the&#160;training&#160;data<br/>once&#160;and&#160;comparing&#160;predictions&#160;to&#160;labels,&#160;the<br/>tuning&#160;data&#160;is&#160;separated&#160;into&#160;uncertain&#160;ques-<br/>tions&#160;and&#160;certain&#160;questions.</p>
<p style="position:absolute;top:1128px;left:472px;white-space:nowrap" class="ft91">2.&#160;Constructing&#160;refusal-aware&#160;training&#160;data&#160;by</p>
<p style="position:absolute;top:1148px;left:492px;white-space:nowrap" class="ft91">appending&#160;refusal&#160;expressions&#160;to&#160;uncertain</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft100{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft101{font-size:16px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft102{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft103{font-size:17px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft104{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft105{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page10-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html010.png" alt="background image"/>
<p style="position:absolute;top:112px;left:139px;white-space:nowrap" class="ft104">training&#160;examples,&#160;before&#160;fine-tuning&#160;the<br/>LLM&#160;on&#160;this&#160;data.</p>
<p style="position:absolute;top:166px;left:106px;white-space:nowrap" class="ft101">Think&#160;While&#160;Effectively&#160;Articulating&#160;Knowl-</p>
<p style="position:absolute;top:186px;left:106px;white-space:nowrap" class="ft104">edge&#160;(TWEAK):&#160;To&#160;reduce&#160;hallucinations,&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Qiu<br/>et&#160;al.,&#160;2023a)&#160;</a>propose&#160;a&#160;new&#160;decoding&#160;method<br/>called&#160;TWEAK.&#160;The&#160;method&#160;treats&#160;the&#160;generated<br/>sequences&#160;at&#160;each&#160;step&#160;and&#160;their&#160;future&#160;sequences<br/>as&#160;hypotheses.&#160;It&#160;ranks&#160;each&#160;generation&#160;candidate<br/>based&#160;on&#160;how&#160;well&#160;their&#160;corresponding&#160;hypotheses<br/>support&#160;the&#160;input&#160;facts,&#160;using&#160;a&#160;Hypothesis&#160;Verifi-<br/>cation&#160;Model&#160;(HVM).</p>
<p style="position:absolute;top:349px;left:123px;white-space:nowrap" class="ft100">The&#160;authors&#160;tweak&#160;only&#160;the&#160;decoding&#160;process</p>
<p style="position:absolute;top:369px;left:106px;white-space:nowrap" class="ft100">without&#160;retraining&#160;the&#160;generative&#160;models.&#160;This</p>
<p style="position:absolute;top:390px;left:106px;white-space:nowrap" class="ft104">makes&#160;their&#160;approach&#160;easily&#160;integrated&#160;with&#160;any<br/>knowledge-to-text&#160;generator.&#160;Existing&#160;decoding<br/>methods&#160;like&#160;beam&#160;search&#160;sample&#160;candidates&#160;only<br/>based&#160;on&#160;predicted&#160;likelihood,&#160;without&#160;considering<br/>faithfulness.&#160;The&#160;authors&#160;propose&#160;a&#160;new&#160;dataset<br/>called&#160;FATE,&#160;which&#160;aligns&#160;input&#160;facts&#160;with&#160;original<br/>and&#160;counterfactual&#160;descriptions&#160;at&#160;the&#160;word&#160;level.</p>
<p style="position:absolute;top:546px;left:106px;white-space:nowrap" class="ft103">4</p>
<p style="position:absolute;top:546px;left:133px;white-space:nowrap" class="ft103">Conclusion</p>
<p style="position:absolute;top:579px;left:106px;white-space:nowrap" class="ft100">This&#160;survey&#160;paper&#160;delves&#160;into&#160;the&#160;critical&#160;is-</p>
<p style="position:absolute;top:599px;left:106px;white-space:nowrap" class="ft100">sue&#160;of&#160;hallucination&#160;in&#160;LLMs,&#160;emphasizing&#160;the</p>
<p style="position:absolute;top:620px;left:106px;white-space:nowrap" class="ft100">widespread&#160;impact&#160;of&#160;LLMs&#160;across&#160;various&#160;do-</p>
<p style="position:absolute;top:640px;left:106px;white-space:nowrap" class="ft104">mains&#160;in&#160;our&#160;lives.&#160;The&#160;paper&#160;highlights&#160;the&#160;chal-<br/>lenge&#160;posed&#160;by&#160;LLMs&#160;generating&#160;incorrect&#160;infor-<br/>mation&#160;and&#160;identifies&#160;it&#160;as&#160;a&#160;significant&#160;concern<br/>for&#160;researchers&#160;working&#160;on&#160;prominent&#160;LLMs&#160;like<br/>GPT-4.&#160;The&#160;paper&#160;explores&#160;recent&#160;advancements&#160;in<br/>the&#160;detection&#160;of&#160;hallucinations,&#160;with&#160;methods&#160;such<br/>as&#160;mFACT,&#160;contextual&#160;information-based&#160;frame-</p>
<p style="position:absolute;top:782px;left:106px;white-space:nowrap" class="ft100">works,&#160;and&#160;the&#160;investigation&#160;of&#160;self-contradiction</p>
<p style="position:absolute;top:803px;left:106px;white-space:nowrap" class="ft104">as&#160;a&#160;contributing&#160;factor.&#160;It&#160;underscores&#160;the&#160;impor-<br/>tance&#160;of&#160;addressing&#160;hallucination&#160;in&#160;LLMs&#160;due&#160;to<br/>their&#160;integral&#160;role&#160;in&#160;critical&#160;tasks.&#160;The&#160;central<br/>contribution&#160;of&#160;the&#160;paper&#160;lies&#160;in&#160;presenting&#160;a&#160;sys-<br/>tematic&#160;taxonomy&#160;for&#160;categorizing&#160;hallucination<br/>mitigation&#160;techniques&#160;in&#160;LLMs,&#160;extending&#160;its&#160;cov-<br/>erage&#160;to&#160;VLMs.&#160;By&#160;synthesizing&#160;essential&#160;features<br/>characterizing&#160;these&#160;techniques,&#160;the&#160;paper&#160;provides<br/>a&#160;foundation&#160;for&#160;more&#160;structured&#160;future&#160;research</p>
<p style="position:absolute;top:985px;left:106px;white-space:nowrap" class="ft100">within&#160;the&#160;domain&#160;of&#160;hallucination&#160;mitigation.&#160;Ad-</p>
<p style="position:absolute;top:1006px;left:106px;white-space:nowrap" class="ft104">ditionally,&#160;the&#160;paper&#160;deliberates&#160;on&#160;the&#160;inherent<br/>limitations&#160;and&#160;challenges&#160;associated&#160;with&#160;these<br/>techniques,&#160;proposing&#160;directions&#160;for&#160;future&#160;research<br/>in&#160;this&#160;area.</p>
<p style="position:absolute;top:1087px;left:123px;white-space:nowrap" class="ft100">In&#160;essence,&#160;this&#160;survey&#160;paper&#160;not&#160;only&#160;sheds&#160;light</p>
<p style="position:absolute;top:1108px;left:106px;white-space:nowrap" class="ft104">on&#160;the&#160;gravity&#160;of&#160;hallucination&#160;in&#160;LLMs&#160;but&#160;also<br/>consolidates&#160;and&#160;organizes&#160;diverse&#160;mitigation&#160;tech-<br/>niques,&#160;contributing&#160;to&#160;the&#160;advancement&#160;of&#160;knowl-</p>
<p style="position:absolute;top:112px;left:459px;white-space:nowrap" class="ft104">edge&#160;in&#160;the&#160;field&#160;of&#160;computational&#160;linguistics.&#160;It<br/>serves&#160;as&#160;a&#160;valuable&#160;resource&#160;for&#160;researchers&#160;and<br/>practitioners&#160;seeking&#160;a&#160;comprehensive&#160;understand-<br/>ing&#160;of&#160;the&#160;current&#160;landscape&#160;of&#160;hallucination&#160;in<br/>LLMs&#160;and&#160;the&#160;strategies&#160;employed&#160;to&#160;address&#160;this<br/>pressing&#160;issue.</p>
<p style="position:absolute;top:252px;left:459px;white-space:nowrap" class="ft103">5</p>
<p style="position:absolute;top:252px;left:486px;white-space:nowrap" class="ft103">Discussion&#160;and&#160;Limitations</p>
<p style="position:absolute;top:288px;left:459px;white-space:nowrap" class="ft104">Hallucination&#160;mitigation&#160;in&#160;LLMs&#160;represents&#160;a&#160;mul-<br/>tifaceted&#160;challenge&#160;addressed&#160;through&#160;a&#160;spectrum<br/>of&#160;innovative&#160;techniques.&#160;The&#160;methodologies&#160;dis-<br/>cussed,&#160;ranging&#160;from&#160;post-generation&#160;refinement&#160;to<br/>supervised&#160;fine-tuning,&#160;underscore&#160;the&#160;gravity&#160;of<br/>the&#160;hallucination&#160;issue&#160;and&#160;the&#160;pressing&#160;need&#160;for<br/>comprehensive&#160;solutions.</p>
<p style="position:absolute;top:432px;left:476px;white-space:nowrap" class="ft100">In&#160;the&#160;realm&#160;of&#160;post-generation&#160;refinement,</p>
<p style="position:absolute;top:452px;left:459px;white-space:nowrap" class="ft104">RARR&#160;stands&#160;out,&#160;automating&#160;the&#160;attribution&#160;pro-<br/>cess&#160;and&#160;aligning&#160;content&#160;with&#160;retrieved&#160;evidence.<br/>High&#160;Entropy&#160;Word&#160;Spotting&#160;and&#160;Replacement<br/>tackles&#160;hallucinations&#160;induced&#160;by&#160;high-entropy</p>
<p style="position:absolute;top:533px;left:459px;white-space:nowrap" class="ft100">words&#160;in&#160;LLM-generated&#160;content,&#160;showcasing&#160;the</p>
<p style="position:absolute;top:554px;left:459px;white-space:nowrap" class="ft100">significance&#160;of&#160;context-aware&#160;replacements.</p>
<p style="position:absolute;top:575px;left:476px;white-space:nowrap" class="ft100">Self-refinement&#160;through&#160;feedback&#160;and&#160;reason-</p>
<p style="position:absolute;top:595px;left:459px;white-space:nowrap" class="ft104">ing&#160;brings&#160;forth&#160;impactful&#160;strategies&#160;like&#160;ChatPro-<br/>tect,&#160;focusing&#160;on&#160;self-contradiction&#160;detection,&#160;and<br/>Self-Reflection&#160;Methodology,&#160;employing&#160;an&#160;itera-<br/>tive&#160;feedback&#160;process&#160;for&#160;hallucination&#160;reduction&#160;in<br/>medical&#160;generative&#160;QA&#160;systems.&#160;Structured&#160;Com-<br/>parative&#160;reasoning&#160;introduces&#160;a&#160;structured&#160;approach<br/>to&#160;text&#160;preference&#160;prediction,&#160;enhancing&#160;coherence<br/>and&#160;reducing&#160;hallucination.</p>
<p style="position:absolute;top:759px;left:476px;white-space:nowrap" class="ft100">Prompt&#160;tuning&#160;emerges&#160;as&#160;a&#160;powerful&#160;technique,</p>
<p style="position:absolute;top:780px;left:459px;white-space:nowrap" class="ft104">with&#160;innovations&#160;like&#160;UPRISE&#160;demonstrating&#160;the<br/>versatility&#160;of&#160;prompt-based&#160;adjustments.&#160;SynTra</p>
<p style="position:absolute;top:820px;left:459px;white-space:nowrap" class="ft104">introduces&#160;synthetic&#160;tasks&#160;for&#160;mitigating&#160;hallucina-<br/>tions&#160;in&#160;abstractive&#160;summarization,&#160;offering&#160;scal-<br/>ability&#160;but&#160;raising&#160;questions&#160;about&#160;effectiveness<br/>compared&#160;to&#160;human&#160;feedback.</p>
<p style="position:absolute;top:903px;left:476px;white-space:nowrap" class="ft100">The&#160;development&#160;of&#160;novel&#160;models&#160;emphasizes</p>
<p style="position:absolute;top:923px;left:459px;white-space:nowrap" class="ft104">decoding&#160;strategies&#160;such&#160;as&#160;CAD&#160;and&#160;DoLa,&#160;both<br/>instrumental&#160;in&#160;reducing&#160;hallucinations&#160;by&#160;guid-<br/>ing&#160;the&#160;generation&#160;phase.</p>
<p style="position:absolute;top:964px;left:655px;white-space:nowrap" class="ft100">KG&#160;utilization&#160;and</p>
<p style="position:absolute;top:984px;left:459px;white-space:nowrap" class="ft104">faithfulness-based&#160;loss&#160;functions&#160;also&#160;play&#160;crucial<br/>roles,&#160;as&#160;seen&#160;in&#160;methods&#160;like&#160;RHO&#160;and&#160;THAM<br/>Framework.</p>
<p style="position:absolute;top:1047px;left:476px;white-space:nowrap" class="ft100">Supervised&#160;fine-tuning,&#160;a&#160;pivotal&#160;phase,&#160;is&#160;ex-</p>
<p style="position:absolute;top:1067px;left:459px;white-space:nowrap" class="ft104">plored&#160;through&#160;various&#160;lenses,&#160;such&#160;as&#160;Knowledge<br/>Injection&#160;and&#160;Teacher-Student&#160;Approaches,&#160;where<br/>domain-specific&#160;knowledge&#160;is&#160;injected&#160;into&#160;weaker<br/>LLMs&#160;and&#160;approaches&#160;like&#160;HAR&#160;employ&#160;counter-<br/>factual&#160;datasets&#160;for&#160;improved&#160;factuality.</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft110{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft111{font-size:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft112{font-size:17px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft113{font-size:14px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft114{font-size:14px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft115{font-size:14px;font-family:YOPPJI+NimbusRomNo9L-ReguItal;color:#000000;}
	.ft116{font-size:16px;line-height:20px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft117{font-size:14px;line-height:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft118{font-size:14px;line-height:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft119{font-size:14px;line-height:16px;font-family:YOPPJI+NimbusRomNo9L-ReguItal;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page11-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html011.png" alt="background image"/>
<p style="position:absolute;top:112px;left:123px;white-space:nowrap" class="ft110">Future&#160;developments&#160;and&#160;improvements&#160;in&#160;a&#160;va-</p>
<p style="position:absolute;top:132px;left:106px;white-space:nowrap" class="ft116">riety&#160;of&#160;areas&#160;are&#160;anticipated&#160;for&#160;language&#160;models’<br/>approach&#160;to&#160;hallucination&#160;mitigation.&#160;The&#160;creation<br/>of&#160;hybrid&#160;models,&#160;which&#160;offer&#160;a&#160;thorough&#160;defense<br/>against&#160;hallucinations&#160;by&#160;seamlessly&#160;integrating&#160;nu-<br/>merous&#160;mitigation&#160;approaches,&#160;is&#160;one&#160;important<br/>direction.&#160;By&#160;reducing&#160;reliance&#160;on&#160;labeled&#160;data,<br/>investigating&#160;the&#160;possibilities&#160;of&#160;unsupervised&#160;or</p>
<p style="position:absolute;top:274px;left:106px;white-space:nowrap" class="ft110">weakly&#160;supervised&#160;learning&#160;techniques&#160;might&#160;im-</p>
<p style="position:absolute;top:295px;left:106px;white-space:nowrap" class="ft116">prove&#160;scalability&#160;and&#160;flexibility.&#160;In&#160;addition,&#160;it&#160;will<br/>be&#160;essential&#160;to&#160;look&#160;into&#160;the&#160;moral&#160;ramifications&#160;and<br/>societal&#160;effects&#160;of&#160;hallucination&#160;mitigation&#160;strate-<br/>gies&#160;to&#160;guarantee&#160;responsible&#160;implementation&#160;and<br/>promote&#160;user&#160;confidence.&#160;Research&#160;on&#160;designs<br/>specifically&#160;intended&#160;to&#160;reduce&#160;hallucinations&#160;is<br/>further&#160;encouraged&#160;by&#160;the&#160;changing&#160;field&#160;of&#160;LLMs,</p>
<p style="position:absolute;top:437px;left:106px;white-space:nowrap" class="ft116">which&#160;could&#160;lead&#160;to&#160;the&#160;development&#160;of&#160;new&#160;models<br/>with&#160;built-in&#160;safety&#160;features.&#160;It&#160;will&#160;be&#160;crucial&#160;for</p>
<p style="position:absolute;top:477px;left:106px;white-space:nowrap" class="ft116">researchers,&#160;business&#160;professionals,&#160;and&#160;ethicists<br/>to&#160;work&#160;together&#160;continuously&#160;to&#160;improve&#160;methods,<br/>benchmark&#160;models,&#160;and&#160;set&#160;standards&#160;that&#160;put&#160;user<br/>comprehension&#160;and&#160;authenticity&#160;first.&#160;The&#160;building<br/>of&#160;language&#160;models&#160;that&#160;produce&#160;coherent&#160;and&#160;con-<br/>textually&#160;relevant&#160;information&#160;while&#160;simultaneously<br/>demonstrating&#160;heightened&#160;awareness&#160;and&#160;mitiga-<br/>tion&#160;of&#160;hallucinatory&#160;outputs&#160;is&#160;the&#160;field’s&#160;collective<br/>goal&#160;as&#160;it&#160;navigates&#160;these&#160;future&#160;possibilities.</p>
<p style="position:absolute;top:661px;left:123px;white-space:nowrap" class="ft110">The&#160;collected&#160;works&#160;on&#160;hallucination&#160;mitigation</p>
<p style="position:absolute;top:682px;left:106px;white-space:nowrap" class="ft116">reveal&#160;a&#160;diverse&#160;array&#160;of&#160;strategies,&#160;each&#160;contribut-<br/>ing&#160;uniquely&#160;to&#160;address&#160;the&#160;nuances&#160;of&#160;hallucina-<br/>tion&#160;in&#160;LLMs.&#160;As&#160;the&#160;field&#160;evolves,&#160;the&#160;synthesis&#160;of<br/>these&#160;approaches&#160;could&#160;pave&#160;the&#160;way&#160;for&#160;more&#160;ro-<br/>bust&#160;and&#160;universally&#160;applicable&#160;solutions,&#160;fostering<br/>trust&#160;and&#160;reliability&#160;in&#160;language&#160;generation&#160;systems.</p>
<p style="position:absolute;top:805px;left:123px;white-space:nowrap" class="ft110">Finally,&#160;the&#160;division&#160;of&#160;the&#160;mitigation&#160;techniques</p>
<p style="position:absolute;top:825px;left:106px;white-space:nowrap" class="ft116">surveyed&#160;can&#160;be&#160;easily&#160;comprehensible&#160;through<br/>table&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#14">1.</a></p>
<p style="position:absolute;top:902px;left:106px;white-space:nowrap" class="ft112">References</p>
<p style="position:absolute;top:933px;left:106px;white-space:nowrap" class="ft113">Farima&#160;Fatahi&#160;Bayat,&#160;Kun&#160;Qian,&#160;Benjamin&#160;Han,&#160;Yisi</p>
<p style="position:absolute;top:950px;left:123px;white-space:nowrap" class="ft118">Sang,&#160;Anton&#160;Belyi,&#160;Samira&#160;Khorshidi,&#160;Fei&#160;Wu,&#160;Ihab&#160;F.<br/>Ilyas,&#160;and&#160;Yunyao&#160;Li.&#160;2023.&#160;<a href="http://arxiv.org/abs/2310.17119">Fleek:&#160;Factual&#160;error<br/>detection&#160;and&#160;correction&#160;with&#160;evidence&#160;retrieved&#160;from<br/>external&#160;knowledge.</a></p>
<p style="position:absolute;top:1033px;left:106px;white-space:nowrap" class="ft113">Hejing&#160;Cao,&#160;Zhenwei&#160;An,&#160;Jiazhan&#160;Feng,&#160;Kun&#160;Xu,&#160;Li-</p>
<p style="position:absolute;top:1049px;left:122px;white-space:nowrap" class="ft113">wei&#160;Chen,&#160;and&#160;Dongyan&#160;Zhao.&#160;2023.&#160;<a href="http://arxiv.org/abs/2311.07491">A&#160;step&#160;closer</a></p>
<p style="position:absolute;top:1066px;left:123px;white-space:nowrap" class="ft118"><a href="http://arxiv.org/abs/2311.07491">to&#160;comprehensive&#160;answers:&#160;Constrained&#160;multi-stage<br/>question&#160;decomposition&#160;with&#160;large&#160;language&#160;models.</a></p>
<p style="position:absolute;top:1116px;left:106px;white-space:nowrap" class="ft113">Yangyi&#160;Chen,&#160;Karan&#160;Sikka,&#160;Michael&#160;Cogswell,&#160;Heng</p>
<p style="position:absolute;top:1133px;left:122px;white-space:nowrap" class="ft117">Ji,&#160;and&#160;Ajay&#160;Divakaran.&#160;2023.&#160;Dress:&#160;Instructing<br/>large&#160;vision-language&#160;models&#160;to&#160;align&#160;and&#160;interact</p>
<p style="position:absolute;top:113px;left:475px;white-space:nowrap" class="ft113">with&#160;humans&#160;via&#160;natural&#160;language&#160;feedback.&#160;arXiv</p>
<p style="position:absolute;top:129px;left:476px;white-space:nowrap" class="ft115">preprint&#160;arXiv:2311.10081</p>
<p style="position:absolute;top:129px;left:637px;white-space:nowrap" class="ft113">.</p>
<p style="position:absolute;top:159px;left:459px;white-space:nowrap" class="ft113">Daixuan&#160;Cheng,&#160;Shaohan&#160;Huang,&#160;Junyu&#160;Bi,&#160;Yuefeng</p>
<p style="position:absolute;top:175px;left:476px;white-space:nowrap" class="ft118">Zhan,&#160;Jianfeng&#160;Liu,&#160;Yujing&#160;Wang,&#160;Hao&#160;Sun,&#160;Furu&#160;Wei,<br/>Denvy&#160;Deng,&#160;and&#160;Qi&#160;Zhang.&#160;2023.&#160;<a href="http://arxiv.org/abs/2303.08518">Uprise:&#160;Universal<br/>prompt&#160;retrieval&#160;for&#160;improving&#160;zero-shot&#160;evaluation.</a></p>
<p style="position:absolute;top:238px;left:459px;white-space:nowrap" class="ft113">George&#160;Chrysostomou&#160;and&#160;Nikolaos&#160;Aletras.&#160;2021.&#160;En-</p>
<p style="position:absolute;top:254px;left:476px;white-space:nowrap" class="ft117">joy&#160;the&#160;salience:&#160;Towards&#160;better&#160;transformer-based<br/>faithful&#160;explanations&#160;with&#160;word&#160;salience.</p>
<p style="position:absolute;top:271px;left:753px;white-space:nowrap" class="ft115">arXiv</p>
<p style="position:absolute;top:287px;left:476px;white-space:nowrap" class="ft115">preprint&#160;arXiv:2108.13759</p>
<p style="position:absolute;top:287px;left:637px;white-space:nowrap" class="ft113">.</p>
<p style="position:absolute;top:317px;left:459px;white-space:nowrap" class="ft113">Yung-Sung&#160;Chuang,&#160;Yujia&#160;Xie,&#160;Hongyin&#160;Luo,&#160;Yoon</p>
<p style="position:absolute;top:333px;left:476px;white-space:nowrap" class="ft118">Kim,&#160;James&#160;Glass,&#160;and&#160;Pengcheng&#160;He.&#160;2023.&#160;<a href="http://arxiv.org/abs/2309.03883">Dola:<br/>Decoding&#160;by&#160;contrasting&#160;layers&#160;improves&#160;factuality<br/>in&#160;large&#160;language&#160;models.</a></p>
<p style="position:absolute;top:396px;left:459px;white-space:nowrap" class="ft113">Hyung&#160;Won&#160;Chung,&#160;Le&#160;Hou,&#160;Shayne&#160;Longpre,&#160;Barret</p>
<p style="position:absolute;top:412px;left:476px;white-space:nowrap" class="ft113">Zoph,&#160;Yi&#160;Tay,&#160;William&#160;Fedus,&#160;Yunxuan&#160;Li,&#160;Xuezhi</p>
<p style="position:absolute;top:429px;left:475px;white-space:nowrap" class="ft113">Wang,&#160;Mostafa&#160;Dehghani,&#160;Siddhartha&#160;Brahma,&#160;Al-</p>
<p style="position:absolute;top:445px;left:476px;white-space:nowrap" class="ft117">bert&#160;Webson,&#160;Shixiang&#160;Shane&#160;Gu,&#160;Zhuyun&#160;Dai,<br/>Mirac&#160;Suzgun,&#160;Xinyun&#160;Chen,&#160;Aakanksha&#160;Chowdh-<br/>ery,&#160;Alex&#160;Castro-Ros,&#160;Marie&#160;Pellat,&#160;Kevin&#160;Robinson,<br/>Dasha&#160;Valter,&#160;Sharan&#160;Narang,&#160;Gaurav&#160;Mishra,&#160;Adams</p>
<p style="position:absolute;top:511px;left:475px;white-space:nowrap" class="ft113">Yu,&#160;Vincent&#160;Zhao,&#160;Yanping&#160;Huang,&#160;Andrew&#160;Dai,</p>
<p style="position:absolute;top:527px;left:476px;white-space:nowrap" class="ft118">Hongkun&#160;Yu,&#160;Slav&#160;Petrov,&#160;Ed&#160;H.&#160;Chi,&#160;Jeff&#160;Dean,&#160;Ja-<br/>cob&#160;Devlin,&#160;Adam&#160;Roberts,&#160;Denny&#160;Zhou,&#160;Quoc&#160;V.&#160;Le,<br/>and&#160;Jason&#160;Wei.&#160;2022.&#160;<a href="http://arxiv.org/abs/2210.11416">Scaling&#160;instruction-finetuned<br/>language&#160;models.</a></p>
<p style="position:absolute;top:606px;left:459px;white-space:nowrap" class="ft113">Shehzaad&#160;Dhuliawala,&#160;Mojtaba&#160;Komeili,&#160;Jing&#160;Xu,</p>
<p style="position:absolute;top:623px;left:476px;white-space:nowrap" class="ft118">Roberta&#160;Raileanu,&#160;Xian&#160;Li,&#160;Asli&#160;Celikyilmaz,&#160;and<br/>Jason&#160;Weston.&#160;2023.&#160;<a href="http://arxiv.org/abs/2309.11495">Chain-of-verification&#160;reduces<br/>hallucination&#160;in&#160;large&#160;language&#160;models.</a></p>
<p style="position:absolute;top:685px;left:459px;white-space:nowrap" class="ft113">Mohamed&#160;Elaraby,&#160;Mengyin&#160;Lu,&#160;Jacob&#160;Dunn,&#160;Xueying</p>
<p style="position:absolute;top:702px;left:476px;white-space:nowrap" class="ft118">Zhang,&#160;Yu&#160;Wang,&#160;Shizhu&#160;Liu,&#160;Pingchuan&#160;Tian,&#160;Yup-<br/>ing&#160;Wang,&#160;and&#160;Yuxuan&#160;Wang.&#160;2023.&#160;<a href="http://arxiv.org/abs/2308.11764">Halo:&#160;Estima-<br/>tion&#160;and&#160;reduction&#160;of&#160;hallucinations&#160;in&#160;open-source</a></p>
<p style="position:absolute;top:751px;left:475px;white-space:nowrap" class="ft114"><a href="http://arxiv.org/abs/2308.11764">weak&#160;large&#160;language&#160;models.</a></p>
<p style="position:absolute;top:781px;left:459px;white-space:nowrap" class="ft113">Philip&#160;Feldman,&#160;James&#160;R.&#160;Foulds,&#160;and&#160;Shimei&#160;Pan.&#160;2023.</p>
<p style="position:absolute;top:797px;left:475px;white-space:nowrap" class="ft114"><a href="http://arxiv.org/abs/2306.06085">Trapping&#160;llm&#160;hallucinations&#160;using&#160;tagged&#160;context</a></p>
<p style="position:absolute;top:814px;left:476px;white-space:nowrap" class="ft114"><a href="http://arxiv.org/abs/2306.06085">prompts.</a></p>
<p style="position:absolute;top:843px;left:459px;white-space:nowrap" class="ft113">Luyu&#160;Gao,&#160;Zhuyun&#160;Dai,&#160;Panupong&#160;Pasupat,&#160;Anthony</p>
<p style="position:absolute;top:860px;left:476px;white-space:nowrap" class="ft119">Chen,&#160;Arun&#160;Tejasvi&#160;Chaganty,&#160;Yicheng&#160;Fan,&#160;Vincent<br/>Zhao,&#160;Ni&#160;Lao,&#160;Hongrae&#160;Lee,&#160;Da-Cheng&#160;Juan,&#160;et&#160;al.<br/>2023.&#160;Rarr:&#160;Researching&#160;and&#160;revising&#160;what&#160;language<br/>models&#160;say,&#160;using&#160;language&#160;models.&#160;In&#160;Proceedings<br/>of&#160;the&#160;61st&#160;Annual&#160;Meeting&#160;of&#160;the&#160;Association&#160;for</p>
<p style="position:absolute;top:942px;left:475px;white-space:nowrap" class="ft115">Computational&#160;Linguistics&#160;(Volume&#160;1:&#160;Long&#160;Papers)</p>
<p style="position:absolute;top:942px;left:785px;white-space:nowrap" class="ft113">,</p>
<p style="position:absolute;top:958px;left:476px;white-space:nowrap" class="ft113">pages&#160;16477–16508.</p>
<p style="position:absolute;top:988px;left:459px;white-space:nowrap" class="ft113">Srinivasan&#160;Iyer,&#160;Xi&#160;Victoria&#160;Lin,&#160;Ramakanth&#160;Pasunuru,</p>
<p style="position:absolute;top:1004px;left:475px;white-space:nowrap" class="ft117">Todor&#160;Mihaylov,&#160;Daniel&#160;Simig,&#160;Ping&#160;Yu,&#160;Kurt&#160;Shuster,<br/>Tianlu&#160;Wang,&#160;Qing&#160;Liu,&#160;Punit&#160;Singh&#160;Koura,&#160;Xian&#160;Li,</p>
<p style="position:absolute;top:1037px;left:476px;white-space:nowrap" class="ft118">Brian&#160;O’Horo,&#160;Gabriel&#160;Pereyra,&#160;Jeff&#160;Wang,&#160;Christo-<br/>pher&#160;Dewan,&#160;Asli&#160;Celikyilmaz,&#160;Luke&#160;Zettlemoyer,<br/>and&#160;Ves&#160;Stoyanov.&#160;2023.&#160;<a href="http://arxiv.org/abs/2212.12017">Opt-iml:&#160;Scaling&#160;language<br/>model&#160;instruction&#160;meta&#160;learning&#160;through&#160;the&#160;lens&#160;of<br/>generalization.</a></p>
<p style="position:absolute;top:1133px;left:459px;white-space:nowrap" class="ft113">Ziwei&#160;Ji,&#160;Zihan&#160;Liu,&#160;Nayeon&#160;Lee,&#160;Tiezheng&#160;Yu,&#160;Bryan</p>
<p style="position:absolute;top:1149px;left:475px;white-space:nowrap" class="ft113">Wilie,&#160;Min&#160;Zeng,&#160;and&#160;Pascale&#160;Fung.&#160;2023a.&#160;<a href="https://doi.org/10.18653/v1/2023.findings-acl.275">RHO:</a></p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft120{font-size:14px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft121{font-size:14px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft122{font-size:14px;font-family:YOPPJI+NimbusRomNo9L-ReguItal;color:#000000;}
	.ft123{font-size:14px;font-family:YOPPJI+NimbusRomNo9L-ReguItal;color:#00007f;}
	.ft124{font-size:14px;line-height:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft125{font-size:14px;line-height:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft126{font-size:14px;line-height:16px;font-family:YOPPJI+NimbusRomNo9L-ReguItal;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page12-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html012.png" alt="background image"/>
<p style="position:absolute;top:113px;left:123px;white-space:nowrap" class="ft120"><a href="https://doi.org/10.18653/v1/2023.findings-acl.275">Reducing&#160;hallucination&#160;in&#160;open-domain&#160;dialogues</a></p>
<p style="position:absolute;top:129px;left:122px;white-space:nowrap" class="ft120"><a href="https://doi.org/10.18653/v1/2023.findings-acl.275">with&#160;knowledge&#160;grounding.&#160;</a>In&#160;Findings&#160;of&#160;the&#160;As-</p>
<p style="position:absolute;top:146px;left:123px;white-space:nowrap" class="ft122">sociation&#160;for&#160;Computational&#160;Linguistics:&#160;ACL&#160;2023</p>
<p style="position:absolute;top:145px;left:432px;white-space:nowrap" class="ft121">,</p>
<p style="position:absolute;top:162px;left:123px;white-space:nowrap" class="ft124">pages&#160;4504–4522,&#160;Toronto,&#160;Canada.&#160;Association&#160;for<br/>Computational&#160;Linguistics.</p>
<p style="position:absolute;top:207px;left:106px;white-space:nowrap" class="ft121">Ziwei&#160;Ji,&#160;Tiezheng&#160;Yu,&#160;Yan&#160;Xu,&#160;Nayeon&#160;Lee,&#160;Etsuko</p>
<p style="position:absolute;top:223px;left:123px;white-space:nowrap" class="ft125">Ishii,&#160;and&#160;Pascale&#160;Fung.&#160;2023b.&#160;<a href="http://arxiv.org/abs/2310.06271">Towards&#160;mitigat-<br/>ing&#160;hallucination&#160;in&#160;large&#160;language&#160;models&#160;via&#160;self-<br/>reflection.</a></p>
<p style="position:absolute;top:285px;left:106px;white-space:nowrap" class="ft121">Erik&#160;Jones,&#160;Hamid&#160;Palangi,&#160;Clarisse&#160;Simões,&#160;Varun</p>
<p style="position:absolute;top:301px;left:123px;white-space:nowrap" class="ft124">Chandrasekaran,&#160;Subhabrata&#160;Mukherjee,&#160;Arindam<br/>Mitra,&#160;Ahmed&#160;Awadallah,&#160;and&#160;Ece&#160;Kamar.&#160;2023.</p>
<p style="position:absolute;top:334px;left:122px;white-space:nowrap" class="ft120"><a href="http://arxiv.org/abs/2310.06827">Teaching&#160;language&#160;models&#160;to&#160;hallucinate&#160;less&#160;with</a></p>
<p style="position:absolute;top:350px;left:123px;white-space:nowrap" class="ft120"><a href="http://arxiv.org/abs/2310.06827">synthetic&#160;tasks.</a></p>
<p style="position:absolute;top:379px;left:106px;white-space:nowrap" class="ft121">Haoqiang&#160;Kang,&#160;Juntong&#160;Ni,&#160;and&#160;Huaxiu&#160;Yao.&#160;2023.</p>
<p style="position:absolute;top:395px;left:123px;white-space:nowrap" class="ft125"><a href="http://arxiv.org/abs/2311.09114">Ever:&#160;Mitigating&#160;hallucination&#160;in&#160;large&#160;language&#160;mod-<br/>els&#160;through&#160;real-time&#160;verification&#160;and&#160;rectification.</a></p>
<p style="position:absolute;top:440px;left:106px;white-space:nowrap" class="ft121">Abdullatif&#160;Köksal,&#160;Renat&#160;Aksitov,&#160;and&#160;Chung-Ching</p>
<p style="position:absolute;top:457px;left:123px;white-space:nowrap" class="ft125">Chang.&#160;2023.&#160;<a href="http://arxiv.org/abs/2311.07424">Hallucination&#160;augmented&#160;recitations<br/>for&#160;language&#160;models.</a></p>
<p style="position:absolute;top:502px;left:106px;white-space:nowrap" class="ft121">Zhenzhong&#160;Lan,&#160;Mingda&#160;Chen,&#160;Sebastian&#160;Goodman,</p>
<p style="position:absolute;top:518px;left:123px;white-space:nowrap" class="ft126">Kevin&#160;Gimpel,&#160;Piyush&#160;Sharma,&#160;and&#160;Radu&#160;Soricut.<br/>2020.&#160;<a href="https://openreview.net/forum?id=H1eA7AEtvS">Albert:&#160;A&#160;lite&#160;bert&#160;for&#160;self-supervised&#160;learning<br/>of&#160;language&#160;representations.&#160;</a>In&#160;International&#160;Confer-<br/>ence&#160;on&#160;Learning&#160;Representations</p>
<p style="position:absolute;top:567px;left:327px;white-space:nowrap" class="ft121">.</p>
<p style="position:absolute;top:596px;left:106px;white-space:nowrap" class="ft121">Mateusz&#160;Lango&#160;and&#160;Ondrej&#160;Dusek.&#160;2023.&#160;<a href="https://doi.org/10.18653/v1/2023.emnlp-main.172">Critic-driven</a></p>
<p style="position:absolute;top:612px;left:123px;white-space:nowrap" class="ft126"><a href="https://doi.org/10.18653/v1/2023.emnlp-main.172">decoding&#160;for&#160;mitigating&#160;hallucinations&#160;in&#160;data-to-text<br/>generation.&#160;</a>In&#160;Proceedings&#160;of&#160;the&#160;2023&#160;Conference<br/>on&#160;Empirical&#160;Methods&#160;in&#160;Natural&#160;Language&#160;Process-<br/>ing</p>
<p style="position:absolute;top:662px;left:142px;white-space:nowrap" class="ft121">,&#160;pages&#160;2853–2862,&#160;Singapore.&#160;Association&#160;for</p>
<p style="position:absolute;top:678px;left:123px;white-space:nowrap" class="ft121">Computational&#160;Linguistics.</p>
<p style="position:absolute;top:706px;left:106px;white-space:nowrap" class="ft121">Deren&#160;Lei,&#160;Yaxi&#160;Li,&#160;Mengya&#160;Hu,&#160;Mingyu&#160;Wang,&#160;Vincent</p>
<p style="position:absolute;top:723px;left:122px;white-space:nowrap" class="ft121">Yun,&#160;Emily&#160;Ching,&#160;and&#160;Eslam&#160;Kamal.&#160;2023.&#160;<a href="http://arxiv.org/abs/2310.03951">Chain</a></p>
<p style="position:absolute;top:739px;left:123px;white-space:nowrap" class="ft125"><a href="http://arxiv.org/abs/2310.03951">of&#160;natural&#160;language&#160;inference&#160;for&#160;reducing&#160;large&#160;lan-<br/>guage&#160;model&#160;ungrounded&#160;hallucinations.</a></p>
<p style="position:absolute;top:784px;left:106px;white-space:nowrap" class="ft121">Brian&#160;Lester,&#160;Rami&#160;Al-Rfou,&#160;and&#160;Noah&#160;Constant.&#160;2021.</p>
<p style="position:absolute;top:801px;left:122px;white-space:nowrap" class="ft120"><a href="https://doi.org/10.18653/v1/2021.emnlp-main.243">The&#160;power&#160;of&#160;scale&#160;for&#160;parameter-efficient&#160;prompt</a></p>
<p style="position:absolute;top:817px;left:123px;white-space:nowrap" class="ft120"><a href="https://doi.org/10.18653/v1/2021.emnlp-main.243">tuning.&#160;</a>In&#160;Proceedings&#160;of&#160;the&#160;2021&#160;Conference&#160;on</p>
<p style="position:absolute;top:834px;left:122px;white-space:nowrap" class="ft122">Empirical&#160;Methods&#160;in&#160;Natural&#160;Language&#160;Processing</p>
<p style="position:absolute;top:834px;left:432px;white-space:nowrap" class="ft121">,</p>
<p style="position:absolute;top:850px;left:123px;white-space:nowrap" class="ft124">pages&#160;3045–3059,&#160;Online&#160;and&#160;Punta&#160;Cana,&#160;Domini-<br/>can&#160;Republic.&#160;Association&#160;for&#160;Computational&#160;Lin-<br/>guistics.</p>
<p style="position:absolute;top:911px;left:106px;white-space:nowrap" class="ft121">Patrick&#160;Lewis,</p>
<p style="position:absolute;top:911px;left:215px;white-space:nowrap" class="ft121">Ethan&#160;Perez,</p>
<p style="position:absolute;top:911px;left:313px;white-space:nowrap" class="ft121">Aleksandra&#160;Piktus,</p>
<p style="position:absolute;top:928px;left:123px;white-space:nowrap" class="ft124">Fabio&#160;Petroni,&#160;Vladimir&#160;Karpukhin,&#160;Naman&#160;Goyal,<br/>Heinrich&#160;Küttler,</p>
<p style="position:absolute;top:944px;left:248px;white-space:nowrap" class="ft121">Mike&#160;Lewis,</p>
<p style="position:absolute;top:944px;left:347px;white-space:nowrap" class="ft121">Wen-tau&#160;Yih,</p>
<p style="position:absolute;top:961px;left:122px;white-space:nowrap" class="ft121">Tim&#160;Rocktäschel,&#160;Sebastian&#160;Riedel,&#160;and&#160;Douwe</p>
<p style="position:absolute;top:977px;left:123px;white-space:nowrap" class="ft124">Kiela.&#160;2021.&#160;Retrieval-augmented&#160;generation&#160;for<br/>knowledge-intensive&#160;nlp&#160;tasks.</p>
<p style="position:absolute;top:994px;left:342px;white-space:nowrap" class="ft122">arXiv&#160;preprint</p>
<p style="position:absolute;top:1010px;left:123px;white-space:nowrap" class="ft122">arXiv:2005.11401v4</p>
<p style="position:absolute;top:1010px;left:246px;white-space:nowrap" class="ft121">.</p>
<p style="position:absolute;top:1038px;left:106px;white-space:nowrap" class="ft121">Kenneth&#160;Li,&#160;Oam&#160;Patel,&#160;Fernanda&#160;Viégas,&#160;Hanspeter</p>
<p style="position:absolute;top:1055px;left:123px;white-space:nowrap" class="ft124">Pfister,&#160;and&#160;Martin&#160;Wattenberg.&#160;2023a.&#160;Inference-<br/>time&#160;intervention:&#160;Eliciting&#160;truthful&#160;answers&#160;from&#160;a<br/>language&#160;model.&#160;arXiv&#160;preprint&#160;arXiv:2306.03341.</p>
<p style="position:absolute;top:1116px;left:106px;white-space:nowrap" class="ft121">Miaoran&#160;Li,&#160;Baolin&#160;Peng,&#160;and&#160;Zhu&#160;Zhang.&#160;2023b.&#160;<a href="http://arxiv.org/abs/2305.14623">Self-</a></p>
<p style="position:absolute;top:1133px;left:123px;white-space:nowrap" class="ft120"><a href="http://arxiv.org/abs/2305.14623">checker:&#160;Plug-and-play&#160;modules&#160;for&#160;fact-checking</a></p>
<p style="position:absolute;top:1149px;left:122px;white-space:nowrap" class="ft120"><a href="http://arxiv.org/abs/2305.14623">with&#160;large&#160;language&#160;models.</a></p>
<p style="position:absolute;top:113px;left:459px;white-space:nowrap" class="ft121">Weize&#160;Liu,&#160;Guocong&#160;Li,&#160;Kai&#160;Zhang,&#160;Bang&#160;Du,&#160;Qiyuan</p>
<p style="position:absolute;top:129px;left:476px;white-space:nowrap" class="ft121">Chen,&#160;Xuming&#160;Hu,&#160;Hongxia&#160;Xu,&#160;Jintai&#160;Chen,&#160;and&#160;Jian</p>
<p style="position:absolute;top:145px;left:475px;white-space:nowrap" class="ft121">Wu.&#160;2023.&#160;<a href="http://arxiv.org/abs/2311.09214">Mind’s&#160;mirror:&#160;Distilling&#160;self-evaluation</a></p>
<p style="position:absolute;top:162px;left:476px;white-space:nowrap" class="ft125"><a href="http://arxiv.org/abs/2311.09214">capability&#160;and&#160;comprehensive&#160;thinking&#160;from&#160;large<br/>language&#160;models.</a></p>
<p style="position:absolute;top:213px;left:459px;white-space:nowrap" class="ft121">Aman&#160;Madaan,&#160;Niket&#160;Tandon,&#160;Prakhar&#160;Gupta,&#160;Skyler</p>
<p style="position:absolute;top:229px;left:476px;white-space:nowrap" class="ft125">Hallinan,&#160;Luyu&#160;Gao,&#160;Sarah&#160;Wiegreffe,&#160;Uri&#160;Alon,<br/>Nouha&#160;Dziri,&#160;Shrimai&#160;Prabhumoye,&#160;Yiming&#160;Yang,<br/>Shashank&#160;Gupta,&#160;Bodhisattwa&#160;Prasad&#160;Majumder,<br/>Katherine&#160;Hermann,&#160;Sean&#160;Welleck,&#160;Amir&#160;Yazdan-<br/>bakhsh,&#160;and&#160;Peter&#160;Clark.&#160;2023.&#160;<a href="http://arxiv.org/abs/2303.17651">Self-refine:&#160;Iterative<br/>refinement&#160;with&#160;self-feedback.</a></p>
<p style="position:absolute;top:346px;left:459px;white-space:nowrap" class="ft121">Niels&#160;Mündler,&#160;Jingxuan&#160;He,&#160;Slobodan&#160;Jenko,&#160;and&#160;Mar-</p>
<p style="position:absolute;top:363px;left:476px;white-space:nowrap" class="ft124">tin&#160;Vechev.&#160;2023.&#160;Self-contradictory&#160;hallucinations<br/>of&#160;large&#160;language&#160;models:&#160;Evaluation,&#160;detection&#160;and<br/>mitigation.&#160;arXiv&#160;preprint&#160;arXiv:2305.15852.</p>
<p style="position:absolute;top:430px;left:459px;white-space:nowrap" class="ft121">Niels&#160;Mündler,&#160;Jingxuan&#160;He,&#160;Slobodan&#160;Jenko,&#160;and&#160;Mar-</p>
<p style="position:absolute;top:447px;left:476px;white-space:nowrap" class="ft125">tin&#160;Vechev.&#160;2023.&#160;<a href="http://arxiv.org/abs/2305.15852">Self-contradictory&#160;hallucinations<br/>of&#160;large&#160;language&#160;models:&#160;Evaluation,&#160;detection&#160;and<br/>mitigation.</a></p>
<p style="position:absolute;top:514px;left:459px;white-space:nowrap" class="ft121">Baolin&#160;Peng,&#160;Michel&#160;Galley,&#160;Pengcheng&#160;He,&#160;Hao&#160;Cheng,</p>
<p style="position:absolute;top:530px;left:475px;white-space:nowrap" class="ft124">Yujia&#160;Xie,&#160;Yu&#160;Hu,&#160;Qiuyuan&#160;Huang,&#160;Lars&#160;Liden,&#160;Zhou<br/>Yu,&#160;Weizhu&#160;Chen,&#160;and&#160;Jianfeng&#160;Gao.&#160;2023.&#160;<a href="http://arxiv.org/abs/2302.12813">Check</a></p>
<p style="position:absolute;top:563px;left:475px;white-space:nowrap" class="ft125"><a href="http://arxiv.org/abs/2302.12813">your&#160;facts&#160;and&#160;try&#160;again:&#160;Improving&#160;large&#160;language<br/>models&#160;with&#160;external&#160;knowledge&#160;and&#160;automated&#160;feed-<br/>back.</a></p>
<p style="position:absolute;top:631px;left:459px;white-space:nowrap" class="ft121">Yifu&#160;Qiu,&#160;Varun&#160;Embar,&#160;Shay&#160;B&#160;Cohen,&#160;and&#160;Benjamin</p>
<p style="position:absolute;top:647px;left:476px;white-space:nowrap" class="ft124">Han.&#160;2023a.&#160;Think&#160;while&#160;you&#160;write:&#160;Hypothesis<br/>verification&#160;promotes&#160;faithful&#160;knowledge-to-text&#160;gen-<br/>eration.&#160;arXiv&#160;preprint&#160;arXiv:2311.09467.</p>
<p style="position:absolute;top:715px;left:459px;white-space:nowrap" class="ft121">Yifu&#160;Qiu,&#160;Yftah&#160;Ziser,&#160;Anna&#160;Korhonen,&#160;Edoardo&#160;M.</p>
<p style="position:absolute;top:731px;left:476px;white-space:nowrap" class="ft125">Ponti,&#160;and&#160;Shay&#160;B.&#160;Cohen.&#160;2023b.&#160;<a href="http://arxiv.org/abs/2305.13632">Detecting&#160;and&#160;mit-<br/>igating&#160;hallucinations&#160;in&#160;multilingual&#160;summarisation.</a></p>
<p style="position:absolute;top:782px;left:459px;white-space:nowrap" class="ft121">Vipula&#160;Rawte,&#160;Swagata&#160;Chakraborty,&#160;Agnibh&#160;Pathak,</p>
<p style="position:absolute;top:799px;left:475px;white-space:nowrap" class="ft124">Anubhav&#160;Sarkar,&#160;S.&#160;M&#160;Towhidul&#160;Islam&#160;Tonmoy,<br/>Aman&#160;Chadha,&#160;Amit&#160;P.&#160;Sheth,&#160;and&#160;Amitava&#160;Das.</p>
<p style="position:absolute;top:832px;left:476px;white-space:nowrap" class="ft125">2023.&#160;<a href="http://arxiv.org/abs/2310.04988">The&#160;troubling&#160;emergence&#160;of&#160;hallucination&#160;in<br/>large&#160;language&#160;models&#160;–&#160;an&#160;extensive&#160;definition,&#160;quan-<br/>tification,&#160;and&#160;prescriptive&#160;remediations.</a></p>
<p style="position:absolute;top:899px;left:459px;white-space:nowrap" class="ft121">Partha&#160;Pratim&#160;Ray.&#160;2023.&#160;<a href="https://doi.org/https://doi.org/10.1016/j.iotcps.2023.04.003">Chatgpt:&#160;A&#160;comprehensive</a></p>
<p style="position:absolute;top:916px;left:476px;white-space:nowrap" class="ft125"><a href="https://doi.org/https://doi.org/10.1016/j.iotcps.2023.04.003">review&#160;on&#160;background,&#160;applications,&#160;key&#160;challenges,<br/>bias,&#160;ethics,&#160;limitations&#160;and&#160;future&#160;scope.&#160;</a>Internet&#160;of</p>
<p style="position:absolute;top:949px;left:475px;white-space:nowrap" class="ft122">Things&#160;and&#160;Cyber-Physical&#160;Systems</p>
<p style="position:absolute;top:948px;left:689px;white-space:nowrap" class="ft121">,&#160;3:121–154.</p>
<p style="position:absolute;top:983px;left:459px;white-space:nowrap" class="ft121">Evgeniia</p>
<p style="position:absolute;top:983px;left:532px;white-space:nowrap" class="ft121">Razumovskaia,</p>
<p style="position:absolute;top:983px;left:648px;white-space:nowrap" class="ft121">Ivan</p>
<p style="position:absolute;top:983px;left:694px;white-space:nowrap" class="ft121">Vuli´c,</p>
<p style="position:absolute;top:983px;left:753px;white-space:nowrap" class="ft121">Pavle</p>
<p style="position:absolute;top:999px;left:476px;white-space:nowrap" class="ft124">Markovi´c,&#160;Tomasz&#160;Cichy,&#160;Qian&#160;Zheng,&#160;Tsung-<br/>Hsien&#160;Wen,</p>
<p style="position:absolute;top:1016px;left:569px;white-space:nowrap" class="ft121">and&#160;Paweł&#160;Budzianowski.&#160;2023.</p>
<p style="position:absolute;top:1032px;left:475px;white-space:nowrap" class="ft123"><a href="http://arxiv.org/abs/2311.09800">Dial&#160;BeInfo&#160;for&#160;Faithfulness</a></p>
<p style="position:absolute;top:1032px;left:643px;white-space:nowrap" class="ft120"><a href="http://arxiv.org/abs/2311.09800">:&#160;Improving&#160;factuality</a></p>
<p style="position:absolute;top:1049px;left:476px;white-space:nowrap" class="ft125"><a href="http://arxiv.org/abs/2311.09800">of&#160;information-seeking&#160;dialogue&#160;via&#160;behavioural<br/>fine-tuning.</a></p>
<p style="position:absolute;top:1100px;left:459px;white-space:nowrap" class="ft121">Victor&#160;Sanh,&#160;Lysandre&#160;Debut,&#160;Julien&#160;Chaumond,&#160;and</p>
<p style="position:absolute;top:1116px;left:475px;white-space:nowrap" class="ft121">Thomas&#160;Wolf.&#160;2019.&#160;Distilbert,&#160;a&#160;distilled&#160;version</p>
<p style="position:absolute;top:1133px;left:476px;white-space:nowrap" class="ft126">of&#160;bert:&#160;smaller,&#160;faster,&#160;cheaper&#160;and&#160;lighter.&#160;arXiv<br/>preprint&#160;arXiv:1910.01108</p>
<p style="position:absolute;top:1149px;left:637px;white-space:nowrap" class="ft121">.</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft130{font-size:14px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft131{font-size:14px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft132{font-size:14px;font-family:YOPPJI+NimbusRomNo9L-ReguItal;color:#000000;}
	.ft133{font-size:14px;line-height:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft134{font-size:14px;line-height:16px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft135{font-size:14px;line-height:16px;font-family:YOPPJI+NimbusRomNo9L-ReguItal;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page13-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html013.png" alt="background image"/>
<p style="position:absolute;top:113px;left:106px;white-space:nowrap" class="ft130">Weijia&#160;Shi,&#160;Xiaochuang&#160;Han,&#160;Mike&#160;Lewis,&#160;Yulia</p>
<p style="position:absolute;top:129px;left:122px;white-space:nowrap" class="ft130">Tsvetkov,&#160;Luke&#160;Zettlemoyer,&#160;and&#160;Scott&#160;Wen&#160;tau&#160;Yih.</p>
<p style="position:absolute;top:145px;left:123px;white-space:nowrap" class="ft133">2023.&#160;<a href="http://arxiv.org/abs/2305.14739">Trusting&#160;your&#160;evidence:&#160;Hallucinate&#160;less&#160;with<br/>context-aware&#160;decoding.</a></p>
<p style="position:absolute;top:193px;left:106px;white-space:nowrap" class="ft130">Chenglei&#160;Si,&#160;Zhe&#160;Gan,&#160;Zhengyuan&#160;Yang,&#160;Shuohang</p>
<p style="position:absolute;top:209px;left:122px;white-space:nowrap" class="ft130">Wang,&#160;Jianfeng&#160;Wang,&#160;Jordan&#160;Boyd-Graber,&#160;and&#160;Li-</p>
<p style="position:absolute;top:226px;left:123px;white-space:nowrap" class="ft134">juan&#160;Wang.&#160;2022.&#160;Prompting&#160;gpt-3&#160;to&#160;be&#160;reliable.<br/>arXiv&#160;preprint&#160;arXiv:2210.09150</p>
<p style="position:absolute;top:242px;left:321px;white-space:nowrap" class="ft130">.</p>
<p style="position:absolute;top:273px;left:106px;white-space:nowrap" class="ft130">Kai&#160;Sun,&#160;Yifan&#160;Ethan&#160;Xu,&#160;Hanwen&#160;Zha,&#160;Yue&#160;Liu,&#160;and</p>
<p style="position:absolute;top:290px;left:122px;white-space:nowrap" class="ft130">Xin&#160;Luna&#160;Dong.&#160;2023a.&#160;Head-to-tail:&#160;How&#160;knowl-</p>
<p style="position:absolute;top:306px;left:123px;white-space:nowrap" class="ft134">edgeable&#160;are&#160;large&#160;language&#160;models&#160;(llm)?&#160;aka&#160;will<br/>llms&#160;replace&#160;knowledge&#160;graphs?</p>
<p style="position:absolute;top:323px;left:344px;white-space:nowrap" class="ft132">arXiv&#160;preprint</p>
<p style="position:absolute;top:339px;left:123px;white-space:nowrap" class="ft132">arXiv:2308.10168</p>
<p style="position:absolute;top:339px;left:232px;white-space:nowrap" class="ft130">.</p>
<p style="position:absolute;top:370px;left:106px;white-space:nowrap" class="ft130">Zhiqing&#160;Sun,&#160;Yikang&#160;Shen,&#160;Qinhong&#160;Zhou,&#160;Hongxin</p>
<p style="position:absolute;top:386px;left:123px;white-space:nowrap" class="ft134">Zhang,&#160;Zhenfang&#160;Chen,&#160;David&#160;Cox,&#160;Yiming&#160;Yang,<br/>and&#160;Chuang&#160;Gan.&#160;2023b.</p>
<p style="position:absolute;top:403px;left:303px;white-space:nowrap" class="ft131"><a href="http://arxiv.org/abs/2305.03047">Principle-driven&#160;self-</a></p>
<p style="position:absolute;top:419px;left:123px;white-space:nowrap" class="ft133"><a href="http://arxiv.org/abs/2305.03047">alignment&#160;of&#160;language&#160;models&#160;from&#160;scratch&#160;with&#160;min-<br/>imal&#160;human&#160;supervision.</a></p>
<p style="position:absolute;top:466px;left:106px;white-space:nowrap" class="ft130">Katherine&#160;Tian,&#160;Eric&#160;Mitchell,&#160;Huaxiu&#160;Yao,&#160;Christo-</p>
<p style="position:absolute;top:483px;left:123px;white-space:nowrap" class="ft133">pher&#160;D.&#160;Manning,&#160;and&#160;Chelsea&#160;Finn.&#160;2023.&#160;<a href="http://arxiv.org/abs/2311.08401">Fine-<br/>tuning&#160;language&#160;models&#160;for&#160;factuality.</a></p>
<p style="position:absolute;top:530px;left:106px;white-space:nowrap" class="ft130">Hugo&#160;Touvron,&#160;Louis&#160;Martin,&#160;Kevin&#160;Stone,&#160;Peter&#160;Al-</p>
<p style="position:absolute;top:547px;left:123px;white-space:nowrap" class="ft133">bert,&#160;Amjad&#160;Almahairi,&#160;Yasmine&#160;Babaei,&#160;Nikolay<br/>Bashlykov,&#160;Soumya&#160;Batra,&#160;Prajjwal&#160;Bhargava,&#160;Shruti<br/>Bhosale,&#160;Dan&#160;Bikel,&#160;Lukas&#160;Blecher,&#160;Cristian&#160;Canton<br/>Ferrer,&#160;Moya&#160;Chen,&#160;Guillem&#160;Cucurull,&#160;David&#160;Esiobu,<br/>Jude&#160;Fernandes,&#160;Jeremy&#160;Fu,&#160;Wenyin&#160;Fu,&#160;Brian&#160;Fuller,<br/>Cynthia&#160;Gao,&#160;Vedanuj&#160;Goswami,&#160;Naman&#160;Goyal,&#160;An-<br/>thony&#160;Hartshorn,&#160;Saghar&#160;Hosseini,&#160;Rui&#160;Hou,&#160;Hakan<br/>Inan,&#160;Marcin&#160;Kardas,&#160;Viktor&#160;Kerkez,&#160;Madian&#160;Khabsa,<br/>Isabel&#160;Kloumann,&#160;Artem&#160;Korenev,&#160;Punit&#160;Singh&#160;Koura,<br/>Marie-Anne&#160;Lachaux,&#160;Thibaut&#160;Lavril,&#160;Jenya&#160;Lee,&#160;Di-<br/>ana&#160;Liskovich,&#160;Yinghai&#160;Lu,&#160;Yuning&#160;Mao,&#160;Xavier&#160;Mar-<br/>tinet,&#160;Todor&#160;Mihaylov,&#160;Pushkar&#160;Mishra,&#160;Igor&#160;Moly-<br/>bog,&#160;Yixin&#160;Nie,&#160;Andrew&#160;Poulton,&#160;Jeremy&#160;Reizen-<br/>stein,&#160;Rashi&#160;Rungta,&#160;Kalyan&#160;Saladi,&#160;Alan&#160;Schelten,<br/>Ruan&#160;Silva,&#160;Eric&#160;Michael&#160;Smith,&#160;Ranjan&#160;Subrama-<br/>nian,&#160;Xiaoqing&#160;Ellen&#160;Tan,&#160;Binh&#160;Tang,&#160;Ross&#160;Tay-<br/>lor,&#160;Adina&#160;Williams,&#160;Jian&#160;Xiang&#160;Kuan,&#160;Puxin&#160;Xu,<br/>Zheng&#160;Yan,&#160;Iliyan&#160;Zarov,&#160;Yuchen&#160;Zhang,&#160;Angela&#160;Fan,<br/>Melanie&#160;Kambadur,&#160;Sharan&#160;Narang,&#160;Aurelien&#160;Ro-<br/>driguez,&#160;Robert&#160;Stojnic,&#160;Sergey&#160;Edunov,&#160;and&#160;Thomas<br/>Scialom.&#160;2023.&#160;<a href="http://arxiv.org/abs/2307.09288">Llama&#160;2:&#160;Open&#160;foundation&#160;and&#160;fine-<br/>tuned&#160;chat&#160;models.</a></p>
<p style="position:absolute;top:923px;left:106px;white-space:nowrap" class="ft130">Neeraj&#160;Varshney,&#160;Wenlin&#160;Yao,&#160;Hongming&#160;Zhang,&#160;Jian-</p>
<p style="position:absolute;top:939px;left:123px;white-space:nowrap" class="ft133">shu&#160;Chen,&#160;and&#160;Dong&#160;Yu.&#160;2023.&#160;<a href="http://arxiv.org/abs/2307.03987">A&#160;stitch&#160;in&#160;time&#160;saves<br/>nine:&#160;Detecting&#160;and&#160;mitigating&#160;hallucinations&#160;of&#160;llms<br/>by&#160;validating&#160;low-confidence&#160;generation.</a></p>
<p style="position:absolute;top:1003px;left:106px;white-space:nowrap" class="ft130">Tu&#160;Vu,&#160;Mohit&#160;Iyyer,&#160;Xuezhi&#160;Wang,&#160;Noah&#160;Constant,&#160;Jerry</p>
<p style="position:absolute;top:1020px;left:122px;white-space:nowrap" class="ft130">Wei,&#160;Jason&#160;Wei,&#160;Chris&#160;Tar,&#160;Yun-Hsuan&#160;Sung,&#160;Denny</p>
<p style="position:absolute;top:1036px;left:123px;white-space:nowrap" class="ft133">Zhou,&#160;Quoc&#160;Le,&#160;and&#160;Thang&#160;Luong.&#160;2023.&#160;<a href="http://arxiv.org/abs/2310.03214">Freshllms:<br/>Refreshing&#160;large&#160;language&#160;models&#160;with&#160;search&#160;engine<br/>augmentation.</a></p>
<p style="position:absolute;top:1100px;left:106px;white-space:nowrap" class="ft130">Yizhong&#160;Wang,&#160;Yeganeh&#160;Kordi,&#160;Swaroop&#160;Mishra,&#160;Alisa</p>
<p style="position:absolute;top:1116px;left:123px;white-space:nowrap" class="ft133">Liu,&#160;Noah&#160;A.&#160;Smith,&#160;Daniel&#160;Khashabi,&#160;and&#160;Hannaneh<br/>Hajishirzi.&#160;2023.&#160;<a href="http://arxiv.org/abs/2212.10560">Self-instruct:&#160;Aligning&#160;language<br/>models&#160;with&#160;self-generated&#160;instructions.</a></p>
<p style="position:absolute;top:113px;left:459px;white-space:nowrap" class="ft130">Jules&#160;White,&#160;Quchen&#160;Fu,&#160;Sam&#160;Hays,&#160;Michael&#160;Sandborn,</p>
<p style="position:absolute;top:129px;left:476px;white-space:nowrap" class="ft133">Carlos&#160;Olea,&#160;Henry&#160;Gilbert,&#160;Ashraf&#160;Elnashar,&#160;Jesse<br/>Spencer-Smith,&#160;and&#160;Douglas&#160;C.&#160;Schmidt.&#160;2023.&#160;<a href="http://arxiv.org/abs/2302.11382">A<br/>prompt&#160;pattern&#160;catalog&#160;to&#160;enhance&#160;prompt&#160;engineer-<br/>ing&#160;with&#160;chatgpt.</a></p>
<p style="position:absolute;top:208px;left:459px;white-space:nowrap" class="ft130">BigScience&#160;Workshop,&#160;Teven&#160;Le&#160;Scao,&#160;Angela&#160;Fan,</p>
<p style="position:absolute;top:225px;left:476px;white-space:nowrap" class="ft134">Christopher&#160;Akiki,&#160;Ellie&#160;Pavlick,&#160;Suzana&#160;Ili´c,&#160;Daniel<br/>Hesslow,&#160;Roman&#160;Castagné,&#160;Alexandra&#160;Sasha&#160;Luc-<br/>cioni,&#160;François&#160;Yvon,&#160;et&#160;al.&#160;2022.&#160;Bloom:&#160;A&#160;176b-<br/>parameter&#160;open-access&#160;multilingual&#160;language&#160;model.<br/>arXiv&#160;preprint&#160;arXiv:2211.05100</p>
<p style="position:absolute;top:290px;left:674px;white-space:nowrap" class="ft130">.</p>
<p style="position:absolute;top:320px;left:459px;white-space:nowrap" class="ft130">Can&#160;Xu,&#160;Qingfeng&#160;Sun,&#160;Kai&#160;Zheng,&#160;Xiubo&#160;Geng,</p>
<p style="position:absolute;top:337px;left:476px;white-space:nowrap" class="ft133">Pu&#160;Zhao,&#160;Jiazhan&#160;Feng,&#160;Chongyang&#160;Tao,&#160;and&#160;Daxin<br/>Jiang.&#160;2023.&#160;<a href="http://arxiv.org/abs/2304.12244">Wizardlm:&#160;Empowering&#160;large&#160;language<br/>models&#160;to&#160;follow&#160;complex&#160;instructions.</a></p>
<p style="position:absolute;top:400px;left:459px;white-space:nowrap" class="ft130">Jing&#160;Nathan&#160;Yan,&#160;Tianqi&#160;Liu,&#160;Justin&#160;T&#160;Chiu,&#160;Jiaming</p>
<p style="position:absolute;top:416px;left:476px;white-space:nowrap" class="ft133">Shen,&#160;Zhen&#160;Qin,&#160;Yue&#160;Yu,&#160;Yao&#160;Zhao,&#160;Charu&#160;Laksh-<br/>manan,&#160;Yair&#160;Kurzion,&#160;Alexander&#160;M.&#160;Rush,&#160;Jialu&#160;Liu,<br/>and&#160;Michael&#160;Bendersky.&#160;2023.&#160;<a href="http://arxiv.org/abs/2311.08390">On&#160;what&#160;basis?&#160;pre-<br/>dicting&#160;text&#160;preference&#160;via&#160;structured&#160;comparative<br/>reasoning.</a></p>
<p style="position:absolute;top:512px;left:459px;white-space:nowrap" class="ft130">Sunjae&#160;Yoon,&#160;Eunseop&#160;Yoon,&#160;Hee&#160;Suk&#160;Yoon,&#160;Junyeong</p>
<p style="position:absolute;top:528px;left:476px;white-space:nowrap" class="ft133">Kim,&#160;and&#160;Chang&#160;Yoo.&#160;2022.&#160;<a href="https://doi.org/10.18653/v1/2022.emnlp-main.280">Information-theoretic<br/>text&#160;hallucination&#160;reduction&#160;for&#160;video-grounded&#160;di-<br/>alogue.&#160;</a>In&#160;Proceedings&#160;of&#160;the&#160;2022&#160;Conference&#160;on</p>
<p style="position:absolute;top:577px;left:475px;white-space:nowrap" class="ft132">Empirical&#160;Methods&#160;in&#160;Natural&#160;Language&#160;Processing</p>
<p style="position:absolute;top:577px;left:785px;white-space:nowrap" class="ft130">,</p>
<p style="position:absolute;top:594px;left:476px;white-space:nowrap" class="ft130">pages&#160;4182–4193,&#160;Abu&#160;Dhabi,&#160;United&#160;Arab&#160;Emirates.</p>
<p style="position:absolute;top:610px;left:475px;white-space:nowrap" class="ft130">Association&#160;for&#160;Computational&#160;Linguistics.</p>
<p style="position:absolute;top:640px;left:459px;white-space:nowrap" class="ft130">Hanning&#160;Zhang,&#160;Shizhe&#160;Diao,&#160;Yong&#160;Lin,&#160;Yi&#160;R&#160;Fung,</p>
<p style="position:absolute;top:657px;left:476px;white-space:nowrap" class="ft135">Qing&#160;Lian,&#160;Xingyao&#160;Wang,&#160;Yangyi&#160;Chen,&#160;Heng&#160;Ji,<br/>and&#160;Tong&#160;Zhang.&#160;2023a.&#160;R-tuning:&#160;Teaching&#160;large<br/>language&#160;models&#160;to&#160;refuse&#160;unknown&#160;questions.&#160;arXiv<br/>preprint&#160;arXiv:2311.09677</p>
<p style="position:absolute;top:706px;left:637px;white-space:nowrap" class="ft130">.</p>
<p style="position:absolute;top:736px;left:459px;white-space:nowrap" class="ft130">Shuo&#160;Zhang,&#160;Liangming&#160;Pan,&#160;Junzhou&#160;Zhao,&#160;and</p>
<p style="position:absolute;top:752px;left:475px;white-space:nowrap" class="ft130">William&#160;Yang&#160;Wang.&#160;2023b.&#160;<a href="http://arxiv.org/abs/2305.13669">The&#160;knowledge&#160;align-</a></p>
<p style="position:absolute;top:769px;left:476px;white-space:nowrap" class="ft133"><a href="http://arxiv.org/abs/2305.13669">ment&#160;problem:&#160;Bridging&#160;human&#160;and&#160;external&#160;knowl-<br/>edge&#160;for&#160;large&#160;language&#160;models.</a></p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft140{font-size:13px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft141{font-size:13px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft142{font-size:13px;font-family:RXNEKG+MSAM10;color:#009600;}
	.ft143{font-size:13px;font-family:OPEKTB+CMSY9;color:#ff0000;}
	.ft144{font-size:8px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft145{font-size:8px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft146{font-size:8px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft147{font-size:8px;font-family:LNLVHQ+CMSY6;color:#ff0000;}
	.ft148{font-size:8px;font-family:LNLVHQ+CMSY6;color:#000000;}
	.ft149{font-size:8px;font-family:LNIRBF+MSAM7;color:#009600;}
	.ft1410{font-size:8px;font-family:YOPPJI+NimbusRomNo9L-ReguItal;color:#000000;}
	.ft1411{font-size:13px;line-height:15px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft1412{font-size:13px;line-height:14px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft1413{font-size:8px;line-height:10px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft1414{font-size:8px;line-height:10px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft1415{font-size:8px;line-height:10px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft1416{font-size:8px;line-height:11px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft1417{font-size:8px;line-height:9px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page14-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html014.png" alt="background image"/>
<p style="position:absolute;top:106px;left:137px;white-space:nowrap" class="ft1411">Table&#160;1:&#160;Summary&#160;of&#160;all&#160;the&#160;works&#160;related&#160;to&#160;hallucination&#160;mitigation&#160;in&#160;two&#160;categories.&#160;Here,&#160;we&#160;have&#160;divided&#160;each<br/>work&#160;by&#160;the&#160;following&#160;factors:1.&#160;Mitigation&#160;Technique,&#160;2.&#160;Detection,&#160;3.&#160;Task(s),&#160;4.&#160;Metrics,&#160;and&#160;5.&#160;Evaluated</p>
<p style="position:absolute;top:136px;left:137px;white-space:nowrap" class="ft1412">LLM(s),<br/>6.&#160;Dataset(s).</p>
<p style="position:absolute;top:151px;left:213px;white-space:nowrap" class="ft142">✓&#160;indicates&#160;that&#160;it&#160;is&#160;present&#160;in&#160;the&#160;paper&#160;whereas&#160;×&#160;indicates&#160;it&#160;is&#160;not&#160;present.</p>
<p style="position:absolute;top:188px;left:129px;white-space:nowrap" class="ft144">Category</p>
<p style="position:absolute;top:188px;left:186px;white-space:nowrap" class="ft1413">Mitigation<br/>Technique(s)</p>
<p style="position:absolute;top:188px;left:258px;white-space:nowrap" class="ft144">Title</p>
<p style="position:absolute;top:188px;left:352px;white-space:nowrap" class="ft144">Detection</p>
<p style="position:absolute;top:188px;left:403px;white-space:nowrap" class="ft144">Task(s)</p>
<p style="position:absolute;top:188px;left:476px;white-space:nowrap" class="ft144">Metric(s)</p>
<p style="position:absolute;top:188px;left:559px;white-space:nowrap" class="ft1413">Evaluated<br/>LLM(s)</p>
<p style="position:absolute;top:188px;left:632px;white-space:nowrap" class="ft144">Dataset(s)</p>
<p style="position:absolute;top:188px;left:704px;white-space:nowrap" class="ft144">Limitation(s)</p>
<p style="position:absolute;top:218px;left:129px;white-space:nowrap" class="ft1414">Prompt<br/>Engineering</p>
<p style="position:absolute;top:218px;left:196px;white-space:nowrap" class="ft145">Retrieval</p>
<p style="position:absolute;top:228px;left:192px;white-space:nowrap" class="ft145">Augmented</p>
<p style="position:absolute;top:238px;left:193px;white-space:nowrap" class="ft145">Generation</p>
<p style="position:absolute;top:249px;left:199px;white-space:nowrap" class="ft145">(Before</p>
<p style="position:absolute;top:259px;left:191px;white-space:nowrap" class="ft145">Generation)</p>
<p style="position:absolute;top:218px;left:258px;white-space:nowrap" class="ft1415">Check&#160;Your&#160;Facts<br/>and&#160;Try&#160;Again:<br/>Improving&#160;Large<br/>Language&#160;Models<br/>with&#160;External<br/>Knowledge&#160;and<br/>Automated<br/>Feedback&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Peng<br/>et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:217px;left:352px;white-space:nowrap" class="ft147">×</p>
<p style="position:absolute;top:218px;left:403px;white-space:nowrap" class="ft1414">Information<br/>seeking&#160;dialog<br/>and<br/>open-domain</p>
<p style="position:absolute;top:259px;left:403px;white-space:nowrap" class="ft1414">Wiki&#160;question<br/>Answering</p>
<p style="position:absolute;top:218px;left:476px;white-space:nowrap" class="ft1414">KF1,&#160;BLEU-4,<br/>ROUGE-1,<br/>METEOR,<br/>BLEURT,<br/>BERTScore,<br/>chrF,<br/>BARTScore</p>
<p style="position:absolute;top:218px;left:559px;white-space:nowrap" class="ft145">GPT-3.5</p>
<p style="position:absolute;top:218px;left:632px;white-space:nowrap" class="ft1414">Manual,<br/>OTT-QA</p>
<p style="position:absolute;top:217px;left:704px;white-space:nowrap" class="ft1414">•&#160;Interactive<br/>feedback&#160;with<br/>ChatGPT&#160;slows<br/>down&#160;user<br/>experience<br/>as&#160;it&#160;requires<br/>multiple&#160;queries<br/>per&#160;response.<br/>•&#160;No&#160;human<br/>evaluation&#160;of<br/>responses&#160;has<br/>been&#160;conducted<br/>yet.</p>
<p style="position:absolute;top:362px;left:258px;white-space:nowrap" class="ft1415">FRESHLLMS:<br/>Refreshing&#160;Large<br/>Language&#160;Models<br/>with&#160;Search&#160;Engine<br/>Augmentation&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Vu<br/>et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:362px;left:352px;white-space:nowrap" class="ft149">✓</p>
<p style="position:absolute;top:362px;left:403px;white-space:nowrap" class="ft145">QA</p>
<p style="position:absolute;top:362px;left:476px;white-space:nowrap" class="ft145">Accuracy</p>
<p style="position:absolute;top:362px;left:559px;white-space:nowrap" class="ft1414">T5,&#160;Palm,<br/>Palmchilla,<br/>Flan-T5,<br/>Flan-Palm,<br/>GPT-3.5,<br/>Codex,&#160;GPT-4</p>
<p style="position:absolute;top:362px;left:632px;white-space:nowrap" class="ft1414">FreshQA<br/>(Own&#160;dataset)</p>
<p style="position:absolute;top:361px;left:704px;white-space:nowrap" class="ft1414">•&#160;Answers&#160;can<br/>become&#160;stale<br/>between&#160;manual<br/>updates&#160;by<br/>maintainers.<br/>•&#160;Method&#160;relies<br/>on&#160;Google<br/>search&#160;API,<br/>simple&#160;English<br/>questions,&#160;and<br/>in-context<br/>learning<br/>without&#160;further<br/>fine-tuning.</p>
<p style="position:absolute;top:527px;left:196px;white-space:nowrap" class="ft145">Retrieval</p>
<p style="position:absolute;top:538px;left:192px;white-space:nowrap" class="ft145">Augmented</p>
<p style="position:absolute;top:548px;left:193px;white-space:nowrap" class="ft145">Generation</p>
<p style="position:absolute;top:559px;left:198px;white-space:nowrap" class="ft145">(During</p>
<p style="position:absolute;top:569px;left:191px;white-space:nowrap" class="ft145">Generation)</p>
<p style="position:absolute;top:527px;left:258px;white-space:nowrap" class="ft1414">A&#160;Stitch&#160;in&#160;Time<br/>Saves&#160;Nine:<br/>Detecting&#160;and<br/>Mitigating<br/>Hallucinations&#160;of<br/>LLMs&#160;by&#160;Validating<br/>Low-Confidence<br/>Generation<br/><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Varshney&#160;et&#160;al.,<br/>2023)</a></p>
<p style="position:absolute;top:527px;left:352px;white-space:nowrap" class="ft149">✓</p>
<p style="position:absolute;top:527px;left:403px;white-space:nowrap" class="ft1414">Article<br/>generation<br/>task,&#160;Multi-<br/>hop&#160;Questions,<br/>False&#160;Premise<br/>Questions</p>
<p style="position:absolute;top:527px;left:476px;white-space:nowrap" class="ft1414">Accuracy&#160;and<br/>Success</p>
<p style="position:absolute;top:527px;left:559px;white-space:nowrap" class="ft1414">GPT-3.5,<br/>Vicuna</p>
<p style="position:absolute;top:527px;left:632px;white-space:nowrap" class="ft145">Manual</p>
<p style="position:absolute;top:527px;left:704px;white-space:nowrap" class="ft1414">Details&#160;not<br/>provided</p>
<p style="position:absolute;top:640px;left:258px;white-space:nowrap" class="ft1414">A&#160;Step&#160;Closer&#160;to<br/>Comprehensive<br/>Answers:<br/>Constrained<br/>Multi-Stage<br/>Question<br/>Decomposition&#160;with<br/>Large&#160;Language<br/>Models&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Cao&#160;et&#160;al.,<br/>2023)</a></p>
<p style="position:absolute;top:640px;left:352px;white-space:nowrap" class="ft147">×</p>
<p style="position:absolute;top:640px;left:403px;white-space:nowrap" class="ft145">QA</p>
<p style="position:absolute;top:640px;left:476px;white-space:nowrap" class="ft1414">Recall&#160;and&#160;F1<br/>score</p>
<p style="position:absolute;top:640px;left:559px;white-space:nowrap" class="ft1414">PaLM,<br/>InstructGPT,<br/>GPT-3&#160;and<br/>LLaMA2</p>
<p style="position:absolute;top:640px;left:632px;white-space:nowrap" class="ft1414">ChitChatQA<br/>and&#160;HotPotQA</p>
<p style="position:absolute;top:640px;left:704px;white-space:nowrap" class="ft1414">Details&#160;not<br/>provided</p>
<p style="position:absolute;top:753px;left:258px;white-space:nowrap" class="ft1415">EVER:&#160;Mitigating<br/>Hallucination&#160;in<br/>Large&#160;Language<br/>Models&#160;through<br/>Real-Time<br/>Verification&#160;and<br/>Rectification&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Kang<br/>et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:753px;left:352px;white-space:nowrap" class="ft149">✓</p>
<p style="position:absolute;top:753px;left:403px;white-space:nowrap" class="ft1414">Short-form<br/>QA,&#160;Biog-<br/>raphy&#160;gen-<br/>eration,&#160;and<br/>Reasoning,</p>
<p style="position:absolute;top:753px;left:476px;white-space:nowrap" class="ft1414">Exact<br/>match(EM),F1-<br/>score,<br/>recall&#160;@&#160;5,<br/>FACTSCORE</p>
<p style="position:absolute;top:753px;left:559px;white-space:nowrap" class="ft1414">InstructGPT,<br/>Llama&#160;2&#160;7B<br/>Chat,&#160;Llama<br/>2&#160;13B&#160;Chat,<br/>Llama&#160;1&#160;65B<br/>and&#160;GPT-3.5</p>
<p style="position:absolute;top:753px;left:632px;white-space:nowrap" class="ft1414">HotPotQA,<br/>TriviaQA,<br/>ALCE-<br/>Qampari<br/>QA,bio<br/>benchmark</p>
<p style="position:absolute;top:753px;left:704px;white-space:nowrap" class="ft1414">•&#160;The&#160;paper<br/>focuses&#160;solely<br/>on&#160;enhancing<br/>text&#160;attribution<br/>to&#160;reduce<br/>hallucinations.<br/>•&#160;It&#160;relies&#160;on<br/>references,<br/>which&#160;may&#160;have<br/>inaccuracies,&#160;to<br/>support&#160;facts.</p>
<p style="position:absolute;top:878px;left:360px;white-space:nowrap" class="ft1410">Continued&#160;on&#160;the&#160;next&#160;page</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft150{font-size:8px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft151{font-size:8px;font-family:YOPPJI+NimbusRomNo9L-ReguItal;color:#000000;}
	.ft152{font-size:8px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft153{font-size:8px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft154{font-size:8px;font-family:LNIRBF+MSAM7;color:#009600;}
	.ft155{font-size:8px;font-family:LNLVHQ+CMSY6;color:#000000;}
	.ft156{font-size:8px;font-family:LNLVHQ+CMSY6;color:#ff0000;}
	.ft157{font-size:8px;line-height:10px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft158{font-size:8px;line-height:10px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft159{font-size:8px;line-height:11px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft1510{font-size:8px;line-height:9px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft1511{font-size:8px;line-height:10px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page15-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html015.png" alt="background image"/>
<p style="position:absolute;top:113px;left:368px;white-space:nowrap" class="ft150">Table&#160;1&#160;–&#160;Continued&#160;from&#160;the&#160;previous&#160;page</p>
<p style="position:absolute;top:133px;left:129px;white-space:nowrap" class="ft152">Category</p>
<p style="position:absolute;top:133px;left:186px;white-space:nowrap" class="ft157">Mitigation<br/>Technique(s)</p>
<p style="position:absolute;top:133px;left:258px;white-space:nowrap" class="ft152">Title</p>
<p style="position:absolute;top:133px;left:352px;white-space:nowrap" class="ft152">Detection</p>
<p style="position:absolute;top:133px;left:403px;white-space:nowrap" class="ft152">Task(s)</p>
<p style="position:absolute;top:133px;left:476px;white-space:nowrap" class="ft152">Metric(s)</p>
<p style="position:absolute;top:133px;left:559px;white-space:nowrap" class="ft157">Evaluated<br/>LLM(s)</p>
<p style="position:absolute;top:133px;left:632px;white-space:nowrap" class="ft152">Dataset(s)</p>
<p style="position:absolute;top:133px;left:704px;white-space:nowrap" class="ft152">Limitation(s)</p>
<p style="position:absolute;top:162px;left:196px;white-space:nowrap" class="ft150">Retrieval</p>
<p style="position:absolute;top:173px;left:192px;white-space:nowrap" class="ft150">Augmented</p>
<p style="position:absolute;top:183px;left:193px;white-space:nowrap" class="ft150">Generation</p>
<p style="position:absolute;top:194px;left:202px;white-space:nowrap" class="ft150">(After</p>
<p style="position:absolute;top:204px;left:191px;white-space:nowrap" class="ft150">Generation)</p>
<p style="position:absolute;top:162px;left:258px;white-space:nowrap" class="ft158">RARR:&#160;Researching<br/>and&#160;Revising&#160;What<br/>Language&#160;Models<br/>Say,&#160;Using<br/>Language&#160;Models<br/><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Gao&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:162px;left:352px;white-space:nowrap" class="ft154">✓</p>
<p style="position:absolute;top:162px;left:403px;white-space:nowrap" class="ft158">Editing&#160;for<br/>Attribution</p>
<p style="position:absolute;top:162px;left:476px;white-space:nowrap" class="ft158">Attributable&#160;to<br/>Identified<br/>Sources&#160;(AIS),<br/>automated&#160;metric,<br/>auto-AIS,&#160;Preser-<br/>vation(intent,<br/>Levenshtein<br/>similarity,<br/>combined)</p>
<p style="position:absolute;top:162px;left:559px;white-space:nowrap" class="ft158">PaLM&#160;540B,<br/>GPT-3,<br/>LaMDA,<br/>EFEC</p>
<p style="position:absolute;top:162px;left:632px;white-space:nowrap" class="ft158">NQ,&#160;SQA&#160;and<br/>QReCC</p>
<p style="position:absolute;top:161px;left:704px;white-space:nowrap" class="ft158">•&#160;Evaluation<br/>metrics&#160;don’t<br/>cover&#160;all<br/>attribution<br/>aspects,&#160;like<br/>self-evident<br/>sentences.<br/>•&#160;Preservation<br/>metrics&#160;penalize<br/>necessary<br/>revisions&#160;for<br/>severely&#160;flawed<br/>input&#160;text.<br/>•&#160;RARR&#160;isn’t<br/>equipped&#160;for<br/>long&#160;documents<br/>due&#160;to&#160;a&#160;lack&#160;of<br/>examples&#160;in&#160;the<br/>few-shot&#160;LLM<br/>prompts.<br/>•&#160;It&#160;tends<br/>to&#160;retain<br/>unattributed<br/>claims,&#160;some&#160;of<br/>which&#160;may&#160;be<br/>hallucinations.<br/>•&#160;The&#160;model&#160;is<br/>computationally<br/>intensive.</p>
<p style="position:absolute;top:474px;left:258px;white-space:nowrap" class="ft158">The&#160;Troubling<br/>Emergence&#160;of<br/>Hallucination&#160;in<br/>Large&#160;Language<br/>Models&#160;–&#160;An<br/>Extensive&#160;Definition,<br/>Quantification,&#160;and<br/>Prescriptive<br/>Remediations<br/><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Rawte&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:474px;left:352px;white-space:nowrap" class="ft154">✓</p>
<p style="position:absolute;top:474px;left:403px;white-space:nowrap" class="ft158">Text&#160;Gen-<br/>eration&#160;and<br/>QA</p>
<p style="position:absolute;top:474px;left:476px;white-space:nowrap" class="ft150">HVI</p>
<p style="position:absolute;top:474px;left:559px;white-space:nowrap" class="ft158">T5,&#160;XLNet,<br/>T0,&#160;BLOOM,<br/>Alpaca,&#160;GPT-<br/>4,&#160;OPT,&#160;Dolly,<br/>GPT-3.5,<br/>LLaMA,&#160;MPT,<br/>Vicuna,&#160;GPT-<br/>2,&#160;StableLM,<br/>GPT-3</p>
<p style="position:absolute;top:474px;left:632px;white-space:nowrap" class="ft158">HILT&#160;(Own<br/>dataset)</p>
<p style="position:absolute;top:473px;left:704px;white-space:nowrap" class="ft159">•&#160;The&#160;paper<br/>annotated&#160;only<br/>one&#160;category<br/>per&#160;sentence&#160;for<br/>simplicity.<br/>•&#160;The&#160;defined<br/>hallucination<br/>categories<br/>might&#160;not&#160;cover<br/>emerging&#160;types.<br/>•&#160;The&#160;bench-<br/>mark&#160;includes</p>
<p style="position:absolute;top:599px;left:704px;white-space:nowrap" class="ft150">15&#160;contempo-</p>
<p style="position:absolute;top:610px;left:704px;white-space:nowrap" class="ft158">rary&#160;models&#160;and<br/>might&#160;overlook<br/>recent&#160;LLM<br/>developments.</p>
<p style="position:absolute;top:660px;left:196px;white-space:nowrap" class="ft150">Retrieval</p>
<p style="position:absolute;top:671px;left:192px;white-space:nowrap" class="ft150">Augmented</p>
<p style="position:absolute;top:681px;left:193px;white-space:nowrap" class="ft150">Generation</p>
<p style="position:absolute;top:691px;left:189px;white-space:nowrap" class="ft150">(End-to-End)</p>
<p style="position:absolute;top:660px;left:258px;white-space:nowrap" class="ft158">Retrieval-<br/>Augmented<br/>Generation&#160;for<br/>Knowledge-<br/>Intensive&#160;NLP&#160;Tasks<br/><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Lewis&#160;et&#160;al.,&#160;2021)</a></p>
<p style="position:absolute;top:659px;left:352px;white-space:nowrap" class="ft156">×</p>
<p style="position:absolute;top:660px;left:403px;white-space:nowrap" class="ft158">Open-<br/>domain&#160;QA,<br/>Abstractive<br/>QA,&#160;Jeopardy<br/>QG,&#160;Fact<br/>Verification</p>
<p style="position:absolute;top:660px;left:476px;white-space:nowrap" class="ft158">BLEU-1,<br/>Q-BLEU-1,<br/>Rouge-L</p>
<p style="position:absolute;top:660px;left:559px;white-space:nowrap" class="ft158">T5&#160;11B,<br/>BART</p>
<p style="position:absolute;top:660px;left:632px;white-space:nowrap" class="ft150">NQ,&#160;TriviaQA,</p>
<p style="position:absolute;top:671px;left:631px;white-space:nowrap" class="ft150">WebQuestions,</p>
<p style="position:absolute;top:681px;left:632px;white-space:nowrap" class="ft158">CuratedTrec,<br/>MSMARCO,<br/>SearchQA,<br/>FEVER-3,<br/>FEVER-2</p>
<p style="position:absolute;top:660px;left:704px;white-space:nowrap" class="ft158">Details&#160;not<br/>provided</p>
<p style="position:absolute;top:742px;left:129px;white-space:nowrap" class="ft150">s</p>
<p style="position:absolute;top:742px;left:205px;white-space:nowrap" class="ft150">Self-</p>
<p style="position:absolute;top:752px;left:193px;white-space:nowrap" class="ft150">refinement</p>
<p style="position:absolute;top:763px;left:199px;white-space:nowrap" class="ft150">through</p>
<p style="position:absolute;top:773px;left:189px;white-space:nowrap" class="ft150">feedback&#160;and</p>
<p style="position:absolute;top:783px;left:195px;white-space:nowrap" class="ft150">reasoning</p>
<p style="position:absolute;top:742px;left:258px;white-space:nowrap" class="ft158">Prompting&#160;GPT-3&#160;to<br/>Be&#160;Reliable&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Si&#160;et&#160;al.,<br/>2022)</a></p>
<p style="position:absolute;top:742px;left:352px;white-space:nowrap" class="ft154">✓</p>
<p style="position:absolute;top:742px;left:403px;white-space:nowrap" class="ft150">QA</p>
<p style="position:absolute;top:742px;left:476px;white-space:nowrap" class="ft158">Accuracy,<br/>Expected<br/>Calibration&#160;Error<br/>(ECE)&#160;and&#160;Brier<br/>score</p>
<p style="position:absolute;top:742px;left:559px;white-space:nowrap" class="ft158">DPR-BERT,<br/>GPT-3</p>
<p style="position:absolute;top:742px;left:632px;white-space:nowrap" class="ft158">NQ,&#160;TriviaQA,<br/>and&#160;HotpotQA</p>
<p style="position:absolute;top:741px;left:704px;white-space:nowrap" class="ft158">•&#160;Explores&#160;four<br/>reliability&#160;facets<br/>but&#160;doesn’t<br/>analyze&#160;to&#160;un-<br/>derstand&#160;model<br/>behaviors.</p>
<p style="position:absolute;top:813px;left:258px;white-space:nowrap" class="ft1511">Self-Contradictory<br/>Hallucinations&#160;of<br/>LLMs:&#160;Evaluation,<br/>Detection&#160;and<br/>Mitigation&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Mündler<br/>et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:813px;left:352px;white-space:nowrap" class="ft154">✓</p>
<p style="position:absolute;top:813px;left:403px;white-space:nowrap" class="ft158">Open-<br/>domain&#160;text<br/>generation,</p>
<p style="position:absolute;top:813px;left:476px;white-space:nowrap" class="ft158">Self&#160;contra<br/>reduced,&#160;Informa-<br/>tive&#160;facts&#160;retained,<br/>Perplexity<br/>increased</p>
<p style="position:absolute;top:813px;left:559px;white-space:nowrap" class="ft158">GPT-4,&#160;GPT-<br/>3.5,&#160;Llama2<br/>70B&#160;Chat,&#160;and<br/>Vicuna</p>
<p style="position:absolute;top:813px;left:632px;white-space:nowrap" class="ft150">Manual</p>
<p style="position:absolute;top:813px;left:704px;white-space:nowrap" class="ft158">Details&#160;not<br/>provided</p>
<p style="position:absolute;top:884px;left:258px;white-space:nowrap" class="ft158">Mind’s&#160;Mirror:<br/>Distilling<br/>Self-Evaluation<br/>Capability&#160;and<br/>Comprehensive<br/>Thinking&#160;from&#160;Large<br/>Language&#160;Models<br/><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Liu&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:884px;left:352px;white-space:nowrap" class="ft154">✓</p>
<p style="position:absolute;top:884px;left:403px;white-space:nowrap" class="ft158">Short-form<br/>QA,&#160;biography<br/>generation,<br/>and&#160;reasoning,</p>
<p style="position:absolute;top:884px;left:476px;white-space:nowrap" class="ft158">Exact&#160;match<br/>(EM)&#160;and<br/>F1-score,<br/>recall@5,<br/>FACTSCORE</p>
<p style="position:absolute;top:884px;left:559px;white-space:nowrap" class="ft158">InstructGPT,<br/>Llama&#160;2&#160;7B<br/>Chat,<br/>Llama&#160;2&#160;13B<br/>Chat,<br/>Llama&#160;1&#160;65B<br/>and&#160;GPT-3.5</p>
<p style="position:absolute;top:884px;left:632px;white-space:nowrap" class="ft158">HotPotQA,<br/>TriviaQA,<br/>ALCE-<br/>Qampari<br/>QA<br/>,bio&#160;bench-<br/>mark</p>
<p style="position:absolute;top:883px;left:704px;white-space:nowrap" class="ft158">•&#160;Experiments<br/>are&#160;conducted<br/>primarily<br/>utilizing&#160;only&#160;a<br/>single&#160;teacher<br/>model,&#160;GPT-<br/>3.5,&#160;and&#160;one<br/>student&#160;model,<br/>T5-Base.<br/>•&#160;The&#160;work<br/>only&#160;evaluates<br/>their&#160;methods<br/>on&#160;three<br/>different&#160;NLP<br/>tasks.<br/>•&#160;Flaws&#160;or<br/>biases&#160;in&#160;the<br/>LLMs&#160;self-<br/>evaluation<br/>mechanism&#160;may<br/>propagate&#160;to&#160;the<br/>distilled&#160;SLM.</p>
<p style="position:absolute;top:1124px;left:360px;white-space:nowrap" class="ft151">Continued&#160;on&#160;the&#160;next&#160;page</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft160{font-size:8px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft161{font-size:8px;font-family:YOPPJI+NimbusRomNo9L-ReguItal;color:#000000;}
	.ft162{font-size:8px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft163{font-size:8px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft164{font-size:8px;font-family:LNIRBF+MSAM7;color:#009600;}
	.ft165{font-size:8px;font-family:LNLVHQ+CMSY6;color:#000000;}
	.ft166{font-size:8px;line-height:10px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft167{font-size:8px;line-height:10px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft168{font-size:8px;line-height:10px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft169{font-size:8px;line-height:11px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft1610{font-size:8px;line-height:9px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page16-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html016.png" alt="background image"/>
<p style="position:absolute;top:113px;left:368px;white-space:nowrap" class="ft160">Table&#160;1&#160;–&#160;Continued&#160;from&#160;the&#160;previous&#160;page</p>
<p style="position:absolute;top:133px;left:129px;white-space:nowrap" class="ft162">Category</p>
<p style="position:absolute;top:133px;left:186px;white-space:nowrap" class="ft166">Mitigation<br/>Technique(s)</p>
<p style="position:absolute;top:133px;left:258px;white-space:nowrap" class="ft162">Title</p>
<p style="position:absolute;top:133px;left:352px;white-space:nowrap" class="ft162">Detection</p>
<p style="position:absolute;top:133px;left:403px;white-space:nowrap" class="ft162">Task(s)</p>
<p style="position:absolute;top:133px;left:476px;white-space:nowrap" class="ft162">Metric(s)</p>
<p style="position:absolute;top:133px;left:559px;white-space:nowrap" class="ft166">Evaluated<br/>LLM(s)</p>
<p style="position:absolute;top:133px;left:632px;white-space:nowrap" class="ft162">Dataset(s)</p>
<p style="position:absolute;top:133px;left:704px;white-space:nowrap" class="ft162">Limitation(s)</p>
<p style="position:absolute;top:162px;left:258px;white-space:nowrap" class="ft168">Towards&#160;Mitigating<br/>Hallucination&#160;in<br/>Large&#160;Language<br/>Models&#160;via<br/>Self-Reflection&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Ji<br/>et&#160;al.,&#160;2023b)</a></p>
<p style="position:absolute;top:162px;left:352px;white-space:nowrap" class="ft164">✓</p>
<p style="position:absolute;top:162px;left:403px;white-space:nowrap" class="ft167">Medical<br/>generative&#160;QA</p>
<p style="position:absolute;top:162px;left:476px;white-space:nowrap" class="ft167">unigram&#160;F1&#160;and<br/>ROUGE-L</p>
<p style="position:absolute;top:162px;left:559px;white-space:nowrap" class="ft167">Vicuna,<br/>Alpaca-<br/>LoRA,<br/>GPT-3.5,<br/>MedAlpaca,<br/>Robin-medical</p>
<p style="position:absolute;top:162px;left:632px;white-space:nowrap" class="ft167">PubMedQA,<br/>MedQuAD,<br/>MEDIQA<br/>2019,<br/>LiveMedQA<br/>2017,&#160;MASH-<br/>QA</p>
<p style="position:absolute;top:161px;left:704px;white-space:nowrap" class="ft167">•&#160;The&#160;study<br/>is&#160;limited<br/>to&#160;English<br/>medical&#160;queries,<br/>limiting&#160;the<br/>generalizability<br/>to&#160;other<br/>languages,<br/>domains,&#160;and<br/>modalities.</p>
<p style="position:absolute;top:275px;left:258px;white-space:nowrap" class="ft168">On&#160;What&#160;Basis?<br/>Predicting&#160;Text<br/>Preference&#160;Via<br/>Structured<br/>Comparative<br/>Reasoning&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Yan<br/>et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:275px;left:352px;white-space:nowrap" class="ft164">✓</p>
<p style="position:absolute;top:275px;left:403px;white-space:nowrap" class="ft167">Summarization,<br/>retrieval,&#160;and<br/>automatic<br/>rating</p>
<p style="position:absolute;top:275px;left:476px;white-space:nowrap" class="ft160">Accuracy</p>
<p style="position:absolute;top:275px;left:559px;white-space:nowrap" class="ft167">GPT-3.5&#160;and<br/>GPT-4</p>
<p style="position:absolute;top:275px;left:632px;white-space:nowrap" class="ft167">TL;DR,<br/>RLAIF-HH<br/>and&#160;TREC<br/>News</p>
<p style="position:absolute;top:274px;left:704px;white-space:nowrap" class="ft167">•&#160;The&#160;eval-<br/>uation&#160;is<br/>conducted&#160;on<br/>a&#160;sample&#160;set&#160;of<br/>datasets.<br/>•&#160;Consistency<br/>measurement<br/>uses&#160;approxi-<br/>mate&#160;metrics<br/>rather&#160;than<br/>more&#160;rigorous<br/>schemes.</p>
<p style="position:absolute;top:409px;left:258px;white-space:nowrap" class="ft167">DRESS:&#160;Instructing<br/>Large<br/>Vision-Language<br/>Models&#160;to&#160;Align&#160;and<br/>Interact&#160;with<br/>Humans&#160;via&#160;Natural<br/>Language&#160;Feedback<br/><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Chen&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:409px;left:352px;white-space:nowrap" class="ft164">✓</p>
<p style="position:absolute;top:409px;left:403px;white-space:nowrap" class="ft160">Visual&#160;QA</p>
<p style="position:absolute;top:409px;left:476px;white-space:nowrap" class="ft167">Helpfulness,<br/>honesty&#160;and<br/>harmlessness</p>
<p style="position:absolute;top:409px;left:559px;white-space:nowrap" class="ft167">BLIP-2&#160;with<br/>T5-XXL,<br/>LLaVA&#160;with<br/>LLaMA-13B,<br/>LLaVA-HF<br/>with&#160;Vicuna,<br/>InstructBLIP<br/>with&#160;Vicuna,<br/>MiniGPT-4<br/>with&#160;Vicuna,<br/>mPLUG-<br/>Owl&#160;with<br/>LLaMA-7B</p>
<p style="position:absolute;top:409px;left:632px;white-space:nowrap" class="ft167">BLIP,&#160;CC3M,<br/>CC12M,&#160;SBU,<br/>LLaVA&#160;and<br/>VLSafe&#160;(Own<br/>dataset)</p>
<p style="position:absolute;top:409px;left:704px;white-space:nowrap" class="ft167">Details&#160;not<br/>provided</p>
<p style="position:absolute;top:553px;left:258px;white-space:nowrap" class="ft168">The&#160;Knowledge<br/>Alignment&#160;Problem:<br/>Bridging&#160;Human<br/>and&#160;External<br/>Knowledge&#160;for<br/>Large&#160;Language<br/>Models&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Zhang<br/>et&#160;al.,&#160;2023b)</a></p>
<p style="position:absolute;top:553px;left:352px;white-space:nowrap" class="ft164">✓</p>
<p style="position:absolute;top:553px;left:403px;white-space:nowrap" class="ft160">QA</p>
<p style="position:absolute;top:553px;left:476px;white-space:nowrap" class="ft167">G-EVAL:<br/>Gold&#160;Answer<br/>Coverage,<br/>Hallucination,<br/>Accepted</p>
<p style="position:absolute;top:553px;left:559px;white-space:nowrap" class="ft160">GPT-3.5,</p>
<p style="position:absolute;top:553px;left:632px;white-space:nowrap" class="ft160">FuzzyQA</p>
<p style="position:absolute;top:553px;left:704px;white-space:nowrap" class="ft167">•&#160;The&#160;addi-<br/>tional&#160;clarifi-<br/>cation&#160;steps<br/>increase&#160;the<br/>computational<br/>load&#160;and&#160;time<br/>consumption.</p>
<p style="position:absolute;top:645px;left:258px;white-space:nowrap" class="ft168">Chain-of-<br/>Verification&#160;Reduces<br/>Hallucination&#160;in<br/>Large&#160;Language<br/>Models&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Dhuliawala<br/>et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:645px;left:352px;white-space:nowrap" class="ft164">✓</p>
<p style="position:absolute;top:645px;left:403px;white-space:nowrap" class="ft160">QA</p>
<p style="position:absolute;top:645px;left:476px;white-space:nowrap" class="ft160">Precision</p>
<p style="position:absolute;top:645px;left:559px;white-space:nowrap" class="ft160">Llama&#160;65B,</p>
<p style="position:absolute;top:645px;left:632px;white-space:nowrap" class="ft167">QUEST,<br/>MultiSpan-QA</p>
<p style="position:absolute;top:645px;left:704px;white-space:nowrap" class="ft167">•&#160;The&#160;work<br/>only&#160;addresses<br/>hallucinations<br/>in&#160;the&#160;form<br/>of&#160;directly<br/>stated&#160;factual<br/>inaccuracies.<br/>•&#160;Computa-<br/>tional&#160;cost&#160;is<br/>increased&#160;due<br/>to&#160;generating<br/>verification<br/>statements<br/>and&#160;addi-<br/>tional&#160;model<br/>deliberation.</p>
<p style="position:absolute;top:821px;left:258px;white-space:nowrap" class="ft168">Chain&#160;of&#160;Natural<br/>Language&#160;Inference<br/>for&#160;Reducing&#160;Large<br/>Language&#160;Model<br/>Ungrounded<br/>Hallucinations&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Lei<br/>et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:821px;left:352px;white-space:nowrap" class="ft164">✓</p>
<p style="position:absolute;top:821px;left:403px;white-space:nowrap" class="ft167">Summarization<br/>and&#160;question<br/>answering</p>
<p style="position:absolute;top:821px;left:476px;white-space:nowrap" class="ft160">F1,Rouge-</p>
<p style="position:absolute;top:832px;left:475px;white-space:nowrap" class="ft167">1,2,&#160;L,&#160;Bleu-<br/>4,&#160;BertScore,</p>
<p style="position:absolute;top:853px;left:476px;white-space:nowrap" class="ft167">FactCC&#160;and<br/>AlignScore-Large</p>
<p style="position:absolute;top:821px;left:559px;white-space:nowrap" class="ft167">GPT-3.5&#160;and<br/>GPT-4</p>
<p style="position:absolute;top:821px;left:632px;white-space:nowrap" class="ft167">HaluEVAL,<br/>FactCC,<br/>SummEval,<br/>QAGS-Xsum,<br/>QAGS-<br/>CNNDM</p>
<p style="position:absolute;top:821px;left:704px;white-space:nowrap" class="ft167">Details&#160;not<br/>provided</p>
<p style="position:absolute;top:903px;left:199px;white-space:nowrap" class="ft167">Prompt<br/>Tuning</p>
<p style="position:absolute;top:903px;left:258px;white-space:nowrap" class="ft168">UPRISE:&#160;Universal<br/>Prompt&#160;Retrieval&#160;for<br/>Improving<br/>Zero-Shot<br/>Evaluation&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Cheng<br/>et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:903px;left:352px;white-space:nowrap" class="ft164">✓</p>
<p style="position:absolute;top:903px;left:403px;white-space:nowrap" class="ft160">QA</p>
<p style="position:absolute;top:903px;left:476px;white-space:nowrap" class="ft160">Accuracy</p>
<p style="position:absolute;top:903px;left:559px;white-space:nowrap" class="ft167">Gpt-3.5,<br/>Gpt-Neo-2.7B,<br/>BLOOM-7.1B,<br/>OPT-66B,<br/>GPT3</p>
<p style="position:absolute;top:903px;left:632px;white-space:nowrap" class="ft167">TruthfulQA,<br/>FEVER2.0,<br/>the&#160;scientific<br/>spilt&#160;of<br/>Covid-19</p>
<p style="position:absolute;top:902px;left:704px;white-space:nowrap" class="ft167">•&#160;It&#160;displays<br/>limited&#160;impact<br/>on&#160;tasks&#160;that<br/>are&#160;directly<br/>formulated<br/>as&#160;language<br/>modeling,&#160;such<br/>as&#160;coreference<br/>resolution&#160;and<br/>commonsense<br/>reasoning.</p>
<p style="position:absolute;top:1026px;left:258px;white-space:nowrap" class="ft167">Teaching&#160;Language<br/>Models&#160;to<br/>Hallucinate&#160;Less<br/>with&#160;Synthetic&#160;Tasks<br/><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Jones&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:1026px;left:352px;white-space:nowrap" class="ft164">✓</p>
<p style="position:absolute;top:1026px;left:403px;white-space:nowrap" class="ft167">Search-and-<br/>retrieve,<br/>meeting<br/>summariza-<br/>tion,&#160;and<br/>clinical&#160;report<br/>generation</p>
<p style="position:absolute;top:1026px;left:476px;white-space:nowrap" class="ft167">ROUGE-1,<br/>ROUGE-2,&#160;and<br/>ROUGE-L</p>
<p style="position:absolute;top:1026px;left:559px;white-space:nowrap" class="ft160">Vicuna,&#160;v1.1</p>
<p style="position:absolute;top:1037px;left:558px;white-space:nowrap" class="ft160">13B,&#160;GPT-4</p>
<p style="position:absolute;top:1026px;left:632px;white-space:nowrap" class="ft167">MS&#160;MARCO,<br/>QMSum,<br/>ACI-Bench</p>
<p style="position:absolute;top:1026px;left:704px;white-space:nowrap" class="ft167">•&#160;It&#160;requires<br/>designing&#160;a<br/>synthetic&#160;task,<br/>and&#160;reduces<br/>hallucination&#160;on<br/>some&#160;models<br/>more&#160;than<br/>others.</p>
<p style="position:absolute;top:1120px;left:360px;white-space:nowrap" class="ft161">Continued&#160;on&#160;the&#160;next&#160;page</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft170{font-size:8px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft171{font-size:8px;font-family:YOPPJI+NimbusRomNo9L-ReguItal;color:#000000;}
	.ft172{font-size:8px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft173{font-size:8px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft174{font-size:8px;font-family:LNLVHQ+CMSY6;color:#ff0000;}
	.ft175{font-size:8px;font-family:LNIRBF+MSAM7;color:#009600;}
	.ft176{font-size:8px;font-family:LNLVHQ+CMSY6;color:#000000;}
	.ft177{font-size:8px;line-height:10px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft178{font-size:8px;line-height:10px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft179{font-size:8px;line-height:10px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft1710{font-size:8px;line-height:11px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft1711{font-size:8px;line-height:9px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page17-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html017.png" alt="background image"/>
<p style="position:absolute;top:113px;left:368px;white-space:nowrap" class="ft170">Table&#160;1&#160;–&#160;Continued&#160;from&#160;the&#160;previous&#160;page</p>
<p style="position:absolute;top:133px;left:129px;white-space:nowrap" class="ft172">Category</p>
<p style="position:absolute;top:133px;left:186px;white-space:nowrap" class="ft177">Mitigation<br/>Technique(s)</p>
<p style="position:absolute;top:133px;left:258px;white-space:nowrap" class="ft172">Title</p>
<p style="position:absolute;top:133px;left:352px;white-space:nowrap" class="ft172">Detection</p>
<p style="position:absolute;top:133px;left:403px;white-space:nowrap" class="ft172">Task(s)</p>
<p style="position:absolute;top:133px;left:476px;white-space:nowrap" class="ft172">Metric(s)</p>
<p style="position:absolute;top:133px;left:559px;white-space:nowrap" class="ft177">Evaluated<br/>LLM(s)</p>
<p style="position:absolute;top:133px;left:632px;white-space:nowrap" class="ft172">Dataset(s)</p>
<p style="position:absolute;top:133px;left:704px;white-space:nowrap" class="ft172">Limitation(s)</p>
<p style="position:absolute;top:162px;left:129px;white-space:nowrap" class="ft178">Developing<br/>Models</p>
<p style="position:absolute;top:162px;left:192px;white-space:nowrap" class="ft170">Introducing</p>
<p style="position:absolute;top:173px;left:188px;white-space:nowrap" class="ft170">new&#160;decoding</p>
<p style="position:absolute;top:183px;left:199px;white-space:nowrap" class="ft170">strategy</p>
<p style="position:absolute;top:162px;left:258px;white-space:nowrap" class="ft178">Trusting&#160;Your<br/>Evidence:<br/>Hallucinate&#160;Less<br/>with&#160;Context-aware<br/>Decoding&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Shi&#160;et&#160;al.,<br/>2023)</a></p>
<p style="position:absolute;top:161px;left:352px;white-space:nowrap" class="ft174">×</p>
<p style="position:absolute;top:162px;left:403px;white-space:nowrap" class="ft178">Summarization,<br/>Knowledge<br/>Conflicts</p>
<p style="position:absolute;top:162px;left:476px;white-space:nowrap" class="ft178">ROUGE-L,<br/>BERT-Precision,<br/>FactKB</p>
<p style="position:absolute;top:162px;left:559px;white-space:nowrap" class="ft178">OPT(13B&#160;and<br/>30B),&#160;GPT-<br/>Neo&#160;(2.7B&#160;and<br/>20B),&#160;LLaMA<br/>(13B&#160;and&#160;30B)<br/>and&#160;FLAN-T5<br/>(XL&#160;3B&#160;and<br/>XXL&#160;11B)</p>
<p style="position:absolute;top:162px;left:632px;white-space:nowrap" class="ft178">CNN-DM,<br/>XSUM,<br/>MemoTrap,<br/>NQ-Swap</p>
<p style="position:absolute;top:162px;left:704px;white-space:nowrap" class="ft178">Details&#160;not<br/>provided</p>
<p style="position:absolute;top:254px;left:258px;white-space:nowrap" class="ft179">DOLA:&#160;Decoding&#160;by<br/>Contrasting&#160;Layers<br/>Improves&#160;Factuality<br/>in&#160;Large&#160;Language<br/>Models&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Chuang<br/>et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:254px;left:352px;white-space:nowrap" class="ft175">✓</p>
<p style="position:absolute;top:254px;left:403px;white-space:nowrap" class="ft178">Multiple<br/>choices<br/>tasks&#160;and<br/>open-ended<br/>generation</p>
<p style="position:absolute;top:254px;left:476px;white-space:nowrap" class="ft178">GPT-4&#160;automatic<br/>evaluation</p>
<p style="position:absolute;top:254px;left:559px;white-space:nowrap" class="ft170">LLaMA-(7B,</p>
<p style="position:absolute;top:265px;left:558px;white-space:nowrap" class="ft170">13B,&#160;33B,</p>
<p style="position:absolute;top:275px;left:559px;white-space:nowrap" class="ft178">65B)&#160;and<br/>GPT4</p>
<p style="position:absolute;top:254px;left:632px;white-space:nowrap" class="ft178">TruthfulQA,<br/>FACTOR,<br/>StrategyQA,<br/>GSM8K,<br/>Vicuna&#160;QA</p>
<p style="position:absolute;top:254px;left:704px;white-space:nowrap" class="ft178">•&#160;The&#160;work<br/>doesn’t&#160;explore<br/>performance<br/>in&#160;other<br/>dimensions<br/>like&#160;instruction<br/>following<br/>or&#160;learning<br/>from&#160;human<br/>feedback.<br/>•&#160;It&#160;relies<br/>on&#160;existing<br/>architecture<br/>and&#160;pre-trained<br/>parameters,<br/>omitting&#160;the<br/>utilization&#160;of<br/>human&#160;labels<br/>or&#160;factual<br/>knowledge<br/>bases&#160;for<br/>fine-tuning,<br/>thereby&#160;limiting<br/>potential<br/>improvements.<br/>•&#160;This&#160;method<br/>solely&#160;relies<br/>on&#160;the&#160;model’s<br/>internal<br/>knowledge&#160;and<br/>lacks&#160;external<br/>retrieval<br/>modules,&#160;which<br/>may&#160;result&#160;in<br/>an&#160;inability<br/>to&#160;correct<br/>misinformation<br/>acquired&#160;during<br/>training.</p>
<p style="position:absolute;top:671px;left:258px;white-space:nowrap" class="ft179">Inference-Time<br/>Intervention:<br/>Eliciting&#160;Truthful<br/>Answers&#160;from&#160;a<br/>Language&#160;Model&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Li<br/>et&#160;al.,&#160;2023a)</a></p>
<p style="position:absolute;top:670px;left:352px;white-space:nowrap" class="ft175">✓</p>
<p style="position:absolute;top:671px;left:403px;white-space:nowrap" class="ft170">QA</p>
<p style="position:absolute;top:671px;left:476px;white-space:nowrap" class="ft178">Attributable&#160;to<br/>Identified<br/>Sources&#160;(AIS),<br/>True*Informative<br/>(%),&#160;True&#160;(%),<br/>MC&#160;acc.&#160;(%),&#160;CE<br/>(pre-training&#160;loss)<br/>and&#160;KL<br/>(divergence<br/>between<br/>next-token<br/>distributions&#160;pre-<br/>and<br/>post-intervention)</p>
<p style="position:absolute;top:671px;left:559px;white-space:nowrap" class="ft178">LLaMA,<br/>Alpaca&#160;and<br/>Vicuna</p>
<p style="position:absolute;top:671px;left:632px;white-space:nowrap" class="ft170">TruthfulQA</p>
<p style="position:absolute;top:671px;left:704px;white-space:nowrap" class="ft178">Details&#160;not<br/>provided</p>
<p style="position:absolute;top:825px;left:189px;white-space:nowrap" class="ft170">Utilization&#160;of</p>
<p style="position:absolute;top:836px;left:192px;white-space:nowrap" class="ft170">Knowledge</p>
<p style="position:absolute;top:846px;left:201px;white-space:nowrap" class="ft170">Graph</p>
<p style="position:absolute;top:825px;left:258px;white-space:nowrap" class="ft178">RHO:&#160;Reducing<br/>Hallucination&#160;in<br/>Open-domain<br/>Dialogues&#160;with<br/>Knowledge<br/>Grounding&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Ji&#160;et&#160;al.,<br/>2023a)</a></p>
<p style="position:absolute;top:825px;left:352px;white-space:nowrap" class="ft174">×</p>
<p style="position:absolute;top:825px;left:403px;white-space:nowrap" class="ft178">Open-domain<br/>dialogue<br/>response<br/>generation</p>
<p style="position:absolute;top:825px;left:476px;white-space:nowrap" class="ft178">BLEU,&#160;ROUGE-<br/>L</p>
<p style="position:absolute;top:825px;left:559px;white-space:nowrap" class="ft178">GPT-2,&#160;BART,<br/>GPT-3.5</p>
<p style="position:absolute;top:825px;left:632px;white-space:nowrap" class="ft170">OpenDialKG</p>
<p style="position:absolute;top:825px;left:704px;white-space:nowrap" class="ft178">•&#160;The&#160;model<br/>identifies<br/>statistical<br/>patterns&#160;and<br/>quantitative<br/>links&#160;among<br/>variables&#160;but<br/>cannot&#160;perceive<br/>qualitative<br/>relationships<br/>like&#160;causality,<br/>hierarchy,<br/>and&#160;other<br/>abstractions.</p>
<p style="position:absolute;top:980px;left:258px;white-space:nowrap" class="ft179">FLEEK:&#160;Factual<br/>Error&#160;Detection&#160;and<br/>Correction&#160;with<br/>Evidence&#160;Retrieved<br/>from&#160;External<br/>Knowledge&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Bayat<br/>et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:980px;left:352px;white-space:nowrap" class="ft175">✓</p>
<p style="position:absolute;top:980px;left:403px;white-space:nowrap" class="ft178">Fact&#160;verifica-<br/>tion&#160;and&#160;Fact<br/>Revision,</p>
<p style="position:absolute;top:980px;left:476px;white-space:nowrap" class="ft178">Accuracy,<br/>precision,&#160;recall,<br/>and&#160;F1&#160;score</p>
<p style="position:absolute;top:980px;left:559px;white-space:nowrap" class="ft178">Vicuna&#160;and<br/>GPT-3</p>
<p style="position:absolute;top:980px;left:632px;white-space:nowrap" class="ft178">BenchLLM<br/>and&#160;BenchText</p>
<p style="position:absolute;top:980px;left:704px;white-space:nowrap" class="ft178">•&#160;The&#160;current<br/>system&#160;relies<br/>on&#160;the&#160;initial<br/>set&#160;of&#160;responses<br/>generated<br/>by&#160;LLMs&#160;to<br/>execute&#160;tasks.<br/>•&#160;The&#160;experi-<br/>ments&#160;presented<br/>are&#160;based&#160;on<br/>small-scale<br/>datasets.</p>
<p style="position:absolute;top:1115px;left:360px;white-space:nowrap" class="ft171">Continued&#160;on&#160;the&#160;next&#160;page</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft180{font-size:8px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft181{font-size:8px;font-family:YOPPJI+NimbusRomNo9L-ReguItal;color:#000000;}
	.ft182{font-size:8px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft183{font-size:8px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft184{font-size:8px;font-family:LNLVHQ+CMSY6;color:#ff0000;}
	.ft185{font-size:8px;font-family:LNLVHQ+CMSY6;color:#000000;}
	.ft186{font-size:8px;font-family:LNIRBF+MSAM7;color:#009600;}
	.ft187{font-size:8px;line-height:10px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft188{font-size:8px;line-height:10px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft189{font-size:8px;line-height:10px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft1810{font-size:8px;line-height:11px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft1811{font-size:8px;line-height:9px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page18-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html018.png" alt="background image"/>
<p style="position:absolute;top:113px;left:368px;white-space:nowrap" class="ft180">Table&#160;1&#160;–&#160;Continued&#160;from&#160;the&#160;previous&#160;page</p>
<p style="position:absolute;top:133px;left:129px;white-space:nowrap" class="ft182">Category</p>
<p style="position:absolute;top:133px;left:186px;white-space:nowrap" class="ft187">Mitigation<br/>Technique(s)</p>
<p style="position:absolute;top:133px;left:258px;white-space:nowrap" class="ft182">Title</p>
<p style="position:absolute;top:133px;left:352px;white-space:nowrap" class="ft182">Detection</p>
<p style="position:absolute;top:133px;left:403px;white-space:nowrap" class="ft182">Task(s)</p>
<p style="position:absolute;top:133px;left:476px;white-space:nowrap" class="ft182">Metric(s)</p>
<p style="position:absolute;top:133px;left:559px;white-space:nowrap" class="ft187">Evaluated<br/>LLM(s)</p>
<p style="position:absolute;top:133px;left:632px;white-space:nowrap" class="ft182">Dataset(s)</p>
<p style="position:absolute;top:133px;left:704px;white-space:nowrap" class="ft182">Limitation(s)</p>
<p style="position:absolute;top:162px;left:192px;white-space:nowrap" class="ft180">Introducing</p>
<p style="position:absolute;top:173px;left:191px;white-space:nowrap" class="ft180">faithfulness-</p>
<p style="position:absolute;top:183px;left:194px;white-space:nowrap" class="ft180">based&#160;loss</p>
<p style="position:absolute;top:194px;left:198px;white-space:nowrap" class="ft180">function</p>
<p style="position:absolute;top:162px;left:258px;white-space:nowrap" class="ft189">Information-<br/>Theoretic&#160;Text<br/>Hallucination<br/>Reduction&#160;for<br/>Video-grounded<br/>Dialogue&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Yoon<br/>et&#160;al.,&#160;2022)</a></p>
<p style="position:absolute;top:161px;left:352px;white-space:nowrap" class="ft184">×</p>
<p style="position:absolute;top:162px;left:403px;white-space:nowrap" class="ft188">Video-<br/>grounded<br/>Dialogues</p>
<p style="position:absolute;top:162px;left:476px;white-space:nowrap" class="ft188">BLEU,<br/>METEOR,<br/>ROUGE-L,<br/>CIDEr</p>
<p style="position:absolute;top:162px;left:559px;white-space:nowrap" class="ft180">T5</p>
<p style="position:absolute;top:162px;left:632px;white-space:nowrap" class="ft188">AVSD@<br/>DSTC7,<br/>AVSD@<br/>DSTC8</p>
<p style="position:absolute;top:161px;left:704px;white-space:nowrap" class="ft188">•&#160;It&#160;requires<br/>pre-training<br/>each&#160;language<br/>model&#160;in&#160;a&#160;two-<br/>stage&#160;training<br/>mechanism&#160;to<br/>mitigate&#160;text<br/>hallucination.</p>
<p style="position:absolute;top:254px;left:258px;white-space:nowrap" class="ft189">Detecting&#160;and<br/>Mitigating<br/>Hallucinations&#160;in<br/>Multilingual<br/>Summarisation&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Qiu<br/>et&#160;al.,&#160;2023b)</a></p>
<p style="position:absolute;top:254px;left:352px;white-space:nowrap" class="ft186">✓</p>
<p style="position:absolute;top:254px;left:403px;white-space:nowrap" class="ft188">Multilingual<br/>Summarisa-<br/>tion</p>
<p style="position:absolute;top:254px;left:476px;white-space:nowrap" class="ft188">faithfulness,<br/>DAE,&#160;QAFactE-<br/>val,ENFS%,<br/>EntFA,&#160;ROUGE-</p>
<p style="position:absolute;top:296px;left:475px;white-space:nowrap" class="ft180">1/2/L&#160;scores</p>
<p style="position:absolute;top:254px;left:559px;white-space:nowrap" class="ft188">BLOOMZ-P3-<br/>7.1B,&#160;Vicuna,<br/>Phoenix-7B,</p>
<p style="position:absolute;top:254px;left:632px;white-space:nowrap" class="ft180">XL-Sum</p>
<p style="position:absolute;top:254px;left:704px;white-space:nowrap" class="ft188">•&#160;It&#160;uses<br/>machine<br/>translation<br/>to&#160;construct<br/>training&#160;data,<br/>which&#160;may&#160;limit<br/>feasibility&#160;for<br/>other&#160;languages.<br/>Translation<br/>errors&#160;may&#160;also<br/>limit&#160;metric<br/>quality.<br/>•&#160;The&#160;weighted-<br/>loss&#160;approach<br/>has&#160;inconsistent<br/>gains&#160;in&#160;faith-<br/>fulness&#160;across<br/>languages.</p>
<p style="position:absolute;top:451px;left:193px;white-space:nowrap" class="ft180">Supervised</p>
<p style="position:absolute;top:461px;left:194px;white-space:nowrap" class="ft180">finetuning</p>
<p style="position:absolute;top:451px;left:258px;white-space:nowrap" class="ft189">HALO:&#160;Estimation<br/>and&#160;Reduction&#160;of<br/>Hallucinations&#160;in<br/>Open-Source&#160;Weak<br/>Large&#160;Language<br/>Models&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#11">(Elaraby<br/>et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:451px;left:352px;white-space:nowrap" class="ft186">✓</p>
<p style="position:absolute;top:451px;left:403px;white-space:nowrap" class="ft180">QA</p>
<p style="position:absolute;top:451px;left:476px;white-space:nowrap" class="ft188">HaloCheck&#160;(Own<br/>metric)</p>
<p style="position:absolute;top:451px;left:559px;white-space:nowrap" class="ft188">BLOOM&#160;7B,<br/>GPT-4</p>
<p style="position:absolute;top:451px;left:632px;white-space:nowrap" class="ft180">Manual</p>
<p style="position:absolute;top:450px;left:704px;white-space:nowrap" class="ft188">•&#160;The&#160;study<br/>includes&#160;only<br/>one&#160;example&#160;of<br/>a&#160;weak&#160;open-<br/>source&#160;LLM<br/>(BLOOM7B)<br/>and&#160;concen-<br/>trated&#160;solely<br/>on&#160;the&#160;NBA<br/>domain&#160;for<br/>analysis.<br/>•&#160;Relies&#160;on<br/>automatically<br/>generated<br/>questions.</p>
<p style="position:absolute;top:616px;left:258px;white-space:nowrap" class="ft188">Hallucination<br/>Augmented<br/>Recitations&#160;for<br/>Language&#160;Models<br/><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Köksal&#160;et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:615px;left:352px;white-space:nowrap" class="ft184">×</p>
<p style="position:absolute;top:616px;left:403px;white-space:nowrap" class="ft180">QA,</p>
<p style="position:absolute;top:616px;left:476px;white-space:nowrap" class="ft180">F1-score</p>
<p style="position:absolute;top:616px;left:559px;white-space:nowrap" class="ft180">T5</p>
<p style="position:absolute;top:616px;left:632px;white-space:nowrap" class="ft180">CFTriviaQA</p>
<p style="position:absolute;top:616px;left:704px;white-space:nowrap" class="ft188">Details&#160;not<br/>provided</p>
<p style="position:absolute;top:677px;left:258px;white-space:nowrap" class="ft189">Fine-tuning<br/>Language&#160;Models<br/>for&#160;Factuality&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Tian<br/>et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:677px;left:352px;white-space:nowrap" class="ft186">✓</p>
<p style="position:absolute;top:677px;left:403px;white-space:nowrap" class="ft188">Biography<br/>generation&#160;and<br/>Medical&#160;QA</p>
<p style="position:absolute;top:677px;left:476px;white-space:nowrap" class="ft180">FactScore</p>
<p style="position:absolute;top:677px;left:559px;white-space:nowrap" class="ft188">Llama1&#160;and<br/>Llama2-chat</p>
<p style="position:absolute;top:677px;left:632px;white-space:nowrap" class="ft180">Manual</p>
<p style="position:absolute;top:677px;left:704px;white-space:nowrap" class="ft188">Details&#160;not<br/>provided</p>
<p style="position:absolute;top:727px;left:258px;white-space:nowrap" class="ft189">Dial&#160;BEINFO&#160;for<br/>Faithfulness:<br/>Improving&#160;Factuality<br/>of<br/>Information-Seeking<br/>Dialogue&#160;via<br/>Behavioural<br/>Fine-Tuning<br/><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Razumovskaia<br/>et&#160;al.,&#160;2023)</a></p>
<p style="position:absolute;top:727px;left:352px;white-space:nowrap" class="ft186">✓</p>
<p style="position:absolute;top:727px;left:403px;white-space:nowrap" class="ft180">QA</p>
<p style="position:absolute;top:727px;left:476px;white-space:nowrap" class="ft188">BLEU,&#160;ROUGE,<br/>BERTScore&#160;and<br/>Precision</p>
<p style="position:absolute;top:727px;left:559px;white-space:nowrap" class="ft188">Flan-T5&#160;(Base,<br/>Large&#160;and&#160;XL)</p>
<p style="position:absolute;top:727px;left:632px;white-space:nowrap" class="ft188">FaithDial,<br/>TopiOCQA<br/>and&#160;DoQA</p>
<p style="position:absolute;top:726px;left:704px;white-space:nowrap" class="ft188">•&#160;The&#160;work<br/>concentrates&#160;on<br/>models&#160;with&#160;a<br/>parameter&#160;limit<br/>of&#160;up&#160;to&#160;3B.<br/>•&#160;It&#160;addresses<br/>the&#160;reduction<br/>of&#160;LLM<br/>hallucinations<br/>in&#160;information-<br/>seeking<br/>dialogue<br/>without<br/>intervening&#160;in<br/>the&#160;knowledge<br/>retrieval<br/>component.</p>
<p style="position:absolute;top:913px;left:258px;white-space:nowrap" class="ft188">R-Tuning:&#160;Teaching<br/>Large&#160;Language<br/>Models&#160;to&#160;Refuse<br/>Unknown&#160;Questions<br/><a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#13">(Zhang&#160;et&#160;al.,&#160;2023a)</a></p>
<p style="position:absolute;top:913px;left:352px;white-space:nowrap" class="ft184">×</p>
<p style="position:absolute;top:913px;left:403px;white-space:nowrap" class="ft180">QA</p>
<p style="position:absolute;top:913px;left:476px;white-space:nowrap" class="ft188">Accuracy,&#160;Av-<br/>erage&#160;Precision<br/>(AP)</p>
<p style="position:absolute;top:913px;left:559px;white-space:nowrap" class="ft188">OpenLLaMA-<br/>3B,&#160;LLaMA-<br/>7B&#160;and<br/>LLaMA-13B</p>
<p style="position:absolute;top:913px;left:632px;white-space:nowrap" class="ft188">ParaRel,<br/>MMLU,</p>
<p style="position:absolute;top:934px;left:631px;white-space:nowrap" class="ft180">WiCE,</p>
<p style="position:absolute;top:945px;left:632px;white-space:nowrap" class="ft188">HotpotQA&#160;and<br/>FEVER</p>
<p style="position:absolute;top:913px;left:704px;white-space:nowrap" class="ft188">Details&#160;not<br/>provided</p>
<p style="position:absolute;top:975px;left:360px;white-space:nowrap" class="ft181">Continued&#160;on&#160;the&#160;next&#160;page</p>
</div>
</body>
</html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html-html.html</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <br/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft190{font-size:8px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft191{font-size:8px;font-family:YOPPJI+NimbusRomNo9L-ReguItal;color:#000000;}
	.ft192{font-size:8px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft193{font-size:8px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft194{font-size:8px;font-family:LNIRBF+MSAM7;color:#009600;}
	.ft195{font-size:8px;font-family:LNLVHQ+CMSY6;color:#000000;}
	.ft196{font-size:8px;line-height:10px;font-family:ZHBKCQ+NimbusRomNo9L-Medi;color:#000000;}
	.ft197{font-size:8px;line-height:10px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft198{font-size:8px;line-height:10px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#00007f;}
	.ft199{font-size:8px;line-height:11px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
	.ft1910{font-size:8px;line-height:9px;font-family:SIMXDZ+NimbusRomNo9L-Regu;color:#000000;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page19-div" style="position:relative;width:892px;height:1262px;">
<img width="892" height="1262" src="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_html019.png" alt="background image"/>
<p style="position:absolute;top:113px;left:368px;white-space:nowrap" class="ft190">Table&#160;1&#160;–&#160;Continued&#160;from&#160;the&#160;previous&#160;page</p>
<p style="position:absolute;top:133px;left:129px;white-space:nowrap" class="ft192">Category</p>
<p style="position:absolute;top:133px;left:186px;white-space:nowrap" class="ft196">Mitigation<br/>Technique(s)</p>
<p style="position:absolute;top:133px;left:258px;white-space:nowrap" class="ft192">Title</p>
<p style="position:absolute;top:133px;left:352px;white-space:nowrap" class="ft192">Detection</p>
<p style="position:absolute;top:133px;left:403px;white-space:nowrap" class="ft192">Task(s)</p>
<p style="position:absolute;top:133px;left:476px;white-space:nowrap" class="ft192">Metric(s)</p>
<p style="position:absolute;top:133px;left:559px;white-space:nowrap" class="ft196">Evaluated<br/>LLM(s)</p>
<p style="position:absolute;top:133px;left:632px;white-space:nowrap" class="ft192">Dataset(s)</p>
<p style="position:absolute;top:133px;left:704px;white-space:nowrap" class="ft192">Limitation(s)</p>
<p style="position:absolute;top:162px;left:258px;white-space:nowrap" class="ft190">Think&#160;While&#160;You</p>
<p style="position:absolute;top:173px;left:258px;white-space:nowrap" class="ft197">Write&#160;Hypothesis<br/>Verification</p>
<p style="position:absolute;top:194px;left:258px;white-space:nowrap" class="ft198">Promotes&#160;Faithful<br/>Knowledge-to-Text<br/>Generation&#160;<a href="A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf_to_htmls.html#12">(Qiu<br/>et&#160;al.,&#160;2023a)</a></p>
<p style="position:absolute;top:162px;left:352px;white-space:nowrap" class="ft194">✓</p>
<p style="position:absolute;top:162px;left:403px;white-space:nowrap" class="ft197">Hypothesis<br/>Verification</p>
<p style="position:absolute;top:162px;left:476px;white-space:nowrap" class="ft197">FactKB,&#160;BLEU,<br/>METEOR,<br/>BERTScore</p>
<p style="position:absolute;top:162px;left:559px;white-space:nowrap" class="ft197">BART-large<br/>and&#160;T5-large</p>
<p style="position:absolute;top:162px;left:632px;white-space:nowrap" class="ft197">FATE&#160;(Own<br/>novel&#160;dataset),</p>
<p style="position:absolute;top:183px;left:631px;white-space:nowrap" class="ft197">WebNLG,<br/>TekGen&#160;and</p>
<p style="position:absolute;top:204px;left:632px;white-space:nowrap" class="ft190">GenWiki</p>
<p style="position:absolute;top:161px;left:704px;white-space:nowrap" class="ft197">•&#160;The&#160;TWEAK<br/>decoding<br/>strategy<br/>increases<br/>computational<br/>cost&#160;during<br/>inference<br/>compared&#160;to<br/>baseline<br/>approaches&#160;like<br/>beam&#160;search.<br/>•&#160;The&#160;approach<br/>has&#160;only<br/>undergone<br/>testing&#160;in&#160;the<br/>English<br/>language.</p>
</div>
</body>
</html>
