{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10c873-2c82-47a3-9e0b-8772a5f3c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import os\n",
    "\n",
    "os.environ[\"DEBUG\"] = \"1\"  # Set to \"1\" if you want to use debug mode.\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231a971-2ce5-4b10-a7ca-ae48f5ad84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from prompttools.experiment import HuggingFaceHubExperiment\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abe729-3ac7-4168-affe-3daf0e39ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompttools.experiment import SequentialChainExperiment\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from prompttools.utils import semantic_similarity\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\"Translate the following review to english:\" \"\\n\\n{Review}\")\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\" \"\\n\\n{English_Review}\"\n",
    ")\n",
    "\n",
    "third_prompt = ChatPromptTemplate.from_template(\"What language is the following review:\\n\\n{Review}\")\n",
    "\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "\n",
    "experiment = SequentialChainExperiment(\n",
    "    llm=[ChatOpenAI],\n",
    "    prompt=[\"أنا أحب أكل الشاورما\", \"me encanta comer shawarma\"],\n",
    "    prompt_template=[[first_prompt, second_prompt, third_prompt, fourth_prompt]],\n",
    "    **{\n",
    "        \"temperature\": [0.1, 0.9],\n",
    "        \"input_variables\": [[\"Review\"]],\n",
    "        \"output_variables\": [[\"English_Review\", \"summary\", \"followup_message\"]],\n",
    "        \"output_key\": [[\"English_Review\", \"summary\", \"language\", \"followup_message\"]],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "experiment.run()\n",
    "\n",
    "experiment.evaluate(\"similar_to_expected\", semantic_similarity, expected=[\"I love to eat shawarma\"] * 4)\n",
    "\n",
    "experiment.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64632bff-0f95-4942-ab81-a9e0ca7d98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"mistralai/Mistral-7B-v0.1\"]\n",
    "prompts = [\n",
    "    \"Two turntables and a?\",\n",
    "    \"Three turntables and a?\",\n",
    "]\n",
    "task = [\"text-generation\"]\n",
    "temperatures = [0.0, 1.0]\n",
    "\n",
    "experiment = HuggingFaceHubExperiment(models, prompts, task, temperature=temperatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d264f61f-2521-41fa-8242-cb37c90695fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e49af-6779-4740-a8df-4050dc67da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompttools.utils import semantic_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89a9dc-0b3c-4a80-a5c6-b28a25dd8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.evaluate(\"similar_to_expected\", semantic_similarity, expected=[\"microphone\"] * 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b6aa7-9b83-4f67-b3d7-71d79985a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.visualize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7809cc-2db0-4d34-8c0d-55833d982487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Hegel AI, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code's license can be found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "import jinja2\n",
    "import prompttools.prompttest as prompttest\n",
    "from prompttools.utils import similarity\n",
    "from prompttools.prompttest.threshold_type import ThresholdType\n",
    "from prompttools.mock.mock import mock_hf_completion_fn\n",
    "from huggingface_hub.inference_api import InferenceApi\n",
    "\n",
    "if not ((\"HUGGINGFACEHUB_API_TOKEN\" in os.environ) or (\"DEBUG\" in os.environ)):\n",
    "    print(\"Error: This example requires you to set either your HUGGINGFACEHUB_API_TOKEN or DEBUG=1\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "client = InferenceApi(\n",
    "    repo_id=\"google/flan-t5-xxl\",\n",
    "    token=os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "    task=\"text2text-generation\",\n",
    ")\n",
    "\n",
    "\n",
    "def create_prompt():\n",
    "    prompt_template = \"Answer the following question: {{ input }}\"\n",
    "    user_input = {\"input\": \"Who was the first president of the USA?\"}\n",
    "    environment = jinja2.Environment()\n",
    "    template = environment.from_string(prompt_template)\n",
    "    return template.render(**user_input)\n",
    "\n",
    "\n",
    "@prompttest.prompttest(\n",
    "    metric_name=\"similar_to_expected\",\n",
    "    eval_fn=similarity.evaluate,\n",
    "    prompts=[create_prompt()],\n",
    "    expected=[\"George Washington\"],\n",
    "    threshold=1.0,\n",
    "    threshold_type=ThresholdType.MAXIMUM,\n",
    ")\n",
    "def completion_fn(prompt: str):\n",
    "    response = None\n",
    "    if os.getenv(\"DEBUG\", default=False):\n",
    "        response = mock_hf_completion_fn(**{\"inputs\": prompt})\n",
    "    else:\n",
    "        response = client(inputs=prompt)\n",
    "    return response[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompttest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62afe4-6861-4587-b257-27a8d628bde0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
